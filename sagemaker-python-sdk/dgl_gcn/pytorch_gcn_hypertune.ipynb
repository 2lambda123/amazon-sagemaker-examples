{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca03c313",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [9]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2cbea52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:02.736405Z",
     "iopub.status.busy": "2021-06-03T00:17:02.735952Z",
     "iopub.status.idle": "2021-06-03T00:17:02.738005Z",
     "shell.execute_reply": "2021-06-03T00:17:02.738374Z"
    },
    "papermill": {
     "duration": 0.019631,
     "end_time": "2021-06-03T00:17:02.738504",
     "exception": false,
     "start_time": "2021-06-03T00:17:02.718873",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "kms_key = \"arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1dfa20",
   "metadata": {
    "papermill": {
     "duration": 0.008536,
     "end_time": "2021-06-03T00:17:02.756362",
     "exception": false,
     "start_time": "2021-06-03T00:17:02.747826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter tuning with Amazon SageMaker and Deep Graph Library with PyTorch backend\n",
    "_**Creating a Hyperparameter Tuning Job for an Deep Graph Library (DGL) Network**_\n",
    "___\n",
    "___\n",
    "\n",
    "\n",
    "## Contents\n",
    "1. [Background](#Background)  \n",
    "2. [Setup](#Setup)  \n",
    "3. [Code](#Code)  \n",
    "4. [Tune](#Train)  \n",
    "5. [Wrap-up](#Wrap-up)  \n",
    "\n",
    "## Background\n",
    "This example notebook focuses on how to create a graph neural network model to train the [Cora dataset] using DGL with PyTorch backend. It leverages SageMaker's hyperparameter tuning to kick off multiple training jobs with different hyperparameter combinations, to find the set with best model performance. This is an important step in the machine learning process as hyperparameter settings can have a large impact on model accuracy. In this example, you use the [Amazon SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) to create a hyperparameter tuning job for an Amazon SageMaker estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf2e3d",
   "metadata": {
    "papermill": {
     "duration": 0.008421,
     "end_time": "2021-06-03T00:17:02.773440",
     "exception": false,
     "start_time": "2021-06-03T00:17:02.765019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "This notebook was created and tested on an ml.p3.2xlarge notebook instance.\n",
    "\n",
    "Prerequisites\n",
    " * You can successfully run the pytorch_gcn example (see pytorch_gcn.ipynb).\n",
    " * An S3 bucket and prefix that you want to use exists for training and model data. This should be within the same Region as the notebook instance, training, and hosting.\n",
    " * You have the IAM role ARN used to give training and hosting access to your data. See the documentation for more details on creating these. If a role not associated with the current notebook instance, or more than one role is required for training and/or hosting, replace sagemaker.get_execution_role() with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbd8e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:02.793925Z",
     "iopub.status.busy": "2021-06-03T00:17:02.793481Z",
     "iopub.status.idle": "2021-06-03T00:17:04.006704Z",
     "shell.execute_reply": "2021-06-03T00:17:04.007099Z"
    },
    "papermill": {
     "duration": 1.225265,
     "end_time": "2021-06-03T00:17:04.007234",
     "exception": false,
     "start_time": "2021-06-03T00:17:02.781969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# Setup session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# Location to put your custom code.\n",
    "custom_code_upload_location = \"customcode\"\n",
    "\n",
    "# IAM execution role that gives Amazon SageMaker access to resources in your AWS account.\n",
    "# You can use the Amazon SageMaker Python SDK to get the role from the notebook environment.\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff9601",
   "metadata": {
    "papermill": {
     "duration": 0.008763,
     "end_time": "2021-06-03T00:17:04.024984",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.016221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now import the Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa7f2d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:04.046612Z",
     "iopub.status.busy": "2021-06-03T00:17:04.046096Z",
     "iopub.status.idle": "2021-06-03T00:17:04.048347Z",
     "shell.execute_reply": "2021-06-03T00:17:04.047892Z"
    },
    "papermill": {
     "duration": 0.014707,
     "end_time": "2021-06-03T00:17:04.048446",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.033739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8943a",
   "metadata": {
    "papermill": {
     "duration": 0.008672,
     "end_time": "2021-06-03T00:17:04.065844",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.057172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Code\n",
    "To use Amazon SageMaker to run Docker containers, provide an Python script for the container to run. In this example, pytorch_gcn.py provides all the code for training an Amazon SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d0a721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:04.086808Z",
     "iopub.status.busy": "2021-06-03T00:17:04.086348Z",
     "iopub.status.idle": "2021-06-03T00:17:04.240976Z",
     "shell.execute_reply": "2021-06-03T00:17:04.241394Z"
    },
    "papermill": {
     "duration": 0.16685,
     "end_time": "2021-06-03T00:17:04.241531",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.074681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\r\n",
      "import json\r\n",
      "import os\r\n",
      "import time\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "import torch\r\n",
      "import torch.nn as nn\r\n",
      "import torch.nn.functional as F\r\n",
      "from dgl import DGLGraph\r\n",
      "from dgl.data import load_data, register_data_args\r\n",
      "from dgl.nn.pytorch import GraphConv\r\n",
      "\r\n",
      "\r\n",
      "# define GCN layer\r\n",
      "class GCN(nn.Module):\r\n",
      "    def __init__(self, g, in_feats, n_hidden, n_classes, n_layers, activation, dropout):\r\n",
      "        super(GCN, self).__init__()\r\n",
      "        self.g = g\r\n",
      "        self.layers = nn.ModuleList()\r\n",
      "        # input layer\r\n",
      "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\r\n",
      "        # hidden layers\r\n",
      "        for i in range(n_layers - 1):\r\n",
      "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\r\n",
      "        # output layer\r\n",
      "        self.layers.append(GraphConv(n_hidden, n_classes))\r\n",
      "        self.dropout = nn.Dropout(p=dropout)\r\n",
      "\r\n",
      "    def forward(self, features):\r\n",
      "        h = features\r\n",
      "        for i, layer in enumerate(self.layers):\r\n",
      "            if i != 0:\r\n",
      "                h = self.dropout(h)\r\n",
      "            h = layer(self.g, h)\r\n",
      "        return h\r\n",
      "\r\n",
      "\r\n",
      "def evaluate(model, features, labels, mask):\r\n",
      "    model.eval()\r\n",
      "    with torch.no_grad():\r\n",
      "        logits = model(features)\r\n",
      "        logits = logits[mask]\r\n",
      "        labels = labels[mask]\r\n",
      "        _, indices = torch.max(logits, dim=1)\r\n",
      "        correct = torch.sum(indices == labels)\r\n",
      "        return correct.item() * 1.0 / len(labels)\r\n",
      "\r\n",
      "\r\n",
      "def main(args):\r\n",
      "    # load and preprocess dataset\r\n",
      "    data = load_data(args)\r\n",
      "    features = torch.FloatTensor(data.features)\r\n",
      "    labels = torch.LongTensor(data.labels)\r\n",
      "    train_mask = torch.ByteTensor(data.train_mask)\r\n",
      "    val_mask = torch.ByteTensor(data.val_mask)\r\n",
      "    test_mask = torch.ByteTensor(data.test_mask)\r\n",
      "    in_feats = features.shape[1]\r\n",
      "    n_classes = data.num_labels\r\n",
      "    n_edges = data.graph.number_of_edges()\r\n",
      "    print(\r\n",
      "        \"\"\"----Data statistics------'\r\n",
      "      #Edges %d\r\n",
      "      #Classes %d\r\n",
      "      #Train samples %d\r\n",
      "      #Val samples %d\r\n",
      "      #Test samples %d\"\"\"\r\n",
      "        % (\r\n",
      "            n_edges,\r\n",
      "            n_classes,\r\n",
      "            train_mask.sum().item(),\r\n",
      "            val_mask.sum().item(),\r\n",
      "            test_mask.sum().item(),\r\n",
      "        )\r\n",
      "    )\r\n",
      "\r\n",
      "    if args.gpu < 0:\r\n",
      "        cuda = False\r\n",
      "    else:\r\n",
      "        cuda = True\r\n",
      "        torch.cuda.set_device(args.gpu)\r\n",
      "        features = features.cuda()\r\n",
      "        labels = labels.cuda()\r\n",
      "        train_mask = train_mask.cuda()\r\n",
      "        val_mask = val_mask.cuda()\r\n",
      "        test_mask = test_mask.cuda()\r\n",
      "\r\n",
      "    # graph preprocess and calculate normalization factor\r\n",
      "    g = data.graph\r\n",
      "    # add self loop\r\n",
      "    if args.self_loop:\r\n",
      "        g.remove_edges_from(g.selfloop_edges())\r\n",
      "        g.add_edges_from(zip(g.nodes(), g.nodes()))\r\n",
      "    g = DGLGraph(g)\r\n",
      "    n_edges = g.number_of_edges()\r\n",
      "    # normalization\r\n",
      "    degs = g.in_degrees().float()\r\n",
      "    norm = torch.pow(degs, -0.5)\r\n",
      "    norm[torch.isinf(norm)] = 0\r\n",
      "    if cuda:\r\n",
      "        norm = norm.cuda()\r\n",
      "    g.ndata[\"norm\"] = norm.unsqueeze(1)\r\n",
      "\r\n",
      "    # create GCN model\r\n",
      "    model = GCN(g, in_feats, args.n_hidden, n_classes, args.n_layers, F.relu, args.dropout)\r\n",
      "\r\n",
      "    if cuda:\r\n",
      "        model.cuda()\r\n",
      "    loss_fcn = torch.nn.CrossEntropyLoss()\r\n",
      "\r\n",
      "    # use optimizer\r\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\r\n",
      "\r\n",
      "    # initialize graph\r\n",
      "    dur = []\r\n",
      "    for epoch in range(args.n_epochs):\r\n",
      "        model.train()\r\n",
      "        if epoch >= 3:\r\n",
      "            t0 = time.time()\r\n",
      "        # forward\r\n",
      "        logits = model(features)\r\n",
      "        loss = loss_fcn(logits[train_mask], labels[train_mask])\r\n",
      "\r\n",
      "        optimizer.zero_grad()\r\n",
      "        loss.backward()\r\n",
      "        optimizer.step()\r\n",
      "\r\n",
      "        if epoch >= 3:\r\n",
      "            dur.append(time.time() - t0)\r\n",
      "\r\n",
      "        acc = evaluate(model, features, labels, val_mask)\r\n",
      "        print(\r\n",
      "            \"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\r\n",
      "            \"ETputs(KTEPS) {:.2f}\".format(\r\n",
      "                epoch, np.mean(dur), loss.item(), acc, n_edges / np.mean(dur) / 1000\r\n",
      "            )\r\n",
      "        )\r\n",
      "\r\n",
      "    print()\r\n",
      "    acc = evaluate(model, features, labels, test_mask)\r\n",
      "    print(\"Test Accuracy {:.2%}\".format(acc))\r\n",
      "\r\n",
      "    torch.save(model.state_dict(), args.save_path)\r\n",
      "\r\n",
      "\r\n",
      "import collections\r\n",
      "import warnings\r\n",
      "\r\n",
      "warnings.filterwarnings(\"ignore\")\r\n",
      "\r\n",
      "\r\n",
      "def parse_args():\r\n",
      "    parser = argparse.ArgumentParser(description=\"GCN\")\r\n",
      "    register_data_args(parser)\r\n",
      "    parser.add_argument(\"--dropout\", type=float, default=0.5, help=\"dropout probability\")\r\n",
      "    parser.add_argument(\"--gpu\", type=int, default=-1, help=\"gpu\")\r\n",
      "    parser.add_argument(\"--lr\", type=float, default=3e-2, help=\"learning rate\")\r\n",
      "    parser.add_argument(\"--n-epochs\", type=int, default=200, help=\"number of training epochs\")\r\n",
      "    parser.add_argument(\"--n-hidden\", type=int, default=16, help=\"number of hidden gcn units\")\r\n",
      "    parser.add_argument(\"--n-layers\", type=int, default=1, help=\"number of hidden gcn layers\")\r\n",
      "    parser.add_argument(\"--weight-decay\", type=float, default=5e-4, help=\"Weight for L2 loss\")\r\n",
      "    parser.add_argument(\"--self-loop\", action=\"store_true\", help=\"graph self-loop (default=False)\")\r\n",
      "    parser.add_argument(\r\n",
      "        \"--save-path\", type=str, default=\"./model/gcn.pt\", help=\"path to save model\"\r\n",
      "    )\r\n",
      "    parser.set_defaults(self_loop=False)\r\n",
      "\r\n",
      "    return parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    args = parse_args()\r\n",
      "\r\n",
      "    num_gpus = int(os.environ[\"SM_NUM_GPUS\"])\r\n",
      "    if num_gpus == 0:\r\n",
      "        args.gpu = -1\r\n",
      "    else:\r\n",
      "        args.gpu = 0\r\n",
      "\r\n",
      "    path = str(os.environ[\"SM_MODEL_DIR\"])\r\n",
      "    args.save_path = os.path.join(path, \"gcn.pt\")\r\n",
      "\r\n",
      "    print(args)\r\n",
      "    main(args)\r\n"
     ]
    }
   ],
   "source": [
    "!cat pytorch_gcn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347d9f5",
   "metadata": {
    "papermill": {
     "duration": 0.009236,
     "end_time": "2021-06-03T00:17:04.260596",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.251360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After you specify and test the training script to ensure it works, you can start the tuning job. Testing can be done in either local mode or using Amazon SageMaker training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90234d8",
   "metadata": {
    "papermill": {
     "duration": 0.009121,
     "end_time": "2021-06-03T00:17:04.278955",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.269834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tune\n",
    "Similar to training a single training job in Amazon SageMaker, define the training estimator passing in the code scripts, IAM role, (per job) hardware configuration, and any hyperparameters you are not tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2abb3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:04.302737Z",
     "iopub.status.busy": "2021-06-03T00:17:04.301989Z",
     "iopub.status.idle": "2021-06-03T00:17:04.630248Z",
     "shell.execute_reply": "2021-06-03T00:17:04.630587Z"
    },
    "papermill": {
     "duration": 0.342642,
     "end_time": "2021-06-03T00:17:04.630716",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.288074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "CODE_PATH = \"pytorch_gcn.py\"\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "params = {}\n",
    "params[\"dataset\"] = \"cora\"\n",
    "estimator = PyTorch(\n",
    "    entry_point=CODE_PATH,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p3.2xlarge\",\n",
    "    framework_version=\"1.3.1\",\n",
    "    py_version=\"py3\",\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters=params,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326a954",
   "metadata": {
    "papermill": {
     "duration": 0.009729,
     "end_time": "2021-06-03T00:17:04.650383",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.640654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After you define the estimator, specify the hyperparameters you want to tune and their possible values. You have three different types of hyperparameters.\n",
    "  * Categorical parameters need to take one value from a discrete set. Define this by passing the list of possible values to CategoricalParameter(list)\n",
    "  * Continuous parameters can take any real number value between the minimum and maximum value, defined by ContinuousParameter(min, max)\n",
    "  * Integer parameters can take any integer value between the minimum and maximum value, defined by IntegerParameter(min, max)\n",
    "  \n",
    "Note, if possible, it's almost always best to specify a value as the least restrictive type. For example, tuning threshold as a continuous value between 0.01 and 0.2 is likely to yield a better result than tuning as a categorical parameter with possible values of 0.01, 0.1, 0.15, or 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97850f14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:04.672993Z",
     "iopub.status.busy": "2021-06-03T00:17:04.672482Z",
     "iopub.status.idle": "2021-06-03T00:17:04.674421Z",
     "shell.execute_reply": "2021-06-03T00:17:04.674783Z"
    },
    "papermill": {
     "duration": 0.014866,
     "end_time": "2021-06-03T00:17:04.674892",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.660026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.01),\n",
    "    \"n-epochs\": IntegerParameter(100, 200),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4588a",
   "metadata": {
    "papermill": {
     "duration": 0.009949,
     "end_time": "2021-06-03T00:17:04.694644",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.684695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, specify the objective metric that you want to tune and its definition. This includes the regular expression (regex) needed to extract that metric from the Amazon CloudWatch logs of the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5ccc04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:04.717482Z",
     "iopub.status.busy": "2021-06-03T00:17:04.717023Z",
     "iopub.status.idle": "2021-06-03T00:17:04.718799Z",
     "shell.execute_reply": "2021-06-03T00:17:04.719156Z"
    },
    "papermill": {
     "duration": 0.014701,
     "end_time": "2021-06-03T00:17:04.719261",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.704560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "objective_metric_name = \"Validation-accuracy\"\n",
    "metric_definitions = [{\"Name\": \"Validation-accuracy\", \"Regex\": \"Test Accuracy ([0-9\\\\.]+)%\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ffcc07",
   "metadata": {
    "papermill": {
     "duration": 0.009747,
     "end_time": "2021-06-03T00:17:04.739126",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.729379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, create a HyperparameterTuner object, which you pass:\n",
    "\n",
    " * The training estimator you created above\n",
    " * The hyperparameter ranges\n",
    " * Objective metric name and definition\n",
    " * Number of training jobs to run in total and how many training jobs should be run simultaneously. More parallel jobs will finish tuning sooner, but may sacrifice accuracy. We recommend you set the parallel jobs value to less than 10% of the total number of training jobs (we'll set it higher just for this example to keep it short).\n",
    " * Whether you should maximize or minimize the objective metric. You haven't specified here since it defaults to 'Maximize', which is what you want for validation accuracy\n",
    "\n",
    "You can also add a task_tag with value 'DGL' to help tracking the hyperparameter tuning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd1a983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:04.762316Z",
     "iopub.status.busy": "2021-06-03T00:17:04.761856Z",
     "iopub.status.idle": "2021-06-03T00:17:04.763571Z",
     "shell.execute_reply": "2021-06-03T00:17:04.763925Z"
    },
    "papermill": {
     "duration": 0.015069,
     "end_time": "2021-06-03T00:17:04.764027",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.748958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_tags = [{\"Key\": \"ML Task\", \"Value\": \"DGL\"}]\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    tags=task_tags,\n",
    "    max_jobs=6,\n",
    "    max_parallel_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf51e5",
   "metadata": {
    "papermill": {
     "duration": 0.009936,
     "end_time": "2021-06-03T00:17:04.784150",
     "exception": false,
     "start_time": "2021-06-03T00:17:04.774214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And finally, start the tuning job by calling .fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65ae28",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0cb3bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-03T00:17:04.806924Z",
     "iopub.status.busy": "2021-06-03T00:17:04.806463Z",
     "iopub.status.idle": "2021-06-03T00:35:17.386420Z",
     "shell.execute_reply": "2021-06-03T00:35:17.385686Z"
    },
    "papermill": {
     "duration": 1092.592559,
     "end_time": "2021-06-03T00:35:17.386652",
     "exception": true,
     "start_time": "2021-06-03T00:17:04.794093",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for HyperParameterTuning job pytorch-training-210603-0017: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7a7c5f4753af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, estimator_kwargs, wait, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_tuning_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_with_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_cls_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_tuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_tuning_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         \"\"\"\n\u001b[1;32m   3161\u001b[0m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_tuning_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3162\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HyperParameterTuningJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3243\u001b[0m                 ),\n\u001b[1;32m   3244\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3245\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3246\u001b[0m             )\n\u001b[1;32m   3247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for HyperParameterTuning job pytorch-training-210603-0017: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details."
     ]
    }
   ],
   "source": [
    "tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fabed7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's just run a quick check of the hyperparameter tuning jobs status to make sure it started successfully and is InProgress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196b3b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3.client(\"sagemaker\").describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name\n",
    ")[\"HyperParameterTuningJobStatus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff16d2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Wrap-up\n",
    "After you start the hyperparameter tuning job, it will run in the background. You can close this notebook. After it finishes, you can go to console to analyze the result.\n",
    "\n",
    "For more information about Amazon SageMaker's Hyperparameter tuning, see the AWS documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1096.034965,
   "end_time": "2021-06-03T00:35:17.744870",
   "environment_variables": {},
   "exception": true,
   "input_path": "pytorch_gcn_hypertune.ipynb",
   "output_path": "/opt/ml/processing/output/pytorch_gcn_hypertune-2021-06-03-00-13-06.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-06-03T00:17:01.709905",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}