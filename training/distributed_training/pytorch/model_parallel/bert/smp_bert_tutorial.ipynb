{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metric-entrepreneur",
   "metadata": {
    "papermill": {
     "duration": 0.007625,
     "end_time": "2021-06-07T00:10:20.118408",
     "exception": false,
     "start_time": "2021-06-07T00:10:20.110783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Use Amazon Sagemaker Distributed Model Parallel to Launch a BERT Training Job with Model Parallelization\n",
    "\n",
    "Sagemaker distributed model parallel (SMP) is a model parallelism library for training large deep learning models that were previously difficult to train due to GPU memory limitations. SMP automatically and efficiently splits a model across multiple GPUs and instances and coordinates model training, allowing you to increase prediction accuracy by creating larger models with more parameters.\n",
    "\n",
    "Use this notebook to configure SMP to train a model using PyTorch (version 1.6.0) and the [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk).\n",
    "\n",
    "In this notebook, you will use a BERT example training script with SMP.\n",
    "The example script is based on [Nvidia Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT) and requires you to download the datasets and upload them to Amazon Simple Storage Service (Amazon S3) as explained in the instructions below. This is a large dataset, and so depending on your connection speed, this process can take hours to complete. \n",
    "\n",
    "This notebook depends on the following files. You can find all files in the [bert directory](https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training/pytorch/model_parallel/bert) in the model parllel section of the Amazon SageMaker Examples notebooks repo.\n",
    "\n",
    "* `bert_example/sagemaker_smp_pretrain.py`: This is an entrypoint script that is passed to the Pytorch estimator in the notebook instructions. This script is responsible for end to end training of the BERT model with SMP. The script has additional comments at places where the SMP API is used.\n",
    "\n",
    "* `bert_example/modeling.py`: This contains the model definition for the BERT model.\n",
    "\n",
    "* `bert_example/bert_config.json`: This allows for additional configuration of the model and is used by `modeling.py`. Additional configuration includes dropout probabilities, pooler and encoder sizes, number of hidden layers in the encoder, size of the intermediate layers in the encoder etc.\n",
    "\n",
    "* `bert_example/schedulers.py`: contains definitions for learning rate schedulers used in end to end training of the BERT model (`bert_example/sagemaker_smp_pretrain.py`).\n",
    "\n",
    "* `bert_example/utils.py`: This contains different helper utility functions used in end to end training of the BERT model (`bert_example/sagemaker_smp_pretrain.py`).\n",
    "\n",
    "* `bert_example/file_utils.py`: Contains different file utility functions used in model definition (`bert_example/modeling.py`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-irrigation",
   "metadata": {
    "papermill": {
     "duration": 0.007436,
     "end_time": "2021-06-07T00:10:20.133370",
     "exception": false,
     "start_time": "2021-06-07T00:10:20.125934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Additional Resources\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to learn more about SMP and using SageMaker with Pytorch. \n",
    "\n",
    "* To learn more about the SageMaker model parallelism library, see [Model Parallel Distributed Training with SageMaker Distributed](http://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html).\n",
    "\n",
    "* To learn more about using the SageMaker Python SDK with Pytorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "\n",
    "* To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "\n",
    "### Prerequisites \n",
    "\n",
    "1. You must create an S3 bucket to store the input data to be used for training. This bucket must must be located in the same AWS Region you use to launch your training job. This is the AWS Region you use to run this notebook. To learn how, see [Creating a bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html) in the Amazon S3 documentation.\n",
    "\n",
    "2. You must download the dataset that you use for training from [Nvidia Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/BERT) and upload it to the S3 bucket you created. To learn more about the datasets and scripts provided to preprocess and download it, see [Getting the data](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#getting-the-data) in the Nvidia Deep Learning Examples repo README. You can also use the [Quick Start Guide](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#quick-start-guide) to learn how to download the dataset. The repository consists of three datasets. Optionally, you can to use the `wiki_only` parameter to only download the Wikipedia dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-olive",
   "metadata": {
    "papermill": {
     "duration": 0.007568,
     "end_time": "2021-06-07T00:10:20.148535",
     "exception": false,
     "start_time": "2021-06-07T00:10:20.140967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Amazon SageMaker Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-congo",
   "metadata": {
    "papermill": {
     "duration": 0.011023,
     "end_time": "2021-06-07T00:10:24.186907",
     "exception": false,
     "start_time": "2021-06-07T00:10:24.175884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Initialize the notebook instance. Get the AWS Region, SageMaker execution role Amazon Resource Name (ARN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comparable-scout",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:24.214749Z",
     "iopub.status.busy": "2021-06-07T00:10:24.214028Z",
     "iopub.status.idle": "2021-06-07T00:10:25.301687Z",
     "shell.execute_reply": "2021-06-07T00:10:25.301185Z"
    },
    "papermill": {
     "duration": 1.104005,
     "end_time": "2021-06-07T00:10:25.301804",
     "exception": false,
     "start_time": "2021-06-07T00:10:24.197799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role:arn:aws:iam::688520471316:role/hongshan-sagemaker-experiment\n",
      "AWS account:688520471316\n",
      "AWS region:us-west-2\n",
      "['', '/home/ubuntu/anaconda3/envs/python3/lib/python36.zip', '/home/ubuntu/anaconda3/envs/python3/lib/python3.6', '/home/ubuntu/anaconda3/envs/python3/lib/python3.6/lib-dynload', '/home/ubuntu/.local/lib/python3.6/site-packages', '/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages', '/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/IPython/extensions', '/home/ubuntu/.ipython']\n",
      "\n",
      "Default bucket for this session:  sagemaker-us-west-2-688520471316\n",
      "CPU times: user 253 ms, sys: 15.4 ms, total: 269 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "import sys\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print()\n",
    "print(\"Default bucket for this session: \", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-spectrum",
   "metadata": {
    "papermill": {
     "duration": 0.011811,
     "end_time": "2021-06-07T00:10:25.325490",
     "exception": false,
     "start_time": "2021-06-07T00:10:25.313679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare/Identify your Training Data in Amazon S3\n",
    "\n",
    "If you don't already have the BERT dataset in an S3 bucket, please see the instructions in [Nvidia BERT Example](https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md) to download the dataset and upload it to a s3 bucket. See the prerequisites at the beginning of this notebook for more information.\n",
    "\n",
    "Replace the instances of `None` below to set the S3 bucket and prefix of your preprocessed\n",
    "data. For example, if your training data is in s3://your-bucket/training, enter `'your-bucket'` for `s3_bucket` and `'training'` for `prefix`. Note that your output data will be stored in the same bucket, under the `output/` prefix.\n",
    "\n",
    "If you proceed with `None` values for both `s3_bucket` and `prefix`, then the program downloads some mock data from a public S3 bucket `sagemaker-sample-files` and uploads it\n",
    "to your default bucket. This is intended for CI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "single-sleeping",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:25.356432Z",
     "iopub.status.busy": "2021-06-07T00:10:25.355701Z",
     "iopub.status.idle": "2021-06-07T00:10:26.883323Z",
     "shell.execute_reply": "2021-06-07T00:10:26.883717Z"
    },
    "papermill": {
     "duration": 1.546605,
     "end_time": "2021-06-07T00:10:26.883867",
     "exception": false,
     "start_time": "2021-06-07T00:10:25.337262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_bucket = None  # Replace None by your bucket\n",
    "prefix = None  # Replace None by the prefix of your data\n",
    "\n",
    "# For CI\n",
    "if s3_bucket is None:\n",
    "    # Donwload some mock data from a public bucket in us-east-1\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket_name = \"sagemaker-sample-files\"\n",
    "    # Phase 1 pretraining\n",
    "    prefix = \"datasets/binary/bert/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en_abstract\"\n",
    "\n",
    "    local_dir = \"/tmp/data\"\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    for obj in bucket.objects.filter(Prefix=prefix):\n",
    "        target = os.path.join(local_dir, obj.key)\n",
    "        if not os.path.exists(os.path.dirname(target)):\n",
    "            os.makedirs(os.path.dirname(target))\n",
    "        bucket.download_file(obj.key, target)\n",
    "\n",
    "    # upload to default bucket\n",
    "    mock_data = sagemaker_session.upload_data(\n",
    "        path=os.path.join(local_dir, prefix),\n",
    "        bucket=sagemaker_session.default_bucket(),\n",
    "        key_prefix=prefix,\n",
    "    )\n",
    "\n",
    "    data_channels = {\"train\": mock_data}\n",
    "else:\n",
    "\n",
    "    s3train = f\"s3://{s3_bucket}/{prefix}\"\n",
    "    train = sagemaker.session.TrainingInput(\n",
    "        s3train, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "    )\n",
    "    data_channels = {\"train\": train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "broadband-hardwood",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:26.910965Z",
     "iopub.status.busy": "2021-06-07T00:10:26.910461Z",
     "iopub.status.idle": "2021-06-07T00:10:26.912617Z",
     "shell.execute_reply": "2021-06-07T00:10:26.912974Z"
    },
    "papermill": {
     "duration": 0.01745,
     "end_time": "2021-06-07T00:10:26.913105",
     "exception": false,
     "start_time": "2021-06-07T00:10:26.895655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-west-2-688520471316/datasets/binary/bert/hdf5_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5/wikicorpus_en_abstract'}\n"
     ]
    }
   ],
   "source": [
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-order",
   "metadata": {
    "papermill": {
     "duration": 0.011915,
     "end_time": "2021-06-07T00:10:26.937015",
     "exception": false,
     "start_time": "2021-06-07T00:10:26.925100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set your output data path. This is where model artifacts are stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "august-haiti",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:26.964461Z",
     "iopub.status.busy": "2021-06-07T00:10:26.963951Z",
     "iopub.status.idle": "2021-06-07T00:10:26.966013Z",
     "shell.execute_reply": "2021-06-07T00:10:26.966360Z"
    },
    "papermill": {
     "duration": 0.017503,
     "end_time": "2021-06-07T00:10:26.966494",
     "exception": false,
     "start_time": "2021-06-07T00:10:26.948991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your output data will be stored in: s3://sagemaker-us-west-2-688520471316/output/bert\n"
     ]
    }
   ],
   "source": [
    "s3_output_location = f\"s3://{default_bucket}/output/bert\"\n",
    "print(f\"your output data will be stored in: s3://{default_bucket}/output/bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-nation",
   "metadata": {
    "papermill": {
     "duration": 0.01207,
     "end_time": "2021-06-07T00:10:26.990734",
     "exception": false,
     "start_time": "2021-06-07T00:10:26.978664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define SageMaker Training Job\n",
    "\n",
    "Next, you will use SageMaker Estimator API to define a SageMaker Training Job. You will use a [`PyTorchEstimator`](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html) to define the number and type of EC2 instances Amazon SageMaker uses for training, as well as the size of the volume attached to those instances. \n",
    "\n",
    "You must update the following:\n",
    "* `instance_count`\n",
    "* `instance_type`\n",
    "* `volume_size`\n",
    "\n",
    "See the following sub-sections for more details. \n",
    "\n",
    "### Update the Type and Number of EC2 Instances Used\n",
    "\n",
    "The instance type and number of instances you specify in `instance_type` and `instance_count` respectively will determine the number of GPUs Amazon SageMaker uses during training. Explicitly, `instance_type` will determine the number of GPUs on a single instance and that number will be multiplied by `instance_count`. \n",
    "\n",
    "You must specify values for `instance_type` and `instance_count` so that the total number of GPUs available for training is equal to `partitions` in `config` of `smp.init` in your training script. \n",
    "\n",
    "If you set ddp to `True`, you must ensure that the total number of GPUs available is divisible by `partitions`. The result of the division is inferred to be the number of model replicas to be used for Horovod (data parallelism degree). \n",
    "\n",
    "See [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/) for SageMaker supported instances and cost information. To look up GPUs for each instance types, see [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/). Use the section **Accelerated Computing** to see general purpose GPU instances. Note that an ml.p3.2xlarge has the same number of GPUs as an p3.2xlarge.\n",
    "\n",
    "### Update your Volume Size\n",
    "\n",
    "The volume size you specify in `volume_size` must be larger than your input data size.\n",
    "\n",
    "### Set your parameters dictionary for SMP and set custom mpioptions\n",
    "\n",
    "With the parameters dictionary you can configure: the number of microbatches, number of partitions, whether to use data parallelism with ddp, the pipelining strategy, the placement strategy and other BERT specific hyperparameters. \n",
    "\n",
    "For more information about smp parameters, check the [Python SDK reference](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html?highlight=microbatches#smdistributed-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deluxe-furniture",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:27.021514Z",
     "iopub.status.busy": "2021-06-07T00:10:27.021028Z",
     "iopub.status.idle": "2021-06-07T00:10:27.022749Z",
     "shell.execute_reply": "2021-06-07T00:10:27.023095Z"
    },
    "papermill": {
     "duration": 0.020404,
     "end_time": "2021-06-07T00:10:27.023225",
     "exception": false,
     "start_time": "2021-06-07T00:10:27.002821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpi_options = \"-verbose --mca orte_base_help_aggregate 0 \"\n",
    "smp_parameters = {\n",
    "    \"optimize\": \"speed\",\n",
    "    \"microbatches\": 12,\n",
    "    \"partitions\": 2,\n",
    "    \"ddp\": True,\n",
    "    \"pipeline\": \"interleaved\",\n",
    "    \"overlapping_allreduce\": True,\n",
    "    \"placement_strategy\": \"cluster\",\n",
    "    \"memory_weight\": 0.3,\n",
    "}\n",
    "timeout = 60 * 60\n",
    "metric_definitions = [{\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}]\n",
    "\n",
    "hyperparameters = {\n",
    "    \"input_dir\": \"/opt/ml/input/data/train\",\n",
    "    \"output_dir\": \"./checkpoints\",\n",
    "    \"config_file\": \"bert_config.json\",\n",
    "    \"bert_model\": \"bert-large-uncased\",\n",
    "    \"train_batch_size\": 48,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"max_predictions_per_seq\": 20,\n",
    "    \"max_steps\": 7038,\n",
    "    \"warmup_proportion\": 0.2843,\n",
    "    \"num_steps_per_checkpoint\": 200,\n",
    "    \"learning_rate\": 6e-3,\n",
    "    \"seed\": 12439,\n",
    "    \"steps_this_run\": 500,\n",
    "    \"allreduce_post_accumulation\": 1,\n",
    "    \"allreduce_post_accumulation_fp16\": 1,\n",
    "    \"do_train\": 1,\n",
    "    \"use_sequential\": 1,\n",
    "    \"skip_checkpoint\": 1,\n",
    "    \"smp\": 1,\n",
    "    \"apply_optimizer\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-viking",
   "metadata": {
    "papermill": {
     "duration": 0.01221,
     "end_time": "2021-06-07T00:10:27.047662",
     "exception": false,
     "start_time": "2021-06-07T00:10:27.035452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Instantiate Pytorch Estimator with SMP enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loved-david",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:27.077463Z",
     "iopub.status.busy": "2021-06-07T00:10:27.076957Z",
     "iopub.status.idle": "2021-06-07T00:10:27.079186Z",
     "shell.execute_reply": "2021-06-07T00:10:27.078800Z"
    },
    "papermill": {
     "duration": 0.01935,
     "end_time": "2021-06-07T00:10:27.079298",
     "exception": false,
     "start_time": "2021-06-07T00:10:27.059948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(\n",
    "    \"sagemaker_smp_pretrain.py\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.p3.16xlarge\",\n",
    "    volume_size=200,\n",
    "    instance_count=1,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    py_version=\"py36\",\n",
    "    framework_version=\"1.6.0\",\n",
    "    distribution={\n",
    "        \"smdistributed\": {\"modelparallel\": {\"enabled\": True, \"parameters\": smp_parameters}},\n",
    "        \"mpi\": {\n",
    "            \"enabled\": True,\n",
    "            \"processes_per_host\": 1,\n",
    "            \"custom_mpi_options\": mpi_options,\n",
    "        },\n",
    "    },\n",
    "    source_dir=\"bert_example\",\n",
    "    output_path=s3_output_location,\n",
    "    max_run=timeout,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-network",
   "metadata": {
    "papermill": {
     "duration": 0.012369,
     "end_time": "2021-06-07T00:10:27.104154",
     "exception": false,
     "start_time": "2021-06-07T00:10:27.091785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, you will use the estimator to launch the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "endless-findings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-07 19:57:00 Starting - Starting the training job...\n",
      "2021-06-07 19:57:27 Starting - Launching requested ML instancesProfilerReport-1623095820: InProgress\n",
      ".........\n",
      "2021-06-07 19:58:48 Starting - Preparing the instances for training.........\n",
      "2021-06-07 20:00:29 Downloading - Downloading input data...\n",
      "2021-06-07 20:00:52 Training - Downloading the training image............\n",
      "2021-06-07 20:03:02 Uploading - Uploading generated training model\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:51,920 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:51,999 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:55,044 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:55,715 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:55,715 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:55,718 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:55,718 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1'] process_per_hosts: 1 num_processes: 1\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:55,722 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:55,803 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 1,\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-verbose --mca orte_base_help_aggregate 0 \",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_instance_type\": \"ml.p3.16xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_steps\": 7038,\n",
      "        \"seed\": 12439,\n",
      "        \"steps_this_run\": 500,\n",
      "        \"allreduce_post_accumulation_fp16\": 1,\n",
      "        \"allreduce_post_accumulation\": 1,\n",
      "        \"apply_optimizer\": 1,\n",
      "        \"use_sequential\": 1,\n",
      "        \"smp\": 1,\n",
      "        \"output_dir\": \"./checkpoints\",\n",
      "        \"max_seq_length\": 128,\n",
      "        \"input_dir\": \"/opt/ml/input/data/train\",\n",
      "        \"num_steps_per_checkpoint\": 200,\n",
      "        \"config_file\": \"bert_config.json\",\n",
      "        \"skip_checkpoint\": 1,\n",
      "        \"bert_model\": \"bert-large-uncased\",\n",
      "        \"do_train\": 1,\n",
      "        \"warmup_proportion\": 0.2843,\n",
      "        \"train_batch_size\": 48,\n",
      "        \"learning_rate\": 0.006,\n",
      "        \"mp_parameters\": {\n",
      "            \"optimize\": \"speed\",\n",
      "            \"microbatches\": 12,\n",
      "            \"partitions\": 2,\n",
      "            \"ddp\": true,\n",
      "            \"pipeline\": \"interleaved\",\n",
      "            \"overlapping_allreduce\": true,\n",
      "            \"placement_strategy\": \"cluster\",\n",
      "            \"memory_weight\": 0.3\n",
      "        },\n",
      "        \"max_predictions_per_seq\": 20\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-06-07-19-57-00-060\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-688520471316/pytorch-training-2021-06-07-19-57-00-060/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sagemaker_smp_pretrain\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sagemaker_smp_pretrain.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sagemaker_smp_pretrain.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose --mca orte_base_help_aggregate 0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":1}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sagemaker_smp_pretrain\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-688520471316/pytorch-training-2021-06-07-19-57-00-060/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p3.16xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-verbose --mca orte_base_help_aggregate 0 \",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":1},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"allreduce_post_accumulation\":1,\"allreduce_post_accumulation_fp16\":1,\"apply_optimizer\":1,\"bert_model\":\"bert-large-uncased\",\"config_file\":\"bert_config.json\",\"do_train\":1,\"input_dir\":\"/opt/ml/input/data/train\",\"learning_rate\":0.006,\"max_predictions_per_seq\":20,\"max_seq_length\":128,\"max_steps\":7038,\"mp_parameters\":{\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"},\"num_steps_per_checkpoint\":200,\"output_dir\":\"./checkpoints\",\"seed\":12439,\"skip_checkpoint\":1,\"smp\":1,\"steps_this_run\":500,\"train_batch_size\":48,\"use_sequential\":1,\"warmup_proportion\":0.2843},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-06-07-19-57-00-060\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-688520471316/pytorch-training-2021-06-07-19-57-00-060/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_smp_pretrain\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sagemaker_smp_pretrain.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--allreduce_post_accumulation\",\"1\",\"--allreduce_post_accumulation_fp16\",\"1\",\"--apply_optimizer\",\"1\",\"--bert_model\",\"bert-large-uncased\",\"--config_file\",\"bert_config.json\",\"--do_train\",\"1\",\"--input_dir\",\"/opt/ml/input/data/train\",\"--learning_rate\",\"0.006\",\"--max_predictions_per_seq\",\"20\",\"--max_seq_length\",\"128\",\"--max_steps\",\"7038\",\"--mp_parameters\",\"ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster\",\"--num_steps_per_checkpoint\",\"200\",\"--output_dir\",\"./checkpoints\",\"--seed\",\"12439\",\"--skip_checkpoint\",\"1\",\"--smp\",\"1\",\"--steps_this_run\",\"500\",\"--train_batch_size\",\"48\",\"--use_sequential\",\"1\",\"--warmup_proportion\",\"0.2843\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=7038\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=12439\u001b[0m\n",
      "\u001b[34mSM_HP_STEPS_THIS_RUN=500\u001b[0m\n",
      "\u001b[34mSM_HP_ALLREDUCE_POST_ACCUMULATION_FP16=1\u001b[0m\n",
      "\u001b[34mSM_HP_ALLREDUCE_POST_ACCUMULATION=1\u001b[0m\n",
      "\u001b[34mSM_HP_APPLY_OPTIMIZER=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE_SEQUENTIAL=1\u001b[0m\n",
      "\u001b[34mSM_HP_SMP=1\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=./checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIR=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_STEPS_PER_CHECKPOINT=200\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG_FILE=bert_config.json\u001b[0m\n",
      "\u001b[34mSM_HP_SKIP_CHECKPOINT=1\u001b[0m\n",
      "\u001b[34mSM_HP_BERT_MODEL=bert-large-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=1\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP_PROPORTION=0.2843\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=48\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.006\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"memory_weight\":0.3,\"microbatches\":12,\"optimize\":\"speed\",\"overlapping_allreduce\":true,\"partitions\":2,\"pipeline\":\"interleaved\",\"placement_strategy\":\"cluster\"}\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_PREDICTIONS_PER_SEQ=20\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 1 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose --mca orte_base_help_aggregate 0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_HP_MAX_STEPS -x SM_HP_SEED -x SM_HP_STEPS_THIS_RUN -x SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16 -x SM_HP_ALLREDUCE_POST_ACCUMULATION -x SM_HP_APPLY_OPTIMIZER -x SM_HP_USE_SEQUENTIAL -x SM_HP_SMP -x SM_HP_OUTPUT_DIR -x SM_HP_MAX_SEQ_LENGTH -x SM_HP_INPUT_DIR -x SM_HP_NUM_STEPS_PER_CHECKPOINT -x SM_HP_CONFIG_FILE -x SM_HP_SKIP_CHECKPOINT -x SM_HP_BERT_MODEL -x SM_HP_DO_TRAIN -x SM_HP_WARMUP_PROPORTION -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_LEARNING_RATE -x SM_HP_MP_PARAMETERS -x SM_HP_MAX_PREDICTIONS_PER_SEQ -x PYTHONPATH /opt/conda/bin/python3.6 -m mpi4py sagemaker_smp_pretrain.py --allreduce_post_accumulation 1 --allreduce_post_accumulation_fp16 1 --apply_optimizer 1 --bert_model bert-large-uncased --config_file bert_config.json --do_train 1 --input_dir /opt/ml/input/data/train --learning_rate 0.006 --max_predictions_per_seq 20 --max_seq_length 128 --max_steps 7038 --mp_parameters ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster --num_steps_per_checkpoint 200 --output_dir ./checkpoints --seed 12439 --skip_checkpoint 1 --smp 1 --steps_this_run 500 --train_batch_size 48 --use_sequential 1 --warmup_proportion 0.2843\n",
      "\n",
      "\n",
      " Data for JOB [41213,1] offset 0 Total slots allocated 1\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: algo-1#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [41213,1] App: 0 Process rank: 0 Bound: N/A\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 38, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    import amp_C\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:ImportError: libc10.so: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "\n",
      "  Process name: [[41213,1],0]\n",
      "  Exit code:    1\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\n",
      "\u001b[0m\n",
      "\u001b[34m2021-06-07 20:02:58,124 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1 -np 1 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose --mca orte_base_help_aggregate 0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN -x SM_HP_MAX_STEPS -x SM_HP_SEED -x SM_HP_STEPS_THIS_RUN -x SM_HP_ALLREDUCE_POST_ACCUMULATION_FP16 -x SM_HP_ALLREDUCE_POST_ACCUMULATION -x SM_HP_APPLY_OPTIMIZER -x SM_HP_USE_SEQUENTIAL -x SM_HP_SMP -x SM_HP_OUTPUT_DIR -x SM_HP_MAX_SEQ_LENGTH -x SM_HP_INPUT_DIR -x SM_HP_NUM_STEPS_PER_CHECKPOINT -x SM_HP_CONFIG_FILE -x SM_HP_SKIP_CHECKPOINT -x SM_HP_BERT_MODEL -x SM_HP_DO_TRAIN -x SM_HP_WARMUP_PROPORTION -x SM_HP_TRAIN_BATCH_SIZE -x SM_HP_LEARNING_RATE -x SM_HP_MP_PARAMETERS -x SM_HP_MAX_PREDICTIONS_PER_SEQ -x PYTHONPATH /opt/conda/bin/python3.6 -m mpi4py sagemaker_smp_pretrain.py --allreduce_post_accumulation 1 --allreduce_post_accumulation_fp16 1 --apply_optimizer 1 --bert_model bert-large-uncased --config_file bert_config.json --do_train 1 --input_dir /opt/ml/input/data/train --learning_rate 0.006 --max_predictions_per_seq 20 --max_seq_length 128 --max_steps 7038 --mp_parameters ddp=True,memory_weight=0.3,microbatches=12,optimize=speed,overlapping_allreduce=True,partitions=2,pipeline=interleaved,placement_strategy=cluster --num_steps_per_checkpoint 200 --output_dir ./checkpoints --seed 12439 --skip_checkpoint 1 --smp 1 --steps_this_run 500 --train_batch_size 48 --use_sequential 1 --warmup_proportion 0.2843\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    \"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/__main__.py\", line 7, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    main()\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 196, in main\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    run_command_line(args)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/site-packages/mpi4py/run.py\", line 47, in run_command_line\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    run_path(sys.argv[0], run_name='__main__')\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 263, in run_path\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    pkg_name=pkg_name, script_name=fname)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 96, in _run_module_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    mod_name, mod_spec, pkg_name, script_name)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    exec(code, run_globals)\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  File \"sagemaker_smp_pretrain.py\", line 38, in <module>\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:    import amp_C\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:ImportError: libc10.so: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "\n",
      "  Process name: [[41213,1],0]\n",
      "  Exit code:    1\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "2021-06-07 20:03:31 Failed - Training job failed\n",
      "ProfilerReport-1623095820: Stopping\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2021-06-07-19-57-00-060: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1 -np 1 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose --mca orte_base_help_aggregate 0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-72f3596ffe90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpytorch_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3681\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3682\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3683\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3243\u001b[0m                 ),\n\u001b[1;32m   3244\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3245\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3246\u001b[0m             )\n\u001b[1;32m   3247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2021-06-07-19-57-00-060: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"mpirun --host algo-1 -np 1 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.6/site-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -verbose --mca orte_base_help_aggregate 0 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit(data_channels, logs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 411.347006,
   "end_time": "2021-06-07T00:17:10.630083",
   "environment_variables": {},
   "exception": true,
   "input_path": "smp_bert_tutorial.ipynb",
   "output_path": "/opt/ml/processing/output/smp_bert_tutorial-2021-06-07-00-06-28.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-06-07T00:10:19.283077",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
