{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stunning-argument",
   "metadata": {
    "papermill": {
     "duration": 0.00915,
     "end_time": "2021-06-07T00:10:18.364817",
     "exception": false,
     "start_time": "2021-06-07T00:10:18.355667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Distributed data parallel MaskRCNN training with TensorFlow 2 and SageMaker distributed\n",
    "\n",
    "[Amazon SageMaker's distributed library](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html) can be used to train deep learning models faster and cheaper. The [data parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html) feature in this library (`smdistributed.dataparallel`) is a distributed data parallel training framework for PyTorch, TensorFlow, and MXNet.\n",
    "\n",
    "This notebook example shows how to use `smdistributed.dataparallel` with TensorFlow (version 2.4.1) on [Amazon SageMaker](https://aws.amazon.com/sagemaker/) to train a MaskRCNN model on [COCO 2017 dataset](https://cocodataset.org/#home) using [Amazon FSx for Lustre file-system](https://aws.amazon.com/fsx/lustre/) as data source.\n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "1. Stage COCO 2017 dataset in [Amazon S3](https://aws.amazon.com/s3/)\n",
    "2. Create Amazon FSx Lustre file-system and import data into the file-system from S3\n",
    "3. Build Docker training image and push it to [Amazon ECR](https://aws.amazon.com/ecr/)\n",
    "4. Configure data input channels for SageMaker\n",
    "5. Configure hyper-prarameters\n",
    "6. Define training metrics\n",
    "7. Define training job, set distribution strategy to `smdistributed.dataparallel` and start training\n",
    "\n",
    "**NOTE:**  With large traning dataset, we recommend using [Amazon FSx](https://aws.amazon.com/fsx/) as the input filesystem for the SageMaker training job. FSx file input to SageMaker significantly cuts down training start up time on SageMaker because it avoids downloading the training data each time you start the training job (as done with S3 input for SageMaker training job) and provides good data read throughput.\n",
    "\n",
    "\n",
    "**NOTE:** This example requires SageMaker Python SDK v2.X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-humanitarian",
   "metadata": {
    "papermill": {
     "duration": 0.008946,
     "end_time": "2021-06-07T00:10:18.382709",
     "exception": false,
     "start_time": "2021-06-07T00:10:18.373763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Amazon SageMaker Initialization\n",
    "\n",
    "Initialize the notebook instance. Get the AWS Region and a SageMaker execution role.\n",
    "\n",
    "### SageMaker role\n",
    "\n",
    "The following code cell defines `role` which is the IAM role ARN used to create and run SageMaker training and hosting jobs. This is the same IAM role used to create this SageMaker Notebook instance. \n",
    "\n",
    "`role` must have permission to create a SageMaker training job and host a model. For granular policies you can use to grant these permissions, see [Amazon SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). If you do not require fine-tuned permissions for this demo, you can used the IAM managed policy AmazonSageMakerFullAccess to complete this demo. \n",
    "\n",
    "As described above, since we will be using FSx, please make sure to attach `FSx Access` permission to this IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gentle-system",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:18.406039Z",
     "iopub.status.busy": "2021-06-07T00:10:18.405543Z",
     "iopub.status.idle": "2021-06-07T00:10:23.137422Z",
     "shell.execute_reply": "2021-06-07T00:10:23.137825Z"
    },
    "papermill": {
     "duration": 4.746268,
     "end_time": "2021-06-07T00:10:23.137966",
     "exception": false,
     "start_time": "2021-06-07T00:10:18.391698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /usr/local/lib/python3.7/site-packages (2.42.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.44.0.tar.gz (414 kB)\n",
      "\u001b[K     |████████████████████████████████| 414 kB 8.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /usr/local/lib/python3.7/site-packages (from sagemaker) (1.17.11)\n",
      "Requirement already satisfied: google-pasta in /usr/local/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/site-packages (from sagemaker) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /usr/local/lib/python3.7/site-packages (from sagemaker) (3.14.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /usr/local/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /usr/local/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /usr/local/lib/python3.7/site-packages (from sagemaker) (3.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from sagemaker) (20.9)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from sagemaker) (0.24.2)\n",
      "Requirement already satisfied: pathos in /usr/local/lib/python3.7/site-packages (from sagemaker) (0.2.7)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.11 in /usr/local/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (1.20.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.11->boto3>=1.16.32->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.11->boto3>=1.16.32->sagemaker) (1.25.11)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/site-packages (from protobuf>=3.1->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.7/site-packages (from pandas->sagemaker) (2021.1)\n",
      "Requirement already satisfied: dill>=0.3.3 in /usr/local/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.11 in /usr/local/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.11.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /usr/local/lib/python3.7/site-packages (from pathos->sagemaker) (1.6.6.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /usr/local/lib/python3.7/site-packages (from pathos->sagemaker) (0.2.9)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.44.0-py2.py3-none-any.whl size=580907 sha256=004d6f9c69eb3b0640391b66838cd464e2583b01458cb5121ce669976c84d396\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/10/83/14cb2db6b3e1dd5ad065911db82b98c6e5cb41103816cbe392\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.42.0\n",
      "    Uninstalling sagemaker-2.42.0:\n",
      "      Successfully uninstalled sagemaker-2.42.0\n",
      "Successfully installed sagemaker-2.44.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "SageMaker Execution Role:arn:aws:iam::521695447989:role/ProdBuildSystemStack-ReleaseBuildRoleFB326D49-QK8LUA2UI1IC\n",
      "AWS account:521695447989\n",
      "AWS region:us-west-2\n",
      "CPU times: user 876 ms, sys: 305 ms, total: 1.18 s\n",
      "Wall time: 4.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! python3 -m pip install --upgrade sagemaker\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = (\n",
    "    get_execution_role()\n",
    ")  # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f\"SageMaker Execution Role:{role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account:{account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region:{region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-position",
   "metadata": {
    "papermill": {
     "duration": 0.01297,
     "end_time": "2021-06-07T00:10:23.164513",
     "exception": false,
     "start_time": "2021-06-07T00:10:23.151543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To verify that the role above has required permissions:\n",
    "\n",
    "1. Go to the IAM console: https://console.aws.amazon.com/iam/home.\n",
    "2. Select **Roles**.\n",
    "3. Enter the role name in the search box to search for that role. \n",
    "4. Select the role.\n",
    "5. Use the **Permissions** tab to verify this role has required permissions attached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-hunger",
   "metadata": {
    "papermill": {
     "duration": 0.012927,
     "end_time": "2021-06-07T00:10:23.190447",
     "exception": false,
     "start_time": "2021-06-07T00:10:23.177520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare SageMaker Training Images\n",
    "\n",
    "1. SageMaker by default use the latest [Amazon Deep Learning Container Images (DLC)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) TensorFlow training image. In this step, we use it as a base image and install additional dependencies required for training MaskRCNN model.\n",
    "2. In the Github repository https://github.com/HerringForks/DeepLearningExamples.git we have made `smdistributed.dataparallel` TensorFlow  MaskRCNN training script available for your use. We will be installing the same on the training image.\n",
    "\n",
    "### Build and Push Docker Image to ECR\n",
    "\n",
    "Run the below command build the docker image and push it to ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powerful-dividend",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:23.219666Z",
     "iopub.status.busy": "2021-06-07T00:10:23.219188Z",
     "iopub.status.idle": "2021-06-07T00:10:23.221609Z",
     "shell.execute_reply": "2021-06-07T00:10:23.221108Z"
    },
    "papermill": {
     "duration": 0.018248,
     "end_time": "2021-06-07T00:10:23.221708",
     "exception": false,
     "start_time": "2021-06-07T00:10:23.203460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = \"<IMAGE_NAME>\"  # Example: tf2-mask-rcnn-smd-dataparallel-sagemaker\n",
    "tag = \"<IMAGE_TAG>\"  # Example: latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "white-raise",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:23.250852Z",
     "iopub.status.busy": "2021-06-07T00:10:23.250390Z",
     "iopub.status.idle": "2021-06-07T00:10:23.978769Z",
     "shell.execute_reply": "2021-06-07T00:10:23.978273Z"
    },
    "papermill": {
     "duration": 0.744149,
     "end_time": "2021-06-07T00:10:23.978892",
     "exception": false,
     "start_time": "2021-06-07T00:10:23.234743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mARG\u001b[39;49;00m region\n",
      "\n",
      "\u001b[34mFROM\u001b[39;49;00m \u001b[33m763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.4.1-gpu-py37-cu110-ubuntu18.04\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m \tpip --no-cache-dir --no-cache install \u001b[33m\\\u001b[39;49;00m\n",
      "        Cython \u001b[33m\\\u001b[39;49;00m\n",
      "        matplotlib \u001b[33m\\\u001b[39;49;00m\n",
      "        opencv-python-headless \u001b[33m\\\u001b[39;49;00m\n",
      "        mpi4py \u001b[33m\\\u001b[39;49;00m\n",
      "        Pillow \u001b[33m\\\u001b[39;49;00m\n",
      "        pytest \u001b[33m\\\u001b[39;49;00m\n",
      "        pyyaml\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m \t\u001b[36mcd\u001b[39;49;00m /root && \u001b[33m\\\u001b[39;49;00m\n",
      "\tgit clone https://github.com/pybind/pybind11 && \u001b[33m\\\u001b[39;49;00m\n",
      "\t\u001b[36mcd\u001b[39;49;00m pybind11 && \u001b[33m\\\u001b[39;49;00m\n",
      "\tcmake . && \u001b[33m\\\u001b[39;49;00m\n",
      "\tmake -j96 install && \u001b[33m\\\u001b[39;49;00m\n",
      "\tpip install .\n",
      "\n",
      "\u001b[34mRUN\u001b[39;49;00m \tpip --no-cache-dir --no-cache install \u001b[33m\\\u001b[39;49;00m\n",
      "    \t\u001b[33m'git+https://github.com/NVIDIA/cocoapi#egg=pycocotools&subdirectory=PythonAPI'\u001b[39;49;00m && \u001b[33m\\\u001b[39;49;00m\n",
      "\tpip --no-cache-dir --no-cache install \u001b[33m\\\u001b[39;49;00m\n",
      "    \t\u001b[33m'git+https://github.com/NVIDIA/dllogger'\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stainless-mozambique",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:24.010909Z",
     "iopub.status.busy": "2021-06-07T00:10:24.010251Z",
     "iopub.status.idle": "2021-06-07T00:10:24.705102Z",
     "shell.execute_reply": "2021-06-07T00:10:24.705551Z"
    },
    "papermill": {
     "duration": 0.712754,
     "end_time": "2021-06-07T00:10:24.705700",
     "exception": false,
     "start_time": "2021-06-07T00:10:23.992946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/usr/bin/env bash\u001b[39;49;00m\n",
      "\u001b[37m# This script shows how to build the Docker image and push it to ECR to be ready for use\u001b[39;49;00m\n",
      "\u001b[37m# by SageMaker.\u001b[39;49;00m\n",
      "\u001b[37m# The argument to this script is the image name. This will be used as the image on the local\u001b[39;49;00m\n",
      "\u001b[37m# machine and combined with the account and region to form the repository name for ECR.\u001b[39;49;00m\n",
      "\u001b[37m# set region\u001b[39;49;00m\n",
      "\n",
      "\u001b[31mDIR\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[34m$(\u001b[39;49;00m \u001b[36mcd\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[34m$(\u001b[39;49;00m dirname \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mBASH_SOURCE\u001b[39;49;00m[0]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m && \u001b[36mpwd\u001b[39;49;00m \u001b[34m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[33m\"\u001b[39;49;00m\u001b[31m$#\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m -eq \u001b[34m3\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "    \u001b[31mregion\u001b[39;49;00m=\u001b[31m$1\u001b[39;49;00m\n",
      "    \u001b[31mimage\u001b[39;49;00m=\u001b[31m$2\u001b[39;49;00m\n",
      "    \u001b[31mtag\u001b[39;49;00m=\u001b[31m$3\u001b[39;49;00m\n",
      "\u001b[34melse\u001b[39;49;00m\n",
      "    \u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33musage: \u001b[39;49;00m\u001b[31m$0\u001b[39;49;00m\u001b[33m <aws-region> \u001b[39;49;00m\u001b[31m$1\u001b[39;49;00m\u001b[33m <image-repo> \u001b[39;49;00m\u001b[31m$2\u001b[39;49;00m\u001b[33m <image-tag>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Get the account number associated with the current IAM credentials\u001b[39;49;00m\n",
      "\u001b[31maccount\u001b[39;49;00m=\u001b[34m$(\u001b[39;49;00maws sts get-caller-identity --query Account --output text\u001b[34m)\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]\n",
      "\u001b[34mthen\u001b[39;49;00m\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m255\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[31mfullname\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31maccount\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.dkr.ecr.\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.amazonaws.com/\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mtag\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# If the repository doesn't exist in ECR, create it.\u001b[39;49;00m\n",
      "aws ecr describe-repositories --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --repository-names \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null \u001b[34m2\u001b[39;49;00m>&\u001b[34m1\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "    \u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcreating ECR repository : \u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    aws ecr create-repository --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --repository-name \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null\n",
      "\u001b[34mfi\u001b[39;49;00m\n",
      "\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --no-include-email --region us-west-2  --registry-ids \u001b[34m763104351884\u001b[39;49;00m\u001b[34m)\u001b[39;49;00m\n",
      "docker build \u001b[33m${\u001b[39;49;00m\u001b[31mDIR\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m/ -t \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m -f \u001b[33m${\u001b[39;49;00m\u001b[31mDIR\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m/Dockerfile  --build-arg \u001b[31mregion\u001b[39;49;00m=\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\n",
      "docker tag \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Get the login command from ECR and execute it directly\u001b[39;49;00m\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --no-include-email\u001b[34m)\u001b[39;49;00m\n",
      "docker push \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -eq \u001b[34m0\u001b[39;49;00m ]; \u001b[34mthen\u001b[39;49;00m\n",
      "\t\u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon ECR URI: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[34melse\u001b[39;49;00m\n",
      "\t\u001b[36mecho\u001b[39;49;00m \u001b[33m\"Error: Image build and push failed\"\u001b[39;49;00m\n",
      "\t\u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\n",
      "\u001b[34mfi\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "warming-asset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:24.739585Z",
     "iopub.status.busy": "2021-06-07T00:10:24.738890Z",
     "iopub.status.idle": "2021-06-07T00:10:24.886520Z",
     "shell.execute_reply": "2021-06-07T00:10:24.886089Z"
    },
    "papermill": {
     "duration": 0.166294,
     "end_time": "2021-06-07T00:10:24.886633",
     "exception": false,
     "start_time": "2021-06-07T00:10:24.720339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: Syntax error: redirection unexpected\n",
      "CPU times: user 1.67 ms, sys: 4.53 ms, total: 6.2 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! chmod +x build_and_push.sh; bash build_and_push.sh {region} {image} {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-progressive",
   "metadata": {
    "papermill": {
     "duration": 0.016132,
     "end_time": "2021-06-07T00:10:24.917831",
     "exception": false,
     "start_time": "2021-06-07T00:10:24.901699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing FSx Input for SageMaker\n",
    "\n",
    "1. Download and prepare your training dataset on S3.\n",
    "2. Follow the steps listed here to create a FSx linked with your S3 bucket with training data - https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-fs-linked-data-repo.html. Make sure to add an endpoint to your VPC allowing S3 access.\n",
    "3. Follow the steps listed here to configure your SageMaker training job to use FSx https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/\n",
    "\n",
    "### Important Caveats\n",
    "\n",
    "1. You need use the same `subnet` and `vpc` and `security group` used with FSx when launching the SageMaker notebook instance. The same configurations will be used by your SageMaker training job.\n",
    "2. Make sure you set appropriate inbound/output rules in the `security group`. Specically, opening up these ports is necessary for SageMaker to access the FSx filesystem in the training job. https://docs.aws.amazon.com/fsx/latest/LustreGuide/limit-access-security-groups.html\n",
    "3. Make sure `SageMaker IAM Role` used to launch this SageMaker training job has access to `AmazonFSx`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-gothic",
   "metadata": {
    "papermill": {
     "duration": 0.014939,
     "end_time": "2021-06-07T00:10:24.948032",
     "exception": false,
     "start_time": "2021-06-07T00:10:24.933093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SageMaker TensorFlow Estimator function options\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distrubtion strategy. You're also passing in the training script you reviewed in the previous cell.\n",
    "\n",
    "**Instance types**\n",
    "\n",
    "`smdistributed.dataparallel` supports model training on SageMaker with the following instance types only. For best performance, it is recommended you use an instance type that supports Amazon Elastic Fabric Adapter (ml.p3dn.24xlarge and ml.p4d.24xlarge).\n",
    "\n",
    "1. ml.p3.16xlarge\n",
    "1. ml.p3dn.24xlarge [Recommended]\n",
    "1. ml.p4d.24xlarge [Recommended]\n",
    "\n",
    "**Instance count**\n",
    "\n",
    "To get the best performance and the most out of `smdistributed.dataparallel`, you should use at least 2 instances, but you can also use 1 for testing this example.\n",
    "\n",
    "**Distribution strategy**\n",
    "\n",
    "Note that to use DDP mode, you update the the `distribution` strategy, and set it to use `smdistributed dataparallel`.\n",
    "\n",
    "### Training script\n",
    "\n",
    "In the Github repository https://github.com/HerringForks/DeepLearningExamples.git we have made reference `smdistributed.dataparallel` TensorFlow MaskRCNN training script available for your use. Clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amateur-corpus",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:24.982342Z",
     "iopub.status.busy": "2021-06-07T00:10:24.981866Z",
     "iopub.status.idle": "2021-06-07T00:10:29.203463Z",
     "shell.execute_reply": "2021-06-07T00:10:29.203878Z"
    },
    "papermill": {
     "duration": 4.241109,
     "end_time": "2021-06-07T00:10:29.204024",
     "exception": false,
     "start_time": "2021-06-07T00:10:24.962915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearningExamples'...\n",
      "remote: Enumerating objects: 23187, done.\u001b[K\n",
      "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
      "remote: Total 23187 (delta 35), reused 59 (delta 17), pack-reused 23090\u001b[K\n",
      "Receiving objects: 100% (23187/23187), 57.83 MiB | 27.48 MiB/s, done.\n",
      "Resolving deltas: 100% (17854/17854), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone smdistributed.dataparallel fork repository for reference implementation of H\n",
    "!rm -rf DeepLearningExamples\n",
    "!git clone --recursive https://github.com/HerringForks/DeepLearningExamples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "level-above",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:29.254105Z",
     "iopub.status.busy": "2021-06-07T00:10:29.253612Z",
     "iopub.status.idle": "2021-06-07T00:10:29.260969Z",
     "shell.execute_reply": "2021-06-07T00:10:29.261394Z"
    },
    "papermill": {
     "duration": 0.03446,
     "end_time": "2021-06-07T00:10:29.261530",
     "exception": false,
     "start_time": "2021-06-07T00:10:29.227070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "senior-duncan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:29.312337Z",
     "iopub.status.busy": "2021-06-07T00:10:29.311693Z",
     "iopub.status.idle": "2021-06-07T00:10:29.313999Z",
     "shell.execute_reply": "2021-06-07T00:10:29.313589Z"
    },
    "papermill": {
     "duration": 0.029838,
     "end_time": "2021-06-07T00:10:29.314101",
     "exception": false,
     "start_time": "2021-06-07T00:10:29.284263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3dn.24xlarge\"  # Other supported instance type: ml.p3.16xlarge, ml.p4d.24xlarge\n",
    "instance_count = 2  # You can use 2, 4, 8 etc.\n",
    "docker_image = f\"{account}.dkr.ecr.{region}.amazonaws.com/{image}:{tag}\"  # YOUR_ECR_IMAGE_BUILT_WITH_ABOVE_DOCKER_FILE\n",
    "username = \"AWS\"\n",
    "subnets = [\"<SUBNET_ID>\"]  # Should be same as Subnet used for FSx. Example: subnet-0f9XXXX\n",
    "security_group_ids = [\n",
    "    \"<SECURITY_GROUP_ID>\"\n",
    "]  # Should be same as Security group used for FSx. sg-03ZZZZZZ\n",
    "job_name = \"tf2-smdataparallel-mrcnn-fsx\"  # This job name is used as prefix to the sagemaker training job. Makes it easy for your look for your training job in SageMaker Training job console.\n",
    "file_system_id = \"<FSX_ID>\"  # FSx file system ID with your training dataset. Example: 'fs-0bYYYYYY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collected-drove",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:29.364759Z",
     "iopub.status.busy": "2021-06-07T00:10:29.364270Z",
     "iopub.status.idle": "2021-06-07T00:10:29.366194Z",
     "shell.execute_reply": "2021-06-07T00:10:29.366575Z"
    },
    "papermill": {
     "duration": 0.029708,
     "end_time": "2021-06-07T00:10:29.366700",
     "exception": false,
     "start_time": "2021-06-07T00:10:29.336992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SM_DATA_ROOT = \"/opt/ml/input/data/train\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"mode\": \"train\",\n",
    "    \"checkpoint\": \"/\".join([SM_DATA_ROOT, \"model/resnet/resnet-nhwc-2018-02-07/model.ckpt-112603\"]),\n",
    "    \"eval_samples\": 5000,\n",
    "    \"init_learning_rate\": 0.04,\n",
    "    \"learning_rate_steps\": \"3750,5000\",\n",
    "    \"model_dir\": \"/opt/ml/code/checkpoints/tensorflow_mask_rcnn\",\n",
    "    \"num_steps_per_eval\": 462,\n",
    "    \"total_steps\": 500,\n",
    "    \"train_batch_size\": 4,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"training_file_pattern\": \"/\".join([SM_DATA_ROOT, \"train\"]),\n",
    "    \"validation_file_pattern\": \"/\".join([SM_DATA_ROOT, \"val\"]),\n",
    "    \"val_json_file\": \"/\".join([SM_DATA_ROOT, \"annotations/instances_val2017.json\"]),\n",
    "    \"amp\": \"\",\n",
    "    \"use_batched_nms\": \"\",\n",
    "    \"xla\": \"\",\n",
    "    \"nouse_custom_box_proposals_op\": \"\",\n",
    "    \"seed\": 987,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "equal-dominant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:29.417153Z",
     "iopub.status.busy": "2021-06-07T00:10:29.416539Z",
     "iopub.status.idle": "2021-06-07T00:10:29.418481Z",
     "shell.execute_reply": "2021-06-07T00:10:29.418863Z"
    },
    "papermill": {
     "duration": 0.029377,
     "end_time": "2021-06-07T00:10:29.418985",
     "exception": false,
     "start_time": "2021-06-07T00:10:29.389608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "    entry_point=\"DeepLearningExamples/TensorFlow2/Segmentation/MaskRCNN/mask_rcnn_sm.py\",\n",
    "    role=role,\n",
    "    image_uri=docker_image,\n",
    "    source_dir=\".\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    subnets=subnets,\n",
    "    hyperparameters=hyperparameters,\n",
    "    security_group_ids=security_group_ids,\n",
    "    debugger_hook_config=False,\n",
    "    # Training using smdistributed.dataparallel Distributed Training Framework\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "changed-recruitment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:29.468951Z",
     "iopub.status.busy": "2021-06-07T00:10:29.468316Z",
     "iopub.status.idle": "2021-06-07T00:10:29.470193Z",
     "shell.execute_reply": "2021-06-07T00:10:29.470576Z"
    },
    "papermill": {
     "duration": 0.028729,
     "end_time": "2021-06-07T00:10:29.470697",
     "exception": false,
     "start_time": "2021-06-07T00:10:29.441968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "file_system_directory_path = \"YOUR_MOUNT_PATH_FOR_TRAINING_DATA\"  # NOTE: '/fsx/' will be the root mount path. Example: '/fsx/mask_rcnn/PyTorch'\n",
    "file_system_access_mode = \"rw\"\n",
    "file_system_type = \"FSxLustre\"\n",
    "train_fs = FileSystemInput(\n",
    "    file_system_id=file_system_id,\n",
    "    file_system_type=file_system_type,\n",
    "    directory_path=file_system_directory_path,\n",
    "    file_system_access_mode=file_system_access_mode,\n",
    ")\n",
    "data_channels = {\"train\": train_fs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "historical-sword",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T00:10:29.519904Z",
     "iopub.status.busy": "2021-06-07T00:10:29.519432Z",
     "iopub.status.idle": "2021-06-07T00:10:48.619072Z",
     "shell.execute_reply": "2021-06-07T00:10:48.618286Z"
    },
    "papermill": {
     "duration": 19.125675,
     "end_time": "2021-06-07T00:10:48.619315",
     "exception": true,
     "start_time": "2021-06-07T00:10:29.493640",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid length for parameter InputDataConfig[0].DataSource.FileSystemDataSource.FileSystemId, value: 8, valid min length: 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-935a095db72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Submit SageMaker training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \"\"\"\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    647\u001b[0m         }\n\u001b[1;32m    648\u001b[0m         request_dict = self._convert_to_request_dict(\n\u001b[0;32m--> 649\u001b[0;31m             api_params, operation_model, context=request_context)\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    695\u001b[0m             api_params, operation_model, context)\n\u001b[1;32m    696\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[0;32m--> 697\u001b[0;31m             api_params, operation_model)\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_host_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0mrequest_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'host_prefix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/botocore/validate.py\u001b[0m in \u001b[0;36mserialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    291\u001b[0m                                                     operation_model.input_shape)\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParamValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         return self._serializer.serialize_to_request(parameters,\n\u001b[1;32m    295\u001b[0m                                                      operation_model)\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nInvalid length for parameter InputDataConfig[0].DataSource.FileSystemDataSource.FileSystemId, value: 8, valid min length: 11"
     ]
    }
   ],
   "source": [
    "# Submit SageMaker training job\n",
    "estimator.fit(inputs=data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-fiber",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "If you are a new user of Amazon SageMaker, you may find the following helpful to understand how SageMaker uses Docker to train custom models.\n",
    "* To learn more about using Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms\n",
    "](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "* To learn more about using Docker to train your own models with Amazon SageMaker, see [Example Notebooks: Use Your Own Algorithm or Model](https://docs.aws.amazon.com/sagemaker/latest/dg/adv-bring-own-examples.html).\n",
    "* To see other examples of distributed training using Amazon SageMaker and TensorFlow, see [Distributed TensorFlow training using Amazon SageMaker\n",
    "](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/distributed_tensorflow_mask_rcnn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.566989,
   "end_time": "2021-06-07T00:10:49.080604",
   "environment_variables": {},
   "exception": true,
   "input_path": "tensorflow2_smdataparallel_maskrcnn_demo.ipynb",
   "output_path": "/opt/ml/processing/output/tensorflow2_smdataparallel_maskrcnn_demo-2021-06-07-00-06-39.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-06-07T00:10:17.513615",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
