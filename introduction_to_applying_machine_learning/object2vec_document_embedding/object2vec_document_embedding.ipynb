{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "relative-liberal",
   "metadata": {
    "papermill": {
     "duration": 0.02257,
     "end_time": "2021-06-17T00:06:35.769006",
     "exception": false,
     "start_time": "2021-06-17T00:06:35.746436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Document Embedding with Amazon SageMaker Object2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-prototype",
   "metadata": {
    "papermill": {
     "duration": 0.023031,
     "end_time": "2021-06-17T00:06:35.813927",
     "exception": false,
     "start_time": "2021-06-17T00:06:35.790896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Background](#Background)\n",
    "  1. [Embedding documents using Object2Vec](#Embedding-documents-using-Object2Vec)\n",
    "3. [Download and preprocess Wikipedia data](#Download-and-preprocess-Wikipedia-data)\n",
    "  1. [Install and load dependencies](#Install-and-load-dependencies)\n",
    "  2. [Build vocabulary and tokenize datasets](#Build-vocabulary-and-tokenize-datasets)\n",
    "  3. [Upload preprocessed data to S3](#Upload-preprocessed-data-to-S3)\n",
    "4. [Define SageMaker session, Object2Vec image, S3 input and output paths](#Define-SageMaker-session,-Object2Vec-image,-S3-input-and-output-paths)\n",
    "5. [Train and deploy doc2vec](#Train-and-deploy-doc2vec)\n",
    "  1. [Learning performance boost with new features](#Learning-performance-boost-with-new-features)\n",
    "  2. [Training speedup with sparse gradient update](#Training-speedup-with-sparse-gradient-update)\n",
    "6. [Apply learned embeddings to document retrieval task](#Apply-learned-embeddings-to-document-retrieval-task)\n",
    "  1. [Comparison with the StarSpace algorithm](#Comparison-with-the-StarSpace-algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-wages",
   "metadata": {
    "papermill": {
     "duration": 0.021472,
     "end_time": "2021-06-17T00:06:35.858221",
     "exception": false,
     "start_time": "2021-06-17T00:06:35.836749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-middle",
   "metadata": {
    "papermill": {
     "duration": 0.021515,
     "end_time": "2021-06-17T00:06:35.901356",
     "exception": false,
     "start_time": "2021-06-17T00:06:35.879841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, we introduce four new features to Object2Vec, a general-purpose neural embedding algorithm: negative sampling, sparse gradient update, weight-sharing, and comparator operator customization. The new features together broaden the applicability of Object2Vec, improve its training speed and accuracy, and provide users with greater flexibility. See [Introduction to the Amazon SageMaker Object2Vec](https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/) if you arenâ€™t already familiar with Object2Vec.\n",
    "\n",
    "We demonstrate how these new features extend the applicability of Object2Vec to a new Document Embedding use-case: A customer has a large collection of documents. Instead of storing these documents in its raw format or as sparse bag-of-words vectors, to achieve training efficiency in the various downstream tasks, she would like to instead embed all documents in a common low-dimensional space, so that the semantic distance between these documents are preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-delicious",
   "metadata": {
    "papermill": {
     "duration": 0.021569,
     "end_time": "2021-06-17T00:06:35.944571",
     "exception": false,
     "start_time": "2021-06-17T00:06:35.923002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-local",
   "metadata": {
    "papermill": {
     "duration": 0.021898,
     "end_time": "2021-06-17T00:06:35.988098",
     "exception": false,
     "start_time": "2021-06-17T00:06:35.966200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Object2Vec is a highly customizable multi-purpose algorithm that can learn embeddings of pairs of objects. The embeddings are learned such that it preserves their pairwise similarities in the original space.\n",
    "\n",
    "- Similarity is user-defined: users need to provide the algorithm with pairs of objects that they define as similar (1) or dissimilar (0); alternatively, the users can define similarity in a continuous sense (provide a real-valued similarity score).\n",
    "\n",
    "- The learned embeddings can be used to efficiently compute nearest neighbors of objects, as well as to visualize natural clusters of related objects in the embedding space. In addition, the embeddings can also be used as features of the corresponding objects in downstream supervised tasks such as classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-going",
   "metadata": {
    "papermill": {
     "duration": 0.021616,
     "end_time": "2021-06-17T00:06:36.031255",
     "exception": false,
     "start_time": "2021-06-17T00:06:36.009639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Embedding documents using Object2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-yukon",
   "metadata": {
    "papermill": {
     "duration": 0.021474,
     "end_time": "2021-06-17T00:06:36.074427",
     "exception": false,
     "start_time": "2021-06-17T00:06:36.052953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We demonstrate how, with the new features, Object2Vec can be used to embed a large collection of documents into vectors in the same latent space.\n",
    "\n",
    "Similar to the widely used Word2Vec algorithm for word embedding, a natural approach to document embedding is to preprocess documents as (sentence, context) pairs, where the sentence and its matching context come from the same document. The matching context is the entire document with the given sentence removed. The idea is to embed both sentence and context into a low dimensional space such that their mutual similarity is maximized, since they belong to the same document and therefore should be semantically related. The learned encoder for the context can then be used to encode new documents into the same embedding space. In order to train the encoders for sentences and documents, we also need negative (sentence, context) pairs so that the model can learn to discriminate between semantically similar and dissimilar pairs. It is easy to generate such negatives by pairing sentences with documents that they do not belong to. Since there are many more negative pairs than positives in naturally occurring data, we typically resort to random sampling techniques to achieve a balance between positive and negative pairs in the training data. The figure below shows pictorially how the positive pairs and negative pairs are generated from unlabeled data for the purpose of learning embeddings for documents (and sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-consumer",
   "metadata": {
    "papermill": {
     "duration": 0.021459,
     "end_time": "2021-06-17T00:06:36.117503",
     "exception": false,
     "start_time": "2021-06-17T00:06:36.096044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"doc_embedding_illustration.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-snowboard",
   "metadata": {
    "papermill": {
     "duration": 0.021519,
     "end_time": "2021-06-17T00:06:36.160590",
     "exception": false,
     "start_time": "2021-06-17T00:06:36.139071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We show how Object2Vec with the new *negative sampling feature* can be applied to the document embedding use-case. In addition, we show how the other new features, namely, *weight-sharing*, *customization of comparator operator*, and *sparse gradient update*, together enhance the algorithm's performance and user-experience in and beyond this use-case. Sections [Learning performance boost with new features](#Learning-performance-boost-with-new-features) and [Training speedup with sparse gradient update](#Training-speedup-with-sparse-gradient-update) in this notebook provide a detailed introduction to the new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-confidentiality",
   "metadata": {
    "papermill": {
     "duration": 0.021346,
     "end_time": "2021-06-17T00:06:36.203791",
     "exception": false,
     "start_time": "2021-06-17T00:06:36.182445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Download and preprocess Wikipedia data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-savannah",
   "metadata": {
    "papermill": {
     "duration": 0.021797,
     "end_time": "2021-06-17T00:06:36.247097",
     "exception": false,
     "start_time": "2021-06-17T00:06:36.225300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Please be aware of the following requirements about the acknowledgment, copyright and availability, cited from the [data source description page](https://github.com/facebookresearch/StarSpace/blob/master/LICENSE.md).\n",
    "\n",
    "> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-porter",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:06:36.294364Z",
     "iopub.status.busy": "2021-06-17T00:06:36.293873Z",
     "iopub.status.idle": "2021-06-17T00:06:47.279184Z",
     "shell.execute_reply": "2021-06-17T00:06:47.279573Z"
    },
    "papermill": {
     "duration": 11.011024,
     "end_time": "2021-06-17T00:06:47.279762",
     "exception": false,
     "start_time": "2021-06-17T00:06:36.268738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wikipedia data\n",
      "wikipedia_train250k.txt\n",
      "wikipedia_dev10k.txt\n",
      "wikipedia_dev_basedocs.txt\n",
      "wikipedia_test_basedocs.txt\n",
      "wikipedia_test10k.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATANAME=\"wikipedia\"\n",
    "DATADIR=\"/tmp/wiki\"\n",
    "\n",
    "mkdir -p \"${DATADIR}\"\n",
    "\n",
    "if [ ! -f \"${DATADIR}/${DATANAME}_train250k.txt\" ]\n",
    "then\n",
    "    echo \"Downloading wikipedia data\"\n",
    "    wget --quiet -c \"https://dl.fbaipublicfiles.com/starspace/wikipedia_train250k.tgz\" -O \"${DATADIR}/${DATANAME}_train.tar.gz\"\n",
    "    tar -xzvf \"${DATADIR}/${DATANAME}_train.tar.gz\" -C \"${DATADIR}\"\n",
    "    wget --quiet -c \"https://dl.fbaipublicfiles.com/starspace/wikipedia_devtst.tgz\" -O \"${DATADIR}/${DATANAME}_test.tar.gz\"\n",
    "    tar -xzvf \"${DATADIR}/${DATANAME}_test.tar.gz\" -C \"${DATADIR}\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "typical-prime",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:06:47.329244Z",
     "iopub.status.busy": "2021-06-17T00:06:47.328753Z",
     "iopub.status.idle": "2021-06-17T00:06:47.330947Z",
     "shell.execute_reply": "2021-06-17T00:06:47.330480Z"
    },
    "papermill": {
     "duration": 0.02817,
     "end_time": "2021-06-17T00:06:47.331053",
     "exception": false,
     "start_time": "2021-06-17T00:06:47.302883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datadir = \"/tmp/wiki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oriented-spanking",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:06:47.379846Z",
     "iopub.status.busy": "2021-06-17T00:06:47.379383Z",
     "iopub.status.idle": "2021-06-17T00:06:47.523837Z",
     "shell.execute_reply": "2021-06-17T00:06:47.523379Z"
    },
    "papermill": {
     "duration": 0.1703,
     "end_time": "2021-06-17T00:06:47.523949",
     "exception": false,
     "start_time": "2021-06-17T00:06:47.353649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_dev10k.txt\t    wikipedia_test_basedocs.txt\n",
      "wikipedia_dev_basedocs.txt  wikipedia_train.tar.gz\n",
      "wikipedia_test.tar.gz\t    wikipedia_train250k.txt\n",
      "wikipedia_test10k.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-press",
   "metadata": {
    "papermill": {
     "duration": 0.022981,
     "end_time": "2021-06-17T00:06:47.569939",
     "exception": false,
     "start_time": "2021-06-17T00:06:47.546958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install and load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stuck-armenia",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:06:47.619742Z",
     "iopub.status.busy": "2021-06-17T00:06:47.619276Z",
     "iopub.status.idle": "2021-06-17T00:06:49.400903Z",
     "shell.execute_reply": "2021-06-17T00:06:49.401303Z"
    },
    "papermill": {
     "duration": 1.808764,
     "end_time": "2021-06-17T00:06:49.401467",
     "exception": false,
     "start_time": "2021-06-17T00:06:47.592703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Using cached jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "identified-wound",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:06:49.454663Z",
     "iopub.status.busy": "2021-06-17T00:06:49.454196Z",
     "iopub.status.idle": "2021-06-17T00:06:50.308486Z",
     "shell.execute_reply": "2021-06-17T00:06:50.308000Z"
    },
    "papermill": {
     "duration": 0.883049,
     "end_time": "2021-06-17T00:06:50.308603",
     "exception": false,
     "start_time": "2021-06-17T00:06:49.425554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note: please run on python 3 kernel\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import json, jsonlines\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from itertools import chain, islice\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "## sagemaker api\n",
    "import sagemaker, boto3\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.predictor import json_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-stretch",
   "metadata": {
    "papermill": {
     "duration": 0.023691,
     "end_time": "2021-06-17T00:06:50.356101",
     "exception": false,
     "start_time": "2021-06-17T00:06:50.332410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Build vocabulary and tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "educational-madagascar",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:06:50.417037Z",
     "iopub.status.busy": "2021-06-17T00:06:50.416368Z",
     "iopub.status.idle": "2021-06-17T00:06:50.418626Z",
     "shell.execute_reply": "2021-06-17T00:06:50.418237Z"
    },
    "papermill": {
     "duration": 0.038867,
     "end_time": "2021-06-17T00:06:50.418733",
     "exception": false,
     "start_time": "2021-06-17T00:06:50.379866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOS_SYMBOL = \"<s>\"\n",
    "EOS_SYMBOL = \"</s>\"\n",
    "UNK_SYMBOL = \"<unk>\"\n",
    "PAD_SYMBOL = \"<pad>\"\n",
    "PAD_ID = 0\n",
    "TOKEN_SEPARATOR = \" \"\n",
    "VOCAB_SYMBOLS = [PAD_SYMBOL, UNK_SYMBOL, BOS_SYMBOL, EOS_SYMBOL]\n",
    "\n",
    "\n",
    "##### utility functions for preprocessing\n",
    "def get_article_iter_from_file(fname):\n",
    "    with open(fname) as f:\n",
    "        for article in f:\n",
    "            yield article\n",
    "\n",
    "\n",
    "def get_article_iter_from_channel(channel, datadir=\"/tmp/wiki\"):\n",
    "    if channel == \"train\":\n",
    "        fname = os.path.join(datadir, \"wikipedia_train250k.txt\")\n",
    "        return get_article_iter_from_file(fname)\n",
    "    else:\n",
    "        iterlist = []\n",
    "        suffix_list = [\"train250k.txt\", \"test10k.txt\", \"dev10k.txt\", \"test_basedocs.txt\"]\n",
    "        for suffix in suffix_list:\n",
    "            fname = os.path.join(datadir, \"wikipedia_\" + suffix)\n",
    "            iterlist.append(get_article_iter_from_file(fname))\n",
    "        return chain.from_iterable(iterlist)\n",
    "\n",
    "\n",
    "def readlines_from_article(article):\n",
    "    return article.strip().split(\"\\t\")\n",
    "\n",
    "\n",
    "def sentence_to_integers(sentence, word_dict, trim_size=None):\n",
    "    \"\"\"\n",
    "    Converts a string of tokens to a list of integers\n",
    "    \"\"\"\n",
    "    if not trim_size:\n",
    "        return [\n",
    "            word_dict[token] if token in word_dict else 0\n",
    "            for token in get_tokens_from_sentence(sentence)\n",
    "        ]\n",
    "    else:\n",
    "        integer_list = []\n",
    "        for token in get_tokens_from_sentence(sentence):\n",
    "            if len(integer_list) < trim_size:\n",
    "                if token in word_dict:\n",
    "                    integer_list.append(word_dict[token])\n",
    "                else:\n",
    "                    integer_list.append(0)\n",
    "            else:\n",
    "                break\n",
    "        return integer_list\n",
    "\n",
    "\n",
    "def get_tokens_from_sentence(sent):\n",
    "    \"\"\"\n",
    "    Yields tokens from input string.\n",
    "\n",
    "    :param line: Input string.\n",
    "    :return: Iterator over tokens.\n",
    "    \"\"\"\n",
    "    for token in sent.split():\n",
    "        if len(token) > 0:\n",
    "            yield normalize_token(token)\n",
    "\n",
    "\n",
    "def get_tokens_from_article(article):\n",
    "    iterlist = []\n",
    "    for sent in readlines_from_article(article):\n",
    "        iterlist.append(get_tokens_from_sentence(sent))\n",
    "    return chain.from_iterable(iterlist)\n",
    "\n",
    "\n",
    "def normalize_token(token):\n",
    "    token = token.lower()\n",
    "    if all(s.isdigit() or s in string.punctuation for s in token):\n",
    "        tok = list(token)\n",
    "        for i in range(len(tok)):\n",
    "            if tok[i].isdigit():\n",
    "                tok[i] = \"0\"\n",
    "        token = \"\".join(tok)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "informative-sport",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:06:50.476184Z",
     "iopub.status.busy": "2021-06-17T00:06:50.475518Z",
     "iopub.status.idle": "2021-06-17T00:06:50.477387Z",
     "shell.execute_reply": "2021-06-17T00:06:50.477747Z"
    },
    "papermill": {
     "duration": 0.034082,
     "end_time": "2021-06-17T00:06:50.477878",
     "exception": false,
     "start_time": "2021-06-17T00:06:50.443796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to build vocabulary\n",
    "\n",
    "\n",
    "def build_vocab(channel, num_words=50000, min_count=1, use_reserved_symbols=True, sort=True):\n",
    "    \"\"\"\n",
    "    Creates a vocabulary mapping from words to ids. Increasing integer ids are assigned by word frequency,\n",
    "    using lexical sorting as a tie breaker. The only exception to this are special symbols such as the padding symbol\n",
    "    (PAD).\n",
    "\n",
    "    :param num_words: Maximum number of words in the vocabulary.\n",
    "    :param min_count: Minimum occurrences of words to be included in the vocabulary.\n",
    "    :return: word-to-id mapping.\n",
    "    \"\"\"\n",
    "    vocab_symbols_set = set(VOCAB_SYMBOLS)\n",
    "    raw_vocab = Counter()\n",
    "    for article in get_article_iter_from_channel(channel):\n",
    "        article_wise_vocab_list = list()\n",
    "        for token in get_tokens_from_article(article):\n",
    "            if token not in vocab_symbols_set:\n",
    "                article_wise_vocab_list.append(token)\n",
    "        raw_vocab.update(article_wise_vocab_list)\n",
    "\n",
    "    print(\"Initial vocabulary: {} types\".format(len(raw_vocab)))\n",
    "\n",
    "    # For words with the same count, they will be ordered reverse alphabetically.\n",
    "    # Not an issue since we only care for consistency\n",
    "    pruned_vocab = sorted(((c, w) for w, c in raw_vocab.items() if c >= min_count), reverse=True)\n",
    "    print(\"Pruned vocabulary: {} types (min frequency {})\".format(len(pruned_vocab), min_count))\n",
    "\n",
    "    # truncate the vocabulary to fit size num_words (only includes the most frequent ones)\n",
    "    vocab = islice((w for c, w in pruned_vocab), num_words)\n",
    "\n",
    "    if sort:\n",
    "        # sort the vocabulary alphabetically\n",
    "        vocab = sorted(vocab)\n",
    "    if use_reserved_symbols:\n",
    "        vocab = chain(VOCAB_SYMBOLS, vocab)\n",
    "\n",
    "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    print(\"Final vocabulary: {} types\".format(len(word_to_id)))\n",
    "\n",
    "    if use_reserved_symbols:\n",
    "        # Important: pad symbol becomes index 0\n",
    "        assert word_to_id[PAD_SYMBOL] == PAD_ID\n",
    "\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "increasing-bibliography",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:06:50.680061Z",
     "iopub.status.busy": "2021-06-17T00:06:50.531639Z",
     "iopub.status.idle": "2021-06-17T00:08:56.748250Z",
     "shell.execute_reply": "2021-06-17T00:08:56.747798Z"
    },
    "papermill": {
     "duration": 126.246465,
     "end_time": "2021-06-17T00:08:56.748370",
     "exception": false,
     "start_time": "2021-06-17T00:06:50.501905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary: 1257162 types\n",
      "Pruned vocabulary: 267518 types (min frequency 5)\n",
      "Final vocabulary: 267522 types\n"
     ]
    }
   ],
   "source": [
    "# build vocab dictionary\n",
    "\n",
    "\n",
    "def build_vocabulary_file(\n",
    "    vocab_fname,\n",
    "    channel,\n",
    "    num_words=50000,\n",
    "    min_count=1,\n",
    "    use_reserved_symbols=True,\n",
    "    sort=True,\n",
    "    force=False,\n",
    "):\n",
    "    if not os.path.exists(vocab_fname) or force:\n",
    "        w_dict = build_vocab(\n",
    "            channel, num_words=num_words, min_count=min_count, use_reserved_symbols=True, sort=True\n",
    "        )\n",
    "        with open(vocab_fname, \"w\") as write_file:\n",
    "            json.dump(w_dict, write_file)\n",
    "\n",
    "\n",
    "channel = \"train\"\n",
    "min_count = 5\n",
    "vocab_fname = os.path.join(datadir, \"wiki-vocab-{}250k-mincount-{}.json\".format(channel, min_count))\n",
    "\n",
    "build_vocabulary_file(vocab_fname, channel, num_words=500000, min_count=min_count, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "incomplete-mapping",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:08:56.803230Z",
     "iopub.status.busy": "2021-06-17T00:08:56.802268Z",
     "iopub.status.idle": "2021-06-17T00:08:56.933504Z",
     "shell.execute_reply": "2021-06-17T00:08:56.933863Z"
    },
    "papermill": {
     "duration": 0.160706,
     "end_time": "2021-06-17T00:08:56.934003",
     "exception": false,
     "start_time": "2021-06-17T00:08:56.773297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab file /tmp/wiki/wiki-vocab-train250k-mincount-5.json ...\n",
      "The vocabulary size is 267522\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading vocab file {} ...\".format(vocab_fname))\n",
    "\n",
    "with open(vocab_fname) as f:\n",
    "    w_dict = json.load(f)\n",
    "    print(\"The vocabulary size is {}\".format(len(w_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smaller-cornwall",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:08:56.990581Z",
     "iopub.status.busy": "2021-06-17T00:08:56.990126Z",
     "iopub.status.idle": "2021-06-17T00:08:56.991812Z",
     "shell.execute_reply": "2021-06-17T00:08:56.992227Z"
    },
    "papermill": {
     "duration": 0.033336,
     "end_time": "2021-06-17T00:08:56.992357",
     "exception": false,
     "start_time": "2021-06-17T00:08:56.959021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to build training data\n",
    "# Tokenize wiki articles to (sentence, document) pairs\n",
    "def generate_sent_article_pairs_from_single_article(article, word_dict):\n",
    "    sent_list = readlines_from_article(article)\n",
    "    art_len = len(sent_list)\n",
    "    idx = random.randint(0, art_len - 1)\n",
    "    wrapper_text_idx = list(range(idx)) + list(range((idx + 1) % art_len, art_len))\n",
    "    wrapper_text_list = sent_list[:idx] + sent_list[(idx + 1) % art_len : art_len]\n",
    "    wrapper_tokens = []\n",
    "    for sent1 in wrapper_text_list:\n",
    "        wrapper_tokens += sentence_to_integers(sent1, word_dict)\n",
    "    sent_tokens = sentence_to_integers(sent_list[idx], word_dict)\n",
    "    yield {\"in0\": sent_tokens, \"in1\": wrapper_tokens, \"label\": 1}\n",
    "\n",
    "\n",
    "def generate_sent_article_pairs_from_single_file(fname, word_dict):\n",
    "    with open(fname) as reader:\n",
    "        iter_list = []\n",
    "        for article in reader:\n",
    "            iter_list.append(generate_sent_article_pairs_from_single_article(article, word_dict))\n",
    "    return chain.from_iterable(iter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "improved-premises",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:08:57.047420Z",
     "iopub.status.busy": "2021-06-17T00:08:57.046731Z",
     "iopub.status.idle": "2021-06-17T00:11:24.372030Z",
     "shell.execute_reply": "2021-06-17T00:11:24.372473Z"
    },
    "papermill": {
     "duration": 147.355297,
     "end_time": "2021-06-17T00:11:24.372627",
     "exception": false,
     "start_time": "2021-06-17T00:08:57.017330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating train250k data of size 250000\n"
     ]
    }
   ],
   "source": [
    "# Build training data\n",
    "\n",
    "# Generate integer positive labeled data\n",
    "train_prefix = \"train250k\"\n",
    "fname = \"wikipedia_{}.txt\".format(train_prefix)\n",
    "outfname = os.path.join(datadir, \"{}_tokenized.jsonl\".format(train_prefix))\n",
    "counter = 0\n",
    "\n",
    "with jsonlines.open(outfname, \"w\") as writer:\n",
    "    for sample in generate_sent_article_pairs_from_single_file(\n",
    "        os.path.join(datadir, fname), w_dict\n",
    "    ):\n",
    "        writer.write(sample)\n",
    "        counter += 1\n",
    "\n",
    "print(\"Finished generating {} data of size {}\".format(train_prefix, counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "radio-lunch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:11:24.427743Z",
     "iopub.status.busy": "2021-06-17T00:11:24.427069Z",
     "iopub.status.idle": "2021-06-17T00:11:25.844523Z",
     "shell.execute_reply": "2021-06-17T00:11:25.844908Z"
    },
    "papermill": {
     "duration": 1.446939,
     "end_time": "2021-06-17T00:11:25.845052",
     "exception": false,
     "start_time": "2021-06-17T00:11:24.398113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle training data\n",
    "!shuf {outfname} > {train_prefix}_tokenized_shuf.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subtle-trustee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:11:25.905571Z",
     "iopub.status.busy": "2021-06-17T00:11:25.905095Z",
     "iopub.status.idle": "2021-06-17T00:11:25.907391Z",
     "shell.execute_reply": "2021-06-17T00:11:25.906988Z"
    },
    "papermill": {
     "duration": 0.036732,
     "end_time": "2021-06-17T00:11:25.907495",
     "exception": false,
     "start_time": "2021-06-17T00:11:25.870763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to generate dev/test data (with both positive and negative labels)\n",
    "\n",
    "\n",
    "def generate_pos_neg_samples_from_single_article(\n",
    "    word_dict, article_idx, article_buffer, negative_sampling_rate=1\n",
    "):\n",
    "    sample_list = []\n",
    "    # generate positive samples\n",
    "    sent_list = readlines_from_article(article_buffer[article_idx])\n",
    "    art_len = len(sent_list)\n",
    "    idx = random.randint(0, art_len - 1)\n",
    "    wrapper_text_idx = list(range(idx)) + list(range((idx + 1) % art_len, art_len))\n",
    "    wrapper_text_list = sent_list[:idx] + sent_list[(idx + 1) % art_len : art_len]\n",
    "    wrapper_tokens = []\n",
    "    for sent1 in wrapper_text_list:\n",
    "        wrapper_tokens += sentence_to_integers(sent1, word_dict)\n",
    "    sent_tokens = sentence_to_integers(sent_list[idx], word_dict)\n",
    "    sample_list.append({\"in0\": sent_tokens, \"in1\": wrapper_tokens, \"label\": 1})\n",
    "    # generate negative sample\n",
    "    buff_len = len(article_buffer)\n",
    "    sampled_inds = np.random.choice(\n",
    "        list(range(article_idx)) + list(range((article_idx + 1) % buff_len, buff_len)),\n",
    "        size=negative_sampling_rate,\n",
    "    )\n",
    "    for n_idx in sampled_inds:\n",
    "        other_article = article_buffer[n_idx]\n",
    "        context_list = readlines_from_article(other_article)\n",
    "        context_tokens = []\n",
    "        for sent2 in context_list:\n",
    "            context_tokens += sentence_to_integers(sent2, word_dict)\n",
    "        sample_list.append({\"in0\": sent_tokens, \"in1\": context_tokens, \"label\": 0})\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "formed-transformation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:11:25.966839Z",
     "iopub.status.busy": "2021-06-17T00:11:25.965679Z",
     "iopub.status.idle": "2021-06-17T00:12:48.177121Z",
     "shell.execute_reply": "2021-06-17T00:12:48.176583Z"
    },
    "papermill": {
     "duration": 82.242486,
     "end_time": "2021-06-17T00:12:48.177240",
     "exception": false,
     "start_time": "2021-06-17T00:11:25.934754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build dev and test data\n",
    "for data in [\"dev10k\", \"test10k\"]:\n",
    "    fname = os.path.join(datadir, \"wikipedia_{}.txt\".format(data))\n",
    "    test_nsr = 5\n",
    "    outfname = \"{}_tokenized-nsr{}.jsonl\".format(data, test_nsr)\n",
    "    article_buffer = list(get_article_iter_from_file(fname))\n",
    "    sample_buffer = []\n",
    "    for article_idx in range(len(article_buffer)):\n",
    "        sample_buffer += generate_pos_neg_samples_from_single_article(\n",
    "            w_dict, article_idx, article_buffer, negative_sampling_rate=test_nsr\n",
    "        )\n",
    "    with jsonlines.open(outfname, \"w\") as writer:\n",
    "        writer.write_all(sample_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-klein",
   "metadata": {
    "papermill": {
     "duration": 0.02569,
     "end_time": "2021-06-17T00:12:48.228976",
     "exception": false,
     "start_time": "2021-06-17T00:12:48.203286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Upload preprocessed data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pointed-aberdeen",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:12:48.284311Z",
     "iopub.status.busy": "2021-06-17T00:12:48.283812Z",
     "iopub.status.idle": "2021-06-17T00:12:48.285525Z",
     "shell.execute_reply": "2021-06-17T00:12:48.285882Z"
    },
    "papermill": {
     "duration": 0.031215,
     "end_time": "2021-06-17T00:12:48.286007",
     "exception": false,
     "start_time": "2021-06-17T00:12:48.254792",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "TRAIN_DATA = \"train250k_tokenized_shuf.jsonl\"\n",
    "DEV_DATA = \"dev10k_tokenized-nsr{}.jsonl\".format(test_nsr)\n",
    "TEST_DATA = \"test10k_tokenized-nsr{}.jsonl\".format(test_nsr)\n",
    "\n",
    "# NOTE: define your s3 bucket and key here\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "S3_KEY = \"object2vec-doc2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "little-basin",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-17T00:12:48.398397Z",
     "iopub.status.busy": "2021-06-17T00:12:48.397900Z",
     "iopub.status.idle": "2021-06-17T00:12:49.757677Z",
     "shell.execute_reply": "2021-06-17T00:12:49.756891Z"
    },
    "papermill": {
     "duration": 1.389437,
     "end_time": "2021-06-17T00:12:49.757878",
     "exception": true,
     "start_time": "2021-06-17T00:12:48.368441",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train250k_tokenized_shuf.jsonl to s3://sagemaker-us-west-2-688520471316/object2vec-doc2vec/input/train/train250k_tokenized_shuf.jsonl\n",
      "upload: ./dev10k_tokenized-nsr5.jsonl to s3://sagemaker-us-west-2-688520471316/object2vec-doc2vec/input/validation/dev10k_tokenized-nsr5.jsonl\n",
      "upload: ./test10k_tokenized-nsr5.jsonl to s3://sagemaker-us-west-2-688520471316/object2vec-doc2vec/input/test/test10k_tokenized-nsr5.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$TRAIN_DATA\" \"$DEV_DATA\" \"$TEST_DATA\" \"$bucket\" \"$S3_KEY\"\n",
    "\n",
    "aws s3 cp \"$1\" s3://$4/$5/input/train/\n",
    "aws s3 cp \"$2\" s3://$4/$5/input/validation/\n",
    "aws s3 cp \"$3\" s3://$4/$5/input/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-essence",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Define Sagemaker session, Object2Vec image, S3 input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "developing-acceptance",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your notebook is running on region 'us-west-2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your IAM role: 'arn:aws:iam::688520471316:role/hongshan-sagemaker-experiment'\n",
      "The image uri used is '174872318107.dkr.ecr.us-west-2.amazonaws.com/object2vec:1'\n",
      "Using s3 buceket: sagemaker-us-west-2-688520471316 and key prefix: object2vec-doc2vec\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "print(\"Your notebook is running on region '{}'\".format(region))\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"Your IAM role: '{}'\".format(role))\n",
    "\n",
    "container = get_image_uri(region, \"object2vec\")\n",
    "print(\"The image uri used is '{}'\".format(container))\n",
    "\n",
    "print(\"Using s3 buceket: {} and key prefix: {}\".format(bucket, S3_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "convertible-record",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "## define input channels\n",
    "\n",
    "s3_input_path = os.path.join(\"s3://\", bucket, S3_KEY, \"input\")\n",
    "\n",
    "s3_train = s3_input(\n",
    "    os.path.join(s3_input_path, \"train\", TRAIN_DATA),\n",
    "    distribution=\"ShardedByS3Key\",\n",
    "    content_type=\"application/jsonlines\",\n",
    ")\n",
    "\n",
    "s3_valid = s3_input(\n",
    "    os.path.join(s3_input_path, \"validation\", DEV_DATA),\n",
    "    distribution=\"ShardedByS3Key\",\n",
    "    content_type=\"application/jsonlines\",\n",
    ")\n",
    "\n",
    "s3_test = s3_input(\n",
    "    os.path.join(s3_input_path, \"test\", TEST_DATA),\n",
    "    distribution=\"ShardedByS3Key\",\n",
    "    content_type=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "consistent-rachel",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define output path\n",
    "output_path = os.path.join(\"s3://\", bucket, S3_KEY, \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-harassment",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Train and deploy doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-still",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We combine four new features into our training of Object2Vec:\n",
    "\n",
    "- Negative sampling: With the new `negative_sampling_rate` hyperparameter, users of Object2Vec only need to provide positively labeled data pairs, and the algorithm automatically samples for negative data internally during training.\n",
    "\n",
    "- Weight-sharing of embedding layer: The new `tied_token_embedding_weight` hyperparameter gives user the flexibility to share the embedding weights for both encoders, and it improves the performance of the algorithm in this use-case\n",
    "\n",
    "- The new `comparator_list` hyperparameter gives users the flexibility to mix-and-match different operators so that they can tune the algorithm towards optimal performance for their applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-serve",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Learning performance boost with new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-warrior",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "_Table 1_ below shows the effect of these features on these two metrics evaluated on a test set obtained from the same data creation process. \n",
    "\n",
    "We see that when negative sampling and weight-sharing of embedding layer is on, and when we use a customized comparator operator (Hadamard product), the model has improved test performance. When all these features are combined together (last row of Table 1), the algorithm has the best performance as measured by accuracy and cross-entropy.\n",
    "\n",
    "\n",
    "### Table 1\n",
    "\n",
    "|negative_sampling_rate|weight-sharing|comparator operator| Test accuracy | Test cross-entropy|\n",
    "| :-------------       | :----------: | :-----------:     | :----------:  | ----------:       |\n",
    "|  off                 | off          | default           | 0.167         |  23               |\n",
    "|  3                 | off          | default             | 0.92          |  0.21             |\n",
    "|  5                 | off          | default             | 0.92          |   0.19            |\n",
    "|  off               | on           | default           | 0.167         |  23               |\n",
    "|  3                 | on           | default           | 0.93         |  0.18               |\n",
    "|  5                 | on           | default           | 0.936         |  0.17               |\n",
    "|  off               | on           | customized        | 0.17         |  23               |\n",
    "|  3                 | on           | customized        | 0.93         |  0.18               |\n",
    "|  5                 | on           | customized        | 0.94         |  0.17               |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- The new `token_embedding_storage_type` hyperparameter flags the use of sparse gradient update, which takes advantage of the sparse input format of Object2Vec. We tested and summarized the training speedup with different GPU and `max_seq_len` configurations in the table below. In a word, we see 2-20 times speed up on different machine and algorithm configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-belly",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Training speedup with sparse gradient update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-wednesday",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "_Table 2_ below shows the training speeds up with sparse gradient update feature turned on, as a function of number of GPUs used for training.\n",
    "\n",
    "### Table 2\n",
    "\n",
    "|num_gpus|Throughput (samples/sec) with dense storage|Throughput with sparse storage|max_seq_len (in0/in1)|Speedup X-times  |\n",
    "| :------------- | :----------: | :-----------:| :----------: | ----------: |\n",
    "|  1             | 5k           | 14k          | 50           |  2.8        |\n",
    "|  2             | 2.7k         | 23k          | 50           |  8.5        |\n",
    "|  3             | 2k           | 23~26k       | 50           |  10         |\n",
    "|  4             | 2k           | 23k          | 50           |  10         |\n",
    "|  8             | 1.1k         | 19k~20k      | 50           |  20         |\n",
    "|  1             | 1.1k         | 2k           | 500          |  2          |\n",
    "|  2             | 1.5k         | 3.6k         | 500          |  2.4        |\n",
    "|  4             | 1.6k         | 6k           | 500          |  3.75       |\n",
    "|  6             | 1.3k         | 6.7k         | 500          |  5.15       |\n",
    "|  8             | 1.1k        | 5.6k         | 500          |  5          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efficient-boulder",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# Define training hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "    \"_kvstore\": \"device\",\n",
    "    \"_num_gpus\": \"auto\",\n",
    "    \"_num_kv_servers\": \"auto\",\n",
    "    \"bucket_width\": 0,\n",
    "    \"dropout\": 0.4,\n",
    "    \"early_stopping_patience\": 2,\n",
    "    \"early_stopping_tolerance\": 0.01,\n",
    "    \"enc0_layers\": \"auto\",\n",
    "    \"enc0_max_seq_len\": 50,\n",
    "    \"enc0_network\": \"pooled_embedding\",\n",
    "    \"enc0_pretrained_embedding_file\": \"\",\n",
    "    \"enc0_token_embedding_dim\": 300,\n",
    "    \"enc0_vocab_size\": 267522,\n",
    "    \"enc1_network\": \"enc0\",\n",
    "    \"enc_dim\": 300,\n",
    "    \"epochs\": 2,   # use 20 to get a great model\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"mini_batch_size\": 512,\n",
    "    \"mlp_activation\": \"relu\",\n",
    "    \"mlp_dim\": 512,\n",
    "    \"mlp_layers\": 2,\n",
    "    \"num_classes\": 2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"output_layer\": \"softmax\",\n",
    "    \"weight_decay\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "hyperparameters[\"negative_sampling_rate\"] = 3\n",
    "hyperparameters[\"tied_token_embedding_weight\"] = \"true\"\n",
    "hyperparameters[\"comparator_list\"] = \"hadamard\"\n",
    "hyperparameters[\"token_embedding_storage_type\"] = \"row_sparse\"\n",
    "\n",
    "\n",
    "# get estimator\n",
    "doc2vec = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p2.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "thermal-equity",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-17 22:40:13 Starting - Starting the training job...\n",
      "2021-06-17 22:40:15 Starting - Launching requested ML instancesProfilerReport-1623969612: InProgress\n",
      ".........\n",
      "2021-06-17 22:42:00 Starting - Preparing the instances for training.........\n",
      "2021-06-17 22:43:41 Downloading - Downloading input data...\n",
      "2021-06-17 22:44:05 Training - Downloading the training image......\n",
      "2021-06-17 22:45:14 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'enc_dim': 4096, 'mlp_dim': 512, 'mlp_activation': 'linear', 'mlp_layers': 2, 'output_layer': 'softmax', 'optimizer': 'adam', 'learning_rate': 0.0004, 'mini_batch_size': 32, 'epochs': 30, 'bucket_width': 0, 'early_stopping_tolerance': 0.01, 'early_stopping_patience': 3, 'dropout': 0, 'weight_decay': 0, 'negative_sampling_rate': 0, 'comparator_list': 'hadamard, concat, abs_diff', 'tied_token_embedding_weight': 'false', 'token_embedding_storage_type': 'dense', 'enc0_network': 'hcnn', 'enc1_network': 'enc0', 'enc0_token_embedding_dim': 300, 'enc0_layers': 'auto', 'enc0_cnn_filter_width': 3, 'enc1_token_embedding_dim': 300, 'enc1_layers': 'auto', 'enc1_cnn_filter_width': 3, 'enc0_pretrained_embedding_file': '', 'enc0_freeze_pretrained_embedding': 'true', 'enc1_pretrained_embedding_file': '', 'enc1_freeze_pretrained_embedding': 'true', 'enc0_vocab_file': '', 'enc1_vocab_file': '', 'num_classes': 2, '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'enc0_network': 'pooled_embedding', 'num_classes': '2', 'comparator_list': 'hadamard', 'enc0_layers': 'auto', 'enc_dim': '300', 'enc1_network': 'enc0', 'negative_sampling_rate': '3', 'enc0_token_embedding_dim': '300', 'mlp_activation': 'relu', 'mlp_dim': '512', 'optimizer': 'adam', '_num_kv_servers': 'auto', 'token_embedding_storage_type': 'row_sparse', 'mlp_layers': '2', 'weight_decay': '0', '_kvstore': 'device', 'enc0_vocab_size': '267522', 'enc0_max_seq_len': '50', 'early_stopping_patience': '2', 'output_layer': 'softmax', 'tied_token_embedding_weight': 'true', '_num_gpus': 'auto', 'dropout': '0.4', 'early_stopping_tolerance': '0.01', 'bucket_width': '0', 'epochs': '20', 'learning_rate': '0.01', 'mini_batch_size': '512'}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Final configuration: {'enc_dim': '300', 'mlp_dim': '512', 'mlp_activation': 'relu', 'mlp_layers': '2', 'output_layer': 'softmax', 'optimizer': 'adam', 'learning_rate': '0.01', 'mini_batch_size': '512', 'epochs': '20', 'bucket_width': '0', 'early_stopping_tolerance': '0.01', 'early_stopping_patience': '2', 'dropout': '0.4', 'weight_decay': '0', 'negative_sampling_rate': '3', 'comparator_list': 'hadamard', 'tied_token_embedding_weight': 'true', 'token_embedding_storage_type': 'row_sparse', 'enc0_network': 'pooled_embedding', 'enc1_network': 'enc0', 'enc0_token_embedding_dim': '300', 'enc0_layers': 'auto', 'enc0_cnn_filter_width': 3, 'enc1_token_embedding_dim': 300, 'enc1_layers': 'auto', 'enc1_cnn_filter_width': 3, 'enc0_pretrained_embedding_file': '', 'enc0_freeze_pretrained_embedding': 'true', 'enc1_pretrained_embedding_file': '', 'enc1_freeze_pretrained_embedding': 'true', 'enc0_vocab_file': '', 'enc1_vocab_file': '', 'num_classes': '2', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', '_kvstore': 'device', 'enc0_vocab_size': '267522', 'enc0_max_seq_len': '50'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Loaded iterator creator application/jsonlines for content type ('application/jsonlines', '1.0')\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] create_iter params {'enc_dim': '300', 'mlp_dim': '512', 'mlp_activation': 'relu', 'mlp_layers': '2', 'output_layer': 'softmax', 'optimizer': 'adam', 'learning_rate': '0.01', 'mini_batch_size': '512', 'epochs': '20', 'bucket_width': '0', 'early_stopping_tolerance': '0.01', 'early_stopping_patience': '2', 'dropout': '0.4', 'weight_decay': '0', 'negative_sampling_rate': '3', 'comparator_list': 'hadamard', 'tied_token_embedding_weight': 'true', 'token_embedding_storage_type': 'row_sparse', 'enc0_network': 'pooled_embedding', 'enc1_network': 'enc0', 'enc0_token_embedding_dim': '300', 'enc0_layers': 'auto', 'enc0_cnn_filter_width': 3, 'enc1_token_embedding_dim': 300, 'enc1_layers': 'auto', 'enc1_cnn_filter_width': 3, 'enc0_pretrained_embedding_file': '', 'enc0_freeze_pretrained_embedding': 'true', 'enc1_pretrained_embedding_file': '', 'enc1_freeze_pretrained_embedding': 'true', 'enc0_vocab_file': '', 'enc1_vocab_file': '', 'num_classes': '2', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', '_kvstore': 'device', 'enc0_vocab_size': '267522', 'enc0_max_seq_len': '50'}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] create_iter content_params {}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Parameters of encoders: [{'network': 'pooled_embedding', 'token_embedding_dim': '300', 'layers': 'auto', 'cnn_filter_width': 3, 'pretrained_embedding_file': '', 'freeze_pretrained_embedding': 'true', 'vocab_file': '', 'vocab_size': '267522', 'max_seq_len': '50'}, {'network': 'enc0', 'token_embedding_dim': 300, 'layers': 'auto', 'cnn_filter_width': 3, 'pretrained_embedding_file': '', 'freeze_pretrained_embedding': 'true', 'vocab_file': ''}]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Encoder configs: [{'num_layers': 1, 'enc_index': 0, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}, {'num_layers': 1, 'enc_index': 1, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Config: {'enc_dim': 300, 'max_seq_lens': [50, 50], 'dropout': 0.4, 'weight_decay': 0.0, 'mlp_activation': 'relu', 'mlp_dim': 512, 'mlp_layers': 2, 'output_layer': 'softmax', 'learning_rate': 0.01, 'optimizer': 'adam', 'num_classes': 2, 'epochs': 20, 'mini_batch_size': 512, 'bucket_width': 0, 'comparator_list': ['hadamard'], 'tied_token_embedding_weight': True, 'negative_sampling_rate': 3, 'token_embedding_storage_type': 'row_sparse', 'early_stopping_patience': 2, 'early_stopping_tolerance': 0.01, 'enc_configs': [{'num_layers': 1, 'enc_index': 0, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}, {'num_layers': 1, 'enc_index': 1, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}]}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] use bucketing: False\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] Creating data iterator for /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:19 INFO 140683922831168] One or more sequences have been truncated because they exceeded max_seq_len\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:56 INFO 140683922831168] Source words: 4389530\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:56 INFO 140683922831168] Target words: 10933115\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:56 INFO 140683922831168] Total: 250000 samples in 1 buckets\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:56 INFO 140683922831168] Bucket of (50, 50) : 250000 samples in 489 batches of 512, approx 22391.0 words/batch\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:56 INFO 140683922831168] 0 sentence pairs discarded\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:56 INFO 140683922831168] fill up mode: replicate\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:56 INFO 140683922831168] \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:56 INFO 140683922831168] Negative sampling enabled with sampling rate 3\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] Replicating 368 random sentences from bucket (50, 50) to size it to multiple of 512\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] Bucket batch sizes: [BucketBatchSize(batch_size=512, average_words_per_batch=22391.01952)]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] create_iter params {'enc_dim': '300', 'mlp_dim': '512', 'mlp_activation': 'relu', 'mlp_layers': '2', 'output_layer': 'softmax', 'optimizer': 'adam', 'learning_rate': '0.01', 'mini_batch_size': '512', 'epochs': '20', 'bucket_width': '0', 'early_stopping_tolerance': '0.01', 'early_stopping_patience': '2', 'dropout': '0.4', 'weight_decay': '0', 'negative_sampling_rate': '3', 'comparator_list': 'hadamard', 'tied_token_embedding_weight': 'true', 'token_embedding_storage_type': 'row_sparse', 'enc0_network': 'pooled_embedding', 'enc1_network': 'enc0', 'enc0_token_embedding_dim': '300', 'enc0_layers': 'auto', 'enc0_cnn_filter_width': 3, 'enc1_token_embedding_dim': 300, 'enc1_layers': 'auto', 'enc1_cnn_filter_width': 3, 'enc0_pretrained_embedding_file': '', 'enc0_freeze_pretrained_embedding': 'true', 'enc1_pretrained_embedding_file': '', 'enc1_freeze_pretrained_embedding': 'true', 'enc0_vocab_file': '', 'enc1_vocab_file': '', 'num_classes': '2', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', '_kvstore': 'device', 'enc0_vocab_size': '267522', 'enc0_max_seq_len': '50'}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] create_iter content_params {}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] Parameters of encoders: [{'network': 'pooled_embedding', 'token_embedding_dim': '300', 'layers': 'auto', 'cnn_filter_width': 3, 'pretrained_embedding_file': '', 'freeze_pretrained_embedding': 'true', 'vocab_file': '', 'vocab_size': '267522', 'max_seq_len': '50'}, {'network': 'enc0', 'token_embedding_dim': 300, 'layers': 'auto', 'cnn_filter_width': 3, 'pretrained_embedding_file': '', 'freeze_pretrained_embedding': 'true', 'vocab_file': ''}]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] Encoder configs: [{'num_layers': 1, 'enc_index': 0, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}, {'num_layers': 1, 'enc_index': 1, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] Config: {'enc_dim': 300, 'max_seq_lens': [50, 50], 'dropout': 0.4, 'weight_decay': 0.0, 'mlp_activation': 'relu', 'mlp_dim': 512, 'mlp_layers': 2, 'output_layer': 'softmax', 'learning_rate': 0.01, 'optimizer': 'adam', 'num_classes': 2, 'epochs': 20, 'mini_batch_size': 512, 'bucket_width': 0, 'comparator_list': ['hadamard'], 'tied_token_embedding_weight': True, 'negative_sampling_rate': 3, 'token_embedding_storage_type': 'row_sparse', 'early_stopping_patience': 2, 'early_stopping_tolerance': 0.01, 'enc_configs': [{'num_layers': 1, 'enc_index': 0, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}, {'num_layers': 1, 'enc_index': 1, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}]}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] use bucketing: False\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] Creating data iterator for /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:45:57 INFO 140683922831168] One or more sequences have been truncated because they exceeded max_seq_len\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Source words: 1055124\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Target words: 2666089\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Total: 60000 samples in 1 buckets\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Bucket of (50, 50) : 60000 samples in 118 batches of 512, approx 22750.6 words/batch\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] 0 sentence pairs discarded\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] fill up mode: replicate\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Negative sampling not used\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Replicating 416 random sentences from bucket (50, 50) to size it to multiple of 512\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] create_iter params {'enc_dim': '300', 'mlp_dim': '512', 'mlp_activation': 'relu', 'mlp_layers': '2', 'output_layer': 'softmax', 'optimizer': 'adam', 'learning_rate': '0.01', 'mini_batch_size': '512', 'epochs': '20', 'bucket_width': '0', 'early_stopping_tolerance': '0.01', 'early_stopping_patience': '2', 'dropout': '0.4', 'weight_decay': '0', 'negative_sampling_rate': '3', 'comparator_list': 'hadamard', 'tied_token_embedding_weight': 'true', 'token_embedding_storage_type': 'row_sparse', 'enc0_network': 'pooled_embedding', 'enc1_network': 'enc0', 'enc0_token_embedding_dim': '300', 'enc0_layers': 'auto', 'enc0_cnn_filter_width': 3, 'enc1_token_embedding_dim': 300, 'enc1_layers': 'auto', 'enc1_cnn_filter_width': 3, 'enc0_pretrained_embedding_file': '', 'enc0_freeze_pretrained_embedding': 'true', 'enc1_pretrained_embedding_file': '', 'enc1_freeze_pretrained_embedding': 'true', 'enc0_vocab_file': '', 'enc1_vocab_file': '', 'num_classes': '2', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', '_kvstore': 'device', 'enc0_vocab_size': '267522', 'enc0_max_seq_len': '50'}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] create_iter content_params {}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Parameters of encoders: [{'network': 'pooled_embedding', 'token_embedding_dim': '300', 'layers': 'auto', 'cnn_filter_width': 3, 'pretrained_embedding_file': '', 'freeze_pretrained_embedding': 'true', 'vocab_file': '', 'vocab_size': '267522', 'max_seq_len': '50'}, {'network': 'enc0', 'token_embedding_dim': 300, 'layers': 'auto', 'cnn_filter_width': 3, 'pretrained_embedding_file': '', 'freeze_pretrained_embedding': 'true', 'vocab_file': ''}]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Encoder configs: [{'num_layers': 1, 'enc_index': 0, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}, {'num_layers': 1, 'enc_index': 1, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Config: {'enc_dim': 300, 'max_seq_lens': [50, 50], 'dropout': 0.4, 'weight_decay': 0.0, 'mlp_activation': 'relu', 'mlp_dim': 512, 'mlp_layers': 2, 'output_layer': 'softmax', 'learning_rate': 0.01, 'optimizer': 'adam', 'num_classes': 2, 'epochs': 20, 'mini_batch_size': 512, 'bucket_width': 0, 'comparator_list': ['hadamard'], 'tied_token_embedding_weight': True, 'negative_sampling_rate': 3, 'token_embedding_storage_type': 'row_sparse', 'early_stopping_patience': 2, 'early_stopping_tolerance': 0.01, 'enc_configs': [{'num_layers': 1, 'enc_index': 0, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}, {'num_layers': 1, 'enc_index': 1, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}]}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] use bucketing: False\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] Creating data iterator for /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:07 INFO 140683922831168] One or more sequences have been truncated because they exceeded max_seq_len\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Source words: 1062024\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Target words: 2683970\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Total: 60000 samples in 1 buckets\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Bucket of (50, 50) : 60000 samples in 118 batches of 512, approx 22903.2 words/batch\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] 0 sentence pairs discarded\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] fill up mode: replicate\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Negative sampling not used\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Replicating 416 random sentences from bucket (50, 50) to size it to multiple of 512\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Parameters of encoders: [{'network': 'pooled_embedding', 'token_embedding_dim': '300', 'layers': 'auto', 'cnn_filter_width': 3, 'pretrained_embedding_file': '', 'freeze_pretrained_embedding': 'true', 'vocab_file': '', 'vocab_size': '267522', 'max_seq_len': '50'}, {'network': 'enc0', 'token_embedding_dim': 300, 'layers': 'auto', 'cnn_filter_width': 3, 'pretrained_embedding_file': '', 'freeze_pretrained_embedding': 'true', 'vocab_file': ''}]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Encoder configs: [{'num_layers': 1, 'enc_index': 0, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}, {'num_layers': 1, 'enc_index': 1, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Config: {'enc_dim': 300, 'max_seq_lens': [50, 50], 'dropout': 0.4, 'weight_decay': 0.0, 'mlp_activation': 'relu', 'mlp_dim': 512, 'mlp_layers': 2, 'output_layer': 'softmax', 'learning_rate': 0.01, 'optimizer': 'adam', 'num_classes': 2, 'epochs': 20, 'mini_batch_size': 512, 'bucket_width': 0, 'comparator_list': ['hadamard'], 'tied_token_embedding_weight': True, 'negative_sampling_rate': 3, 'token_embedding_storage_type': 'row_sparse', 'early_stopping_patience': 2, 'early_stopping_tolerance': 0.01, 'enc_configs': [{'num_layers': 1, 'enc_index': 0, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}, {'num_layers': 1, 'enc_index': 1, 'token_embedding_dim': 300, 'vocab_size': 267522, 'vocab_file': '', 'vocab_dict': None, 'dropout': 0.4, 'pretrained_embedding_file': '', 'pretrained_embedding_file_path': None, 'freeze_pretrained_embedding': True, 'is_train': True}]}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Creating new state\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] params {'enc_dim': '300', 'mlp_dim': '512', 'mlp_activation': 'relu', 'mlp_layers': '2', 'output_layer': 'softmax', 'optimizer': 'adam', 'learning_rate': '0.01', 'mini_batch_size': '512', 'epochs': '20', 'bucket_width': '0', 'early_stopping_tolerance': '0.01', 'early_stopping_patience': '2', 'dropout': '0.4', 'weight_decay': '0', 'negative_sampling_rate': '3', 'comparator_list': 'hadamard', 'tied_token_embedding_weight': 'true', 'token_embedding_storage_type': 'row_sparse', 'enc0_network': 'pooled_embedding', 'enc1_network': 'enc0', 'enc0_token_embedding_dim': '300', 'enc0_layers': 'auto', 'enc0_cnn_filter_width': 3, 'enc1_token_embedding_dim': 300, 'enc1_layers': 'auto', 'enc1_cnn_filter_width': 3, 'enc0_pretrained_embedding_file': '', 'enc0_freeze_pretrained_embedding': 'true', 'enc1_pretrained_embedding_file': '', 'enc1_freeze_pretrained_embedding': 'true', 'enc0_vocab_file': '', 'enc1_vocab_file': '', 'num_classes': '2', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', '_kvstore': 'device', 'enc0_vocab_size': '267522', 'enc0_max_seq_len': '50', 'default_bucket_key': (50, 50)}\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] default_bucket_key (50, 50)\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] nvidia-smi: took 0.043 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] nvidia-smi identified 1 GPUs.\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] context [gpu(0)]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Create Store: device\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] data_names: ['source', 'target']\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] label_names: ['out_layer_label']\u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                                        Output Shape            Param #     Previous Layer                  \u001b[0m\n",
      "\u001b[34m========================================================================================================================\u001b[0m\n",
      "\u001b[34msource(null)                                        50                      0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34membed_shared(_contrib_SparseEmbedding)              50x300                  0           source                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_not_equal_scalar0(_not_equal_scalar)               50                      0           source                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mreshape0(Reshape)                                   50x1                    0           _not_equal_scalar0              \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_mul0(broadcast_mul)                       50x300                  0           embed_shared                    \n",
      "                                                                                        reshape0                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum0(sum)                                           300                     0           broadcast_mul0                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum1(sum)                                           1                       0           reshape0                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mzeros_like0(zeros_like)                             1                       0           sum1                            \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_equal0(_equal)                                     1                       0           sum1                            \n",
      "                                                                                        zeros_like0                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_plus0(elemwise_add)                                1                       0           sum1                            \n",
      "                                                                                        _equal0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_div0(broadcast_div)                       300                     0           sum0                            \n",
      "                                                                                        _plus0                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout0(Dropout)                                   300                     0           broadcast_div0                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34membed_shared(_contrib_SparseEmbedding)              50x300                  0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_not_equal_scalar1(_not_equal_scalar)               50                      0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mreshape1(Reshape)                                   50x1                    0           _not_equal_scalar1              \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_mul1(broadcast_mul)                       50x300                  0           embed_shared                    \n",
      "                                                                                        reshape1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum2(sum)                                           300                     0           broadcast_mul1                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum3(sum)                                           1                       0           reshape1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mzeros_like1(zeros_like)                             1                       0           sum3                            \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_equal1(_equal)                                     1                       0           sum3                            \n",
      "                                                                                        zeros_like1                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_plus1(elemwise_add)                                1                       0           sum3                            \n",
      "                                                                                        _equal1                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_div1(broadcast_div)                       300                     0           sum2                            \n",
      "                                                                                        _plus1                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout1(Dropout)                                   300                     0           broadcast_div1                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_mul0(elemwise_mul)                                 300                     0           dropout0                        \n",
      "                                                                                        dropout1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mconcat0(Concat)                                     300                     0           _mul0                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mmlp_fc0(FullyConnected)                             512                     154112      concat0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mactivation0(Activation)                             512                     0           mlp_fc0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout2(Dropout)                                   512                     0           activation0                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mmlp_fc1(FullyConnected)                             512                     262656      dropout2                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mactivation1(Activation)                             512                     0           mlp_fc1                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout3(Dropout)                                   512                     0           activation1                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34moutput_layer(FullyConnected)                        2                       1026        dropout3                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mout_layer(SoftmaxOutput)                            2                       0           output_layer                    \u001b[0m\n",
      "\u001b[34m========================================================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 417794\u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] data_shapes [DataDesc[source,(512, 50),<class 'numpy.float32'>,NTC], DataDesc[target,(512, 50),<class 'numpy.float32'>,NTC]]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] label_shapes [DataDesc[out_layer_label,(512,),<class 'numpy.float32'>,NTC]]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] fixed_param_names: []\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:16 INFO 140683922831168] Initialized BucketingPlus Module\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:22 INFO 140683922831168] arg_params keys for module initialization: dict_keys([])\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:22 INFO 140683922831168] all params:dict_keys(['embed_shared_weight', 'mlp_fc0_weight', 'mlp_fc0_bias', 'mlp_fc1_weight', 'mlp_fc1_bias', 'output_layer_weight', 'output_layer_bias'])\u001b[0m\n",
      "\u001b[34m[22:46:22] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.2769.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:306: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use kv.row_sparse_pull() or module.prepare() with row_ids.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623969919.6027164, \"EndTime\": 1623969982.9248552, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 6494.412899017334, \"count\": 1, \"min\": 6494.412899017334, \"max\": 6494.412899017334}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623969982.925016, \"EndTime\": 1623969982.925076, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[22:46:24] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.2769.0/AL2_x86_64/generic-flavor/src/src/operator/././../common/utils.h:450: Optimizer with lazy_update = True detected. Be aware that lazy update with row_sparse gradient is different from standard update, and may lead to different empirical results. See https://mxnet.incubator.apache.org/api/python/optimization/optimization.html for more details.\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:28 INFO 140683922831168] Epoch: 0, batches: 100, num_examples: 51200, 13789.2 samples/sec, epoch time so far: 0:00:03.713037\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:28 INFO 140683922831168] #011Training metrics: perplexity: 1.638 cross_entropy: 0.493 accuracy: 0.766 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:31 INFO 140683922831168] Epoch: 0, batches: 200, num_examples: 102400, 14243.7 samples/sec, epoch time so far: 0:00:07.189158\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:31 INFO 140683922831168] #011Training metrics: perplexity: 1.550 cross_entropy: 0.438 accuracy: 0.795 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:35 INFO 140683922831168] Epoch: 0, batches: 300, num_examples: 153600, 14358.5 samples/sec, epoch time so far: 0:00:10.697510\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:35 INFO 140683922831168] #011Training metrics: perplexity: 1.504 cross_entropy: 0.408 accuracy: 0.813 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:38 INFO 140683922831168] Epoch: 0, batches: 400, num_examples: 204800, 14374.3 samples/sec, epoch time so far: 0:00:14.247677\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:38 INFO 140683922831168] #011Training metrics: perplexity: 1.474 cross_entropy: 0.388 accuracy: 0.825 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:42 INFO 140683922831168] Epoch: 0, batches: 500, num_examples: 256000, 14407.8 samples/sec, epoch time so far: 0:00:17.768153\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:42 INFO 140683922831168] #011Training metrics: perplexity: 1.453 cross_entropy: 0.374 accuracy: 0.834 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:45 INFO 140683922831168] Epoch: 0, batches: 600, num_examples: 307200, 14413.7 samples/sec, epoch time so far: 0:00:21.313051\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:45 INFO 140683922831168] #011Training metrics: perplexity: 1.436 cross_entropy: 0.362 accuracy: 0.840 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:49 INFO 140683922831168] Epoch: 0, batches: 700, num_examples: 358400, 14432.0 samples/sec, epoch time so far: 0:00:24.833774\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:49 INFO 140683922831168] #011Training metrics: perplexity: 1.423 cross_entropy: 0.353 accuracy: 0.846 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:52 INFO 140683922831168] Epoch: 0, batches: 800, num_examples: 409600, 14429.6 samples/sec, epoch time so far: 0:00:28.386039\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:52 INFO 140683922831168] #011Training metrics: perplexity: 1.412 cross_entropy: 0.345 accuracy: 0.850 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:56 INFO 140683922831168] Epoch: 0, batches: 900, num_examples: 460800, 14423.9 samples/sec, epoch time so far: 0:00:31.946871\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:56 INFO 140683922831168] #011Training metrics: perplexity: 1.403 cross_entropy: 0.339 accuracy: 0.854 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:59 INFO 140683922831168] Epoch: 0, batches: 1000, num_examples: 512000, 14418.0 samples/sec, epoch time so far: 0:00:35.511123\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:46:59 INFO 140683922831168] #011Training metrics: perplexity: 1.394 cross_entropy: 0.332 accuracy: 0.857 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:03 INFO 140683922831168] Epoch: 0, batches: 1100, num_examples: 563200, 14403.7 samples/sec, epoch time so far: 0:00:39.101076\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:03 INFO 140683922831168] #011Training metrics: perplexity: 1.387 cross_entropy: 0.327 accuracy: 0.860 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:07 INFO 140683922831168] Epoch: 0, batches: 1200, num_examples: 614400, 14385.8 samples/sec, epoch time so far: 0:00:42.708863\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:07 INFO 140683922831168] #011Training metrics: perplexity: 1.380 cross_entropy: 0.322 accuracy: 0.862 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:10 INFO 140683922831168] Epoch: 0, batches: 1300, num_examples: 665600, 14395.7 samples/sec, epoch time so far: 0:00:46.235979\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:10 INFO 140683922831168] #011Training metrics: perplexity: 1.375 cross_entropy: 0.319 accuracy: 0.864 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:14 INFO 140683922831168] Epoch: 0, batches: 1400, num_examples: 716800, 14403.4 samples/sec, epoch time so far: 0:00:49.765968\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:14 INFO 140683922831168] #011Training metrics: perplexity: 1.370 cross_entropy: 0.315 accuracy: 0.866 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:17 INFO 140683922831168] Epoch: 0, batches: 1500, num_examples: 768000, 14419.3 samples/sec, epoch time so far: 0:00:53.261823\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:17 INFO 140683922831168] #011Training metrics: perplexity: 1.367 cross_entropy: 0.312 accuracy: 0.868 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:21 INFO 140683922831168] Epoch: 0, batches: 1600, num_examples: 819200, 14433.5 samples/sec, epoch time so far: 0:00:56.757026\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:21 INFO 140683922831168] #011Training metrics: perplexity: 1.363 cross_entropy: 0.309 accuracy: 0.870 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:24 INFO 140683922831168] Epoch: 0, batches: 1700, num_examples: 870400, 14441.3 samples/sec, epoch time so far: 0:01:00.271542\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:24 INFO 140683922831168] #011Training metrics: perplexity: 1.359 cross_entropy: 0.307 accuracy: 0.871 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:28 INFO 140683922831168] Epoch: 0, batches: 1800, num_examples: 921600, 14450.2 samples/sec, epoch time so far: 0:01:03.777619\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:28 INFO 140683922831168] #011Training metrics: perplexity: 1.356 cross_entropy: 0.304 accuracy: 0.872 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:31 INFO 140683922831168] Epoch: 0, batches: 1900, num_examples: 972800, 14454.1 samples/sec, epoch time so far: 0:01:07.302647\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:31 INFO 140683922831168] #011Training metrics: perplexity: 1.353 cross_entropy: 0.302 accuracy: 0.874 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:33 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:33 INFO 140683922831168] Completed Epoch: 0, time taken: 0:01:09.207756\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:33 INFO 140683922831168] Epoch 0 Training metrics:   perplexity: 1.351 cross_entropy: 0.301 accuracy: 0.874 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:33 INFO 140683922831168] #quality_metric: host=algo-1, epoch=0, train cross_entropy <loss>=0.3009043032168976\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:33 INFO 140683922831168] #quality_metric: host=algo-1, epoch=0, train accuracy <score>=0.8743719598054276\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:36 INFO 140683922831168] Epoch 0 Validation metrics: perplexity: 1.232 cross_entropy: 0.208 accuracy: 0.915 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:36 INFO 140683922831168] #quality_metric: host=algo-1, epoch=0, validation cross_entropy <loss>=0.20847056439872516\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:36 INFO 140683922831168] #quality_metric: host=algo-1, epoch=0, validation accuracy <score>=0.9150225105932204\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:36 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623969982.924961, \"EndTime\": 1623970056.6261582, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"early_stop.time\": {\"sum\": 0.7870197296142578, \"count\": 1, \"min\": 0.7870197296142578, \"max\": 0.7870197296142578}, \"update.time\": {\"sum\": 72288.22994232178, \"count\": 1, \"min\": 72288.22994232178, \"max\": 72288.22994232178}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:36 INFO 140683922831168] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623969984.3378851, \"EndTime\": 1623970056.6267817, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 999936.0, \"count\": 1, \"min\": 999936, \"max\": 999936}, \"Total Batches Seen\": {\"sum\": 1953.0, \"count\": 1, \"min\": 1953, \"max\": 1953}, \"Max Records Seen Between Resets\": {\"sum\": 999936.0, \"count\": 1, \"min\": 999936, \"max\": 999936}, \"Max Batches Seen Between Resets\": {\"sum\": 1953.0, \"count\": 1, \"min\": 1953, \"max\": 1953}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 999936.0, \"count\": 1, \"min\": 999936, \"max\": 999936}, \"Number of Batches Since Last Reset\": {\"sum\": 1953.0, \"count\": 1, \"min\": 1953, \"max\": 1953}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:36 INFO 140683922831168] #throughput_metric: host=algo-1, train throughput=13832.433437532603 records/second\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:41 INFO 140683922831168] Epoch: 1, batches: 100, num_examples: 51200, 14464.3 samples/sec, epoch time so far: 0:00:03.539754\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:41 INFO 140683922831168] #011Training metrics: perplexity: 1.213 cross_entropy: 0.193 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:44 INFO 140683922831168] Epoch: 1, batches: 200, num_examples: 102400, 14466.9 samples/sec, epoch time so far: 0:00:07.078234\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:44 INFO 140683922831168] #011Training metrics: perplexity: 1.212 cross_entropy: 0.193 accuracy: 0.925 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:48 INFO 140683922831168] Epoch: 1, batches: 300, num_examples: 153600, 14510.6 samples/sec, epoch time so far: 0:00:10.585354\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:48 INFO 140683922831168] #011Training metrics: perplexity: 1.212 cross_entropy: 0.193 accuracy: 0.925 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:51 INFO 140683922831168] Epoch: 1, batches: 400, num_examples: 204800, 14524.0 samples/sec, epoch time so far: 0:00:14.100833\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:51 INFO 140683922831168] #011Training metrics: perplexity: 1.212 cross_entropy: 0.192 accuracy: 0.925 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:55 INFO 140683922831168] Epoch: 1, batches: 500, num_examples: 256000, 14549.2 samples/sec, epoch time so far: 0:00:17.595491\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:55 INFO 140683922831168] #011Training metrics: perplexity: 1.213 cross_entropy: 0.193 accuracy: 0.925 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:58 INFO 140683922831168] Epoch: 1, batches: 600, num_examples: 307200, 14562.7 samples/sec, epoch time so far: 0:00:21.095018\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:47:58 INFO 140683922831168] #011Training metrics: perplexity: 1.213 cross_entropy: 0.193 accuracy: 0.925 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:02 INFO 140683922831168] Epoch: 1, batches: 700, num_examples: 358400, 14542.8 samples/sec, epoch time so far: 0:00:24.644455\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:02 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.925 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:05 INFO 140683922831168] Epoch: 1, batches: 800, num_examples: 409600, 14466.6 samples/sec, epoch time so far: 0:00:28.313555\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:05 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:09 INFO 140683922831168] Epoch: 1, batches: 900, num_examples: 460800, 14468.6 samples/sec, epoch time so far: 0:00:31.848204\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:09 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:12 INFO 140683922831168] Epoch: 1, batches: 1000, num_examples: 512000, 14465.9 samples/sec, epoch time so far: 0:00:35.393510\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:12 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:16 INFO 140683922831168] Epoch: 1, batches: 1100, num_examples: 563200, 14467.2 samples/sec, epoch time so far: 0:00:38.929448\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:16 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:19 INFO 140683922831168] Epoch: 1, batches: 1200, num_examples: 614400, 14478.5 samples/sec, epoch time so far: 0:00:42.435273\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:19 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:23 INFO 140683922831168] Epoch: 1, batches: 1300, num_examples: 665600, 14481.3 samples/sec, epoch time so far: 0:00:45.962755\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:23 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.925 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:26 INFO 140683922831168] Epoch: 1, batches: 1400, num_examples: 716800, 14489.6 samples/sec, epoch time so far: 0:00:49.470068\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:26 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:30 INFO 140683922831168] Epoch: 1, batches: 1500, num_examples: 768000, 14486.4 samples/sec, epoch time so far: 0:00:53.015332\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:30 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:34 INFO 140683922831168] Epoch: 1, batches: 1600, num_examples: 819200, 14495.9 samples/sec, epoch time so far: 0:00:56.512428\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:34 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:37 INFO 140683922831168] Epoch: 1, batches: 1700, num_examples: 870400, 14496.9 samples/sec, epoch time so far: 0:01:00.040332\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:37 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:41 INFO 140683922831168] Epoch: 1, batches: 1800, num_examples: 921600, 14493.6 samples/sec, epoch time so far: 0:01:03.586703\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:41 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:44 INFO 140683922831168] Epoch: 1, batches: 1900, num_examples: 972800, 14498.1 samples/sec, epoch time so far: 0:01:07.098336\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:44 INFO 140683922831168] #011Training metrics: perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:46 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:46 INFO 140683922831168] Completed Epoch: 1, time taken: 0:01:09.085752\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:46 INFO 140683922831168] Epoch 1 Training metrics:   perplexity: 1.214 cross_entropy: 0.194 accuracy: 0.924 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:46 INFO 140683922831168] #quality_metric: host=algo-1, epoch=1, train cross_entropy <loss>=0.19404637645206324\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:46 INFO 140683922831168] #quality_metric: host=algo-1, epoch=1, train accuracy <score>=0.9243703268788344\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:49 INFO 140683922831168] Epoch 1 Validation metrics: perplexity: 1.218 cross_entropy: 0.198 accuracy: 0.922 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:49 INFO 140683922831168] #quality_metric: host=algo-1, epoch=1, validation cross_entropy <loss>=0.19760655043488842\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:49 INFO 140683922831168] #quality_metric: host=algo-1, epoch=1, validation accuracy <score>=0.9223715572033898\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:49 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970056.6264005, \"EndTime\": 1623970129.6615603, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"early_stop.time\": {\"sum\": 19.077539443969727, \"count\": 1, \"min\": 19.077539443969727, \"max\": 19.077539443969727}, \"update.time\": {\"sum\": 72165.01569747925, \"count\": 1, \"min\": 72165.01569747925, \"max\": 72165.01569747925}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:49 INFO 140683922831168] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970057.496516, \"EndTime\": 1623970129.662189, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2001408.0, \"count\": 1, \"min\": 2001408, \"max\": 2001408}, \"Total Batches Seen\": {\"sum\": 3909.0, \"count\": 1, \"min\": 3909, \"max\": 3909}, \"Max Records Seen Between Resets\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Max Batches Seen Between Resets\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Number of Batches Since Last Reset\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:49 INFO 140683922831168] #throughput_metric: host=algo-1, train throughput=13877.364833992 records/second\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:53 INFO 140683922831168] Epoch: 2, batches: 100, num_examples: 51200, 14570.1 samples/sec, epoch time so far: 0:00:03.514049\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:53 INFO 140683922831168] #011Training metrics: perplexity: 1.160 cross_entropy: 0.148 accuracy: 0.943 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:57 INFO 140683922831168] Epoch: 2, batches: 200, num_examples: 102400, 14499.5 samples/sec, epoch time so far: 0:00:07.062305\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:48:57 INFO 140683922831168] #011Training metrics: perplexity: 1.158 cross_entropy: 0.147 accuracy: 0.944 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:00 INFO 140683922831168] Epoch: 2, batches: 300, num_examples: 153600, 14459.6 samples/sec, epoch time so far: 0:00:10.622717\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:00 INFO 140683922831168] #011Training metrics: perplexity: 1.161 cross_entropy: 0.149 accuracy: 0.943 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:04 INFO 140683922831168] Epoch: 2, batches: 400, num_examples: 204800, 14373.6 samples/sec, epoch time so far: 0:00:14.248382\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:04 INFO 140683922831168] #011Training metrics: perplexity: 1.162 cross_entropy: 0.150 accuracy: 0.943 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:08 INFO 140683922831168] Epoch: 2, batches: 500, num_examples: 256000, 14291.3 samples/sec, epoch time so far: 0:00:17.913044\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:08 INFO 140683922831168] #011Training metrics: perplexity: 1.162 cross_entropy: 0.150 accuracy: 0.943 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:11 INFO 140683922831168] Epoch: 2, batches: 600, num_examples: 307200, 14307.4 samples/sec, epoch time so far: 0:00:21.471420\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:11 INFO 140683922831168] #011Training metrics: perplexity: 1.163 cross_entropy: 0.151 accuracy: 0.943 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:15 INFO 140683922831168] Epoch: 2, batches: 700, num_examples: 358400, 14322.5 samples/sec, epoch time so far: 0:00:25.023490\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:15 INFO 140683922831168] #011Training metrics: perplexity: 1.164 cross_entropy: 0.151 accuracy: 0.942 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:18 INFO 140683922831168] Epoch: 2, batches: 800, num_examples: 409600, 14324.8 samples/sec, epoch time so far: 0:00:28.593799\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:18 INFO 140683922831168] #011Training metrics: perplexity: 1.164 cross_entropy: 0.152 accuracy: 0.942 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:22 INFO 140683922831168] Epoch: 2, batches: 900, num_examples: 460800, 14334.7 samples/sec, epoch time so far: 0:00:32.145875\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:22 INFO 140683922831168] #011Training metrics: perplexity: 1.165 cross_entropy: 0.152 accuracy: 0.942 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:25 INFO 140683922831168] Epoch: 2, batches: 1000, num_examples: 512000, 14342.4 samples/sec, epoch time so far: 0:00:35.698411\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:25 INFO 140683922831168] #011Training metrics: perplexity: 1.165 cross_entropy: 0.153 accuracy: 0.942 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:29 INFO 140683922831168] Epoch: 2, batches: 1100, num_examples: 563200, 14350.4 samples/sec, epoch time so far: 0:00:39.246339\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:29 INFO 140683922831168] #011Training metrics: perplexity: 1.165 cross_entropy: 0.153 accuracy: 0.942 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:32 INFO 140683922831168] Epoch: 2, batches: 1200, num_examples: 614400, 14345.5 samples/sec, epoch time so far: 0:00:42.828613\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:32 INFO 140683922831168] #011Training metrics: perplexity: 1.166 cross_entropy: 0.153 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:36 INFO 140683922831168] Epoch: 2, batches: 1300, num_examples: 665600, 14347.8 samples/sec, epoch time so far: 0:00:46.390423\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:36 INFO 140683922831168] #011Training metrics: perplexity: 1.166 cross_entropy: 0.154 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:40 INFO 140683922831168] Epoch: 2, batches: 1400, num_examples: 716800, 14352.4 samples/sec, epoch time so far: 0:00:49.942915\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:40 INFO 140683922831168] #011Training metrics: perplexity: 1.167 cross_entropy: 0.154 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:43 INFO 140683922831168] Epoch: 2, batches: 1500, num_examples: 768000, 14353.6 samples/sec, epoch time so far: 0:00:53.505725\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:43 INFO 140683922831168] #011Training metrics: perplexity: 1.167 cross_entropy: 0.154 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:47 INFO 140683922831168] Epoch: 2, batches: 1600, num_examples: 819200, 14359.5 samples/sec, epoch time so far: 0:00:57.049392\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:47 INFO 140683922831168] #011Training metrics: perplexity: 1.167 cross_entropy: 0.155 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:50 INFO 140683922831168] Epoch: 2, batches: 1700, num_examples: 870400, 14358.4 samples/sec, epoch time so far: 0:01:00.619390\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:50 INFO 140683922831168] #011Training metrics: perplexity: 1.167 cross_entropy: 0.155 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:54 INFO 140683922831168] Epoch: 2, batches: 1800, num_examples: 921600, 14363.4 samples/sec, epoch time so far: 0:01:04.163118\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:54 INFO 140683922831168] #011Training metrics: perplexity: 1.168 cross_entropy: 0.155 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:57 INFO 140683922831168] Epoch: 2, batches: 1900, num_examples: 972800, 14364.7 samples/sec, epoch time so far: 0:01:07.721580\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:57 INFO 140683922831168] #011Training metrics: perplexity: 1.168 cross_entropy: 0.155 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:59 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:59 INFO 140683922831168] Completed Epoch: 2, time taken: 0:01:09.704620\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:59 INFO 140683922831168] Epoch 2 Training metrics:   perplexity: 1.168 cross_entropy: 0.155 accuracy: 0.941 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:59 INFO 140683922831168] #quality_metric: host=algo-1, epoch=2, train cross_entropy <loss>=0.15506397776214256\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:49:59 INFO 140683922831168] #quality_metric: host=algo-1, epoch=2, train accuracy <score>=0.9408001421906953\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] Epoch 2 Validation metrics: perplexity: 1.224 cross_entropy: 0.203 accuracy: 0.925 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] #quality_metric: host=algo-1, epoch=2, validation cross_entropy <loss>=0.20252395598059994\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] #quality_metric: host=algo-1, epoch=2, validation accuracy <score>=0.9246226165254238\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] patience losses: [0.20847056439872516, 0.19760655043488842]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] min patience losses: 0.19760655043488842\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] current loss: 0.20252395598059994\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] absolute loss difference: 0.004917405545711517\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970129.6616979, \"EndTime\": 1623970202.9950414, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"early_stop.time\": {\"sum\": 0.6115436553955078, \"count\": 1, \"min\": 0.6115436553955078, \"max\": 0.6115436553955078}, \"update.time\": {\"sum\": 72882.76433944702, \"count\": 1, \"min\": 72882.76433944702, \"max\": 72882.76433944702}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970130.1122434, \"EndTime\": 1623970202.9953182, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3002880.0, \"count\": 1, \"min\": 3002880, \"max\": 3002880}, \"Total Batches Seen\": {\"sum\": 5865.0, \"count\": 1, \"min\": 5865, \"max\": 5865}, \"Max Records Seen Between Resets\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Max Batches Seen Between Resets\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Number of Batches Since Last Reset\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:02 INFO 140683922831168] #throughput_metric: host=algo-1, train throughput=13740.775960639015 records/second\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:06 INFO 140683922831168] Epoch: 3, batches: 100, num_examples: 51200, 13960.5 samples/sec, epoch time so far: 0:00:03.667483\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:06 INFO 140683922831168] #011Training metrics: perplexity: 1.134 cross_entropy: 0.126 accuracy: 0.952 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:10 INFO 140683922831168] Epoch: 3, batches: 200, num_examples: 102400, 14174.1 samples/sec, epoch time so far: 0:00:07.224439\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:10 INFO 140683922831168] #011Training metrics: perplexity: 1.135 cross_entropy: 0.127 accuracy: 0.952 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:14 INFO 140683922831168] Epoch: 3, batches: 300, num_examples: 153600, 14235.5 samples/sec, epoch time so far: 0:00:10.789898\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:14 INFO 140683922831168] #011Training metrics: perplexity: 1.135 cross_entropy: 0.127 accuracy: 0.952 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:17 INFO 140683922831168] Epoch: 3, batches: 400, num_examples: 204800, 14292.0 samples/sec, epoch time so far: 0:00:14.329669\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:17 INFO 140683922831168] #011Training metrics: perplexity: 1.135 cross_entropy: 0.127 accuracy: 0.952 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:21 INFO 140683922831168] Epoch: 3, batches: 500, num_examples: 256000, 14309.2 samples/sec, epoch time so far: 0:00:17.890532\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:21 INFO 140683922831168] #011Training metrics: perplexity: 1.137 cross_entropy: 0.128 accuracy: 0.952 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:24 INFO 140683922831168] Epoch: 3, batches: 600, num_examples: 307200, 14323.4 samples/sec, epoch time so far: 0:00:21.447490\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:24 INFO 140683922831168] #011Training metrics: perplexity: 1.138 cross_entropy: 0.130 accuracy: 0.951 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:28 INFO 140683922831168] Epoch: 3, batches: 700, num_examples: 358400, 14323.1 samples/sec, epoch time so far: 0:00:25.022576\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:28 INFO 140683922831168] #011Training metrics: perplexity: 1.139 cross_entropy: 0.130 accuracy: 0.951 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:31 INFO 140683922831168] Epoch: 3, batches: 800, num_examples: 409600, 14330.9 samples/sec, epoch time so far: 0:00:28.581641\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:31 INFO 140683922831168] #011Training metrics: perplexity: 1.140 cross_entropy: 0.131 accuracy: 0.951 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:35 INFO 140683922831168] Epoch: 3, batches: 900, num_examples: 460800, 14344.8 samples/sec, epoch time so far: 0:00:32.123142\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:35 INFO 140683922831168] #011Training metrics: perplexity: 1.140 cross_entropy: 0.131 accuracy: 0.951 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:38 INFO 140683922831168] Epoch: 3, batches: 1000, num_examples: 512000, 14336.9 samples/sec, epoch time so far: 0:00:35.712013\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:38 INFO 140683922831168] #011Training metrics: perplexity: 1.140 cross_entropy: 0.131 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:42 INFO 140683922831168] Epoch: 3, batches: 1100, num_examples: 563200, 14327.1 samples/sec, epoch time so far: 0:00:39.310107\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:42 INFO 140683922831168] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:46 INFO 140683922831168] Epoch: 3, batches: 1200, num_examples: 614400, 14332.7 samples/sec, epoch time so far: 0:00:42.866878\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:46 INFO 140683922831168] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:49 INFO 140683922831168] Epoch: 3, batches: 1300, num_examples: 665600, 14333.1 samples/sec, epoch time so far: 0:00:46.438087\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:49 INFO 140683922831168] #011Training metrics: perplexity: 1.141 cross_entropy: 0.132 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:53 INFO 140683922831168] Epoch: 3, batches: 1400, num_examples: 716800, 14338.0 samples/sec, epoch time so far: 0:00:49.992989\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:53 INFO 140683922831168] #011Training metrics: perplexity: 1.142 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:56 INFO 140683922831168] Epoch: 3, batches: 1500, num_examples: 768000, 14343.5 samples/sec, epoch time so far: 0:00:53.543535\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:50:56 INFO 140683922831168] #011Training metrics: perplexity: 1.142 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:00 INFO 140683922831168] Epoch: 3, batches: 1600, num_examples: 819200, 14353.2 samples/sec, epoch time so far: 0:00:57.074252\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:00 INFO 140683922831168] #011Training metrics: perplexity: 1.143 cross_entropy: 0.133 accuracy: 0.950 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:03 INFO 140683922831168] Epoch: 3, batches: 1700, num_examples: 870400, 14337.6 samples/sec, epoch time so far: 0:01:00.707632\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:03 INFO 140683922831168] #011Training metrics: perplexity: 1.143 cross_entropy: 0.134 accuracy: 0.949 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:07 INFO 140683922831168] Epoch: 3, batches: 1800, num_examples: 921600, 14317.1 samples/sec, epoch time so far: 0:01:04.370787\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:07 INFO 140683922831168] #011Training metrics: perplexity: 1.143 cross_entropy: 0.134 accuracy: 0.949 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:11 INFO 140683922831168] Epoch: 3, batches: 1900, num_examples: 972800, 14318.6 samples/sec, epoch time so far: 0:01:07.939484\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:11 INFO 140683922831168] #011Training metrics: perplexity: 1.144 cross_entropy: 0.134 accuracy: 0.949 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:13 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:13 INFO 140683922831168] Completed Epoch: 3, time taken: 0:01:09.942325\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:13 INFO 140683922831168] Epoch 3 Training metrics:   perplexity: 1.144 cross_entropy: 0.135 accuracy: 0.949 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:13 INFO 140683922831168] #quality_metric: host=algo-1, epoch=3, train cross_entropy <loss>=0.134727257892398\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:13 INFO 140683922831168] #quality_metric: host=algo-1, epoch=3, train accuracy <score>=0.9492027735173824\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] Epoch 3 Validation metrics: perplexity: 1.199 cross_entropy: 0.181 accuracy: 0.932 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] #quality_metric: host=algo-1, epoch=3, validation cross_entropy <loss>=0.18127504824581792\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] #quality_metric: host=algo-1, epoch=3, validation accuracy <score>=0.9315744173728814\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] patience losses: [0.19760655043488842, 0.20252395598059994]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] min patience losses: 0.19760655043488842\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] current loss: 0.18127504824581792\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] absolute loss difference: 0.016331502189070507\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970202.9951537, \"EndTime\": 1623970276.28434, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"early_stop.time\": {\"sum\": 14.819145202636719, \"count\": 1, \"min\": 14.819145202636719, \"max\": 14.819145202636719}, \"update.time\": {\"sum\": 73054.08263206482, \"count\": 1, \"min\": 73054.08263206482, \"max\": 73054.08263206482}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970203.2302203, \"EndTime\": 1623970276.284698, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4004352.0, \"count\": 1, \"min\": 4004352, \"max\": 4004352}, \"Total Batches Seen\": {\"sum\": 7821.0, \"count\": 1, \"min\": 7821, \"max\": 7821}, \"Max Records Seen Between Resets\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Max Batches Seen Between Resets\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Number of Batches Since Last Reset\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:16 INFO 140683922831168] #throughput_metric: host=algo-1, train throughput=13708.529113822617 records/second\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:20 INFO 140683922831168] Epoch: 4, batches: 100, num_examples: 51200, 14247.0 samples/sec, epoch time so far: 0:00:03.593748\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:20 INFO 140683922831168] #011Training metrics: perplexity: 1.123 cross_entropy: 0.116 accuracy: 0.957 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:23 INFO 140683922831168] Epoch: 4, batches: 200, num_examples: 102400, 14278.9 samples/sec, epoch time so far: 0:00:07.171437\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:23 INFO 140683922831168] #011Training metrics: perplexity: 1.123 cross_entropy: 0.116 accuracy: 0.957 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:27 INFO 140683922831168] Epoch: 4, batches: 300, num_examples: 153600, 14304.5 samples/sec, epoch time so far: 0:00:10.737857\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:27 INFO 140683922831168] #011Training metrics: perplexity: 1.123 cross_entropy: 0.116 accuracy: 0.957 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:31 INFO 140683922831168] Epoch: 4, batches: 400, num_examples: 204800, 14360.8 samples/sec, epoch time so far: 0:00:14.261067\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:31 INFO 140683922831168] #011Training metrics: perplexity: 1.124 cross_entropy: 0.117 accuracy: 0.957 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:34 INFO 140683922831168] Epoch: 4, batches: 500, num_examples: 256000, 14365.9 samples/sec, epoch time so far: 0:00:17.820025\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:34 INFO 140683922831168] #011Training metrics: perplexity: 1.125 cross_entropy: 0.118 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:38 INFO 140683922831168] Epoch: 4, batches: 600, num_examples: 307200, 14405.9 samples/sec, epoch time so far: 0:00:21.324593\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:38 INFO 140683922831168] #011Training metrics: perplexity: 1.126 cross_entropy: 0.118 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:41 INFO 140683922831168] Epoch: 4, batches: 700, num_examples: 358400, 14438.2 samples/sec, epoch time so far: 0:00:24.823095\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:41 INFO 140683922831168] #011Training metrics: perplexity: 1.126 cross_entropy: 0.118 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:45 INFO 140683922831168] Epoch: 4, batches: 800, num_examples: 409600, 14453.1 samples/sec, epoch time so far: 0:00:28.339921\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:45 INFO 140683922831168] #011Training metrics: perplexity: 1.127 cross_entropy: 0.119 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:48 INFO 140683922831168] Epoch: 4, batches: 900, num_examples: 460800, 14457.4 samples/sec, epoch time so far: 0:00:31.872859\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:48 INFO 140683922831168] #011Training metrics: perplexity: 1.127 cross_entropy: 0.119 accuracy: 0.956 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:52 INFO 140683922831168] Epoch: 4, batches: 1000, num_examples: 512000, 14476.0 samples/sec, epoch time so far: 0:00:35.368826\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:52 INFO 140683922831168] #011Training metrics: perplexity: 1.128 cross_entropy: 0.120 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:55 INFO 140683922831168] Epoch: 4, batches: 1100, num_examples: 563200, 14468.9 samples/sec, epoch time so far: 0:00:38.924834\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:55 INFO 140683922831168] #011Training metrics: perplexity: 1.128 cross_entropy: 0.120 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:59 INFO 140683922831168] Epoch: 4, batches: 1200, num_examples: 614400, 14461.6 samples/sec, epoch time so far: 0:00:42.485039\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:51:59 INFO 140683922831168] #011Training metrics: perplexity: 1.128 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:02 INFO 140683922831168] Epoch: 4, batches: 1300, num_examples: 665600, 14442.0 samples/sec, epoch time so far: 0:00:46.087642\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:02 INFO 140683922831168] #011Training metrics: perplexity: 1.128 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:06 INFO 140683922831168] Epoch: 4, batches: 1400, num_examples: 716800, 14394.3 samples/sec, epoch time so far: 0:00:49.797583\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:06 INFO 140683922831168] #011Training metrics: perplexity: 1.129 cross_entropy: 0.121 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:10 INFO 140683922831168] Epoch: 4, batches: 1500, num_examples: 768000, 14396.0 samples/sec, epoch time so far: 0:00:53.348072\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:10 INFO 140683922831168] #011Training metrics: perplexity: 1.130 cross_entropy: 0.122 accuracy: 0.955 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:13 INFO 140683922831168] Epoch: 4, batches: 1600, num_examples: 819200, 14412.9 samples/sec, epoch time so far: 0:00:56.838010\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:13 INFO 140683922831168] #011Training metrics: perplexity: 1.130 cross_entropy: 0.122 accuracy: 0.954 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:17 INFO 140683922831168] Epoch: 4, batches: 1700, num_examples: 870400, 14423.0 samples/sec, epoch time so far: 0:01:00.347985\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:17 INFO 140683922831168] #011Training metrics: perplexity: 1.130 cross_entropy: 0.122 accuracy: 0.954 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:20 INFO 140683922831168] Epoch: 4, batches: 1800, num_examples: 921600, 14427.5 samples/sec, epoch time so far: 0:01:03.878040\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:20 INFO 140683922831168] #011Training metrics: perplexity: 1.130 cross_entropy: 0.123 accuracy: 0.954 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:24 INFO 140683922831168] Epoch: 4, batches: 1900, num_examples: 972800, 14436.7 samples/sec, epoch time so far: 0:01:07.383611\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:24 INFO 140683922831168] #011Training metrics: perplexity: 1.131 cross_entropy: 0.123 accuracy: 0.954 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:26 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:26 INFO 140683922831168] Completed Epoch: 4, time taken: 0:01:09.358397\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:26 INFO 140683922831168] Epoch 4 Training metrics:   perplexity: 1.131 cross_entropy: 0.123 accuracy: 0.954 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:26 INFO 140683922831168] #quality_metric: host=algo-1, epoch=4, train cross_entropy <loss>=0.12289354092763367\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:26 INFO 140683922831168] #quality_metric: host=algo-1, epoch=4, train accuracy <score>=0.9542173919989775\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] Epoch 4 Validation metrics: perplexity: 1.217 cross_entropy: 0.197 accuracy: 0.931 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] #quality_metric: host=algo-1, epoch=4, validation cross_entropy <loss>=0.19688176300566076\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] #quality_metric: host=algo-1, epoch=4, validation accuracy <score>=0.9313592425847458\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] patience losses: [0.20252395598059994, 0.18127504824581792]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] min patience losses: 0.18127504824581792\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] current loss: 0.19688176300566076\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] absolute loss difference: 0.015606714759842849\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970276.2844546, \"EndTime\": 1623970349.1587791, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"early_stop.time\": {\"sum\": 0.6098747253417969, \"count\": 1, \"min\": 0.6098747253417969, \"max\": 0.6098747253417969}, \"update.time\": {\"sum\": 72381.66952133179, \"count\": 1, \"min\": 72381.66952133179, \"max\": 72381.66952133179}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970276.7770724, \"EndTime\": 1623970349.1591172, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5005824.0, \"count\": 1, \"min\": 5005824, \"max\": 5005824}, \"Total Batches Seen\": {\"sum\": 9777.0, \"count\": 1, \"min\": 9777, \"max\": 9777}, \"Max Records Seen Between Resets\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Max Batches Seen Between Resets\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Number of Batches Since Last Reset\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:29 INFO 140683922831168] #throughput_metric: host=algo-1, train throughput=13835.88194924601 records/second\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:32 INFO 140683922831168] Epoch: 5, batches: 100, num_examples: 51200, 14556.1 samples/sec, epoch time so far: 0:00:03.517416\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:32 INFO 140683922831168] #011Training metrics: perplexity: 1.119 cross_entropy: 0.112 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:36 INFO 140683922831168] Epoch: 5, batches: 200, num_examples: 102400, 14560.4 samples/sec, epoch time so far: 0:00:07.032792\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:36 INFO 140683922831168] #011Training metrics: perplexity: 1.117 cross_entropy: 0.110 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:39 INFO 140683922831168] Epoch: 5, batches: 300, num_examples: 153600, 14538.2 samples/sec, epoch time so far: 0:00:10.565278\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:39 INFO 140683922831168] #011Training metrics: perplexity: 1.115 cross_entropy: 0.108 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:43 INFO 140683922831168] Epoch: 5, batches: 400, num_examples: 204800, 14558.2 samples/sec, epoch time so far: 0:00:14.067661\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:43 INFO 140683922831168] #011Training metrics: perplexity: 1.114 cross_entropy: 0.108 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:46 INFO 140683922831168] Epoch: 5, batches: 500, num_examples: 256000, 14570.0 samples/sec, epoch time so far: 0:00:17.570405\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:46 INFO 140683922831168] #011Training metrics: perplexity: 1.114 cross_entropy: 0.108 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:50 INFO 140683922831168] Epoch: 5, batches: 600, num_examples: 307200, 14574.3 samples/sec, epoch time so far: 0:00:21.078136\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:50 INFO 140683922831168] #011Training metrics: perplexity: 1.115 cross_entropy: 0.109 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:53 INFO 140683922831168] Epoch: 5, batches: 700, num_examples: 358400, 14570.4 samples/sec, epoch time so far: 0:00:24.597892\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:53 INFO 140683922831168] #011Training metrics: perplexity: 1.115 cross_entropy: 0.109 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:57 INFO 140683922831168] Epoch: 5, batches: 800, num_examples: 409600, 14578.2 samples/sec, epoch time so far: 0:00:28.096761\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:52:57 INFO 140683922831168] #011Training metrics: perplexity: 1.115 cross_entropy: 0.109 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:00 INFO 140683922831168] Epoch: 5, batches: 900, num_examples: 460800, 14581.1 samples/sec, epoch time so far: 0:00:31.602653\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:00 INFO 140683922831168] #011Training metrics: perplexity: 1.116 cross_entropy: 0.110 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:04 INFO 140683922831168] Epoch: 5, batches: 1000, num_examples: 512000, 14538.3 samples/sec, epoch time so far: 0:00:35.217276\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:04 INFO 140683922831168] #011Training metrics: perplexity: 1.116 cross_entropy: 0.110 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:08 INFO 140683922831168] Epoch: 5, batches: 1100, num_examples: 563200, 14490.7 samples/sec, epoch time so far: 0:00:38.866286\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:08 INFO 140683922831168] #011Training metrics: perplexity: 1.117 cross_entropy: 0.111 accuracy: 0.960 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:11 INFO 140683922831168] Epoch: 5, batches: 1200, num_examples: 614400, 14494.2 samples/sec, epoch time so far: 0:00:42.389482\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:11 INFO 140683922831168] #011Training metrics: perplexity: 1.118 cross_entropy: 0.111 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:15 INFO 140683922831168] Epoch: 5, batches: 1300, num_examples: 665600, 14509.8 samples/sec, epoch time so far: 0:00:45.872315\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:15 INFO 140683922831168] #011Training metrics: perplexity: 1.118 cross_entropy: 0.112 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:18 INFO 140683922831168] Epoch: 5, batches: 1400, num_examples: 716800, 14518.6 samples/sec, epoch time so far: 0:00:49.371317\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:18 INFO 140683922831168] #011Training metrics: perplexity: 1.118 cross_entropy: 0.112 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:22 INFO 140683922831168] Epoch: 5, batches: 1500, num_examples: 768000, 14523.5 samples/sec, epoch time so far: 0:00:52.879838\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:22 INFO 140683922831168] #011Training metrics: perplexity: 1.119 cross_entropy: 0.112 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:25 INFO 140683922831168] Epoch: 5, batches: 1600, num_examples: 819200, 14524.1 samples/sec, epoch time so far: 0:00:56.402667\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:25 INFO 140683922831168] #011Training metrics: perplexity: 1.119 cross_entropy: 0.112 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:29 INFO 140683922831168] Epoch: 5, batches: 1700, num_examples: 870400, 14524.4 samples/sec, epoch time so far: 0:00:59.926803\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:29 INFO 140683922831168] #011Training metrics: perplexity: 1.119 cross_entropy: 0.113 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:32 INFO 140683922831168] Epoch: 5, batches: 1800, num_examples: 921600, 14520.8 samples/sec, epoch time so far: 0:01:03.467735\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:32 INFO 140683922831168] #011Training metrics: perplexity: 1.119 cross_entropy: 0.113 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:36 INFO 140683922831168] Epoch: 5, batches: 1900, num_examples: 972800, 14513.2 samples/sec, epoch time so far: 0:01:07.028477\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:36 INFO 140683922831168] #011Training metrics: perplexity: 1.120 cross_entropy: 0.113 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:38 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:38 INFO 140683922831168] Completed Epoch: 5, time taken: 0:01:09.009860\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:38 INFO 140683922831168] Epoch 5 Training metrics:   perplexity: 1.120 cross_entropy: 0.113 accuracy: 0.959 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:38 INFO 140683922831168] #quality_metric: host=algo-1, epoch=5, train cross_entropy <loss>=0.11344317137501783\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:38 INFO 140683922831168] #quality_metric: host=algo-1, epoch=5, train accuracy <score>=0.9585629952709611\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] Epoch 5 Validation metrics: perplexity: 1.212 cross_entropy: 0.193 accuracy: 0.935 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] #quality_metric: host=algo-1, epoch=5, validation cross_entropy <loss>=0.1930039009805453\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] #quality_metric: host=algo-1, epoch=5, validation accuracy <score>=0.934719279661017\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] patience losses: [0.18127504824581792, 0.19688176300566076]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] min patience losses: 0.18127504824581792\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] current loss: 0.1930039009805453\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] absolute loss difference: 0.011728852734727374\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970349.158901, \"EndTime\": 1623970421.441029, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"early_stop.time\": {\"sum\": 0.5507469177246094, \"count\": 1, \"min\": 0.5507469177246094, \"max\": 0.5507469177246094}, \"update.time\": {\"sum\": 72067.22235679626, \"count\": 1, \"min\": 72067.22235679626, \"max\": 72067.22235679626}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970349.3737786, \"EndTime\": 1623970421.4412909, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 6007296.0, \"count\": 1, \"min\": 6007296, \"max\": 6007296}, \"Total Batches Seen\": {\"sum\": 11733.0, \"count\": 1, \"min\": 11733, \"max\": 11733}, \"Max Records Seen Between Resets\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Max Batches Seen Between Resets\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Number of Batches Since Last Reset\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:41 INFO 140683922831168] #throughput_metric: host=algo-1, train throughput=13896.27145913053 records/second\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:45 INFO 140683922831168] Epoch: 6, batches: 100, num_examples: 51200, 14560.4 samples/sec, epoch time so far: 0:00:03.516385\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:45 INFO 140683922831168] #011Training metrics: perplexity: 1.108 cross_entropy: 0.103 accuracy: 0.963 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:48 INFO 140683922831168] Epoch: 6, batches: 200, num_examples: 102400, 14465.1 samples/sec, epoch time so far: 0:00:07.079129\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:48 INFO 140683922831168] #011Training metrics: perplexity: 1.108 cross_entropy: 0.102 accuracy: 0.962 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:52 INFO 140683922831168] Epoch: 6, batches: 300, num_examples: 153600, 14454.4 samples/sec, epoch time so far: 0:00:10.626530\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:52 INFO 140683922831168] #011Training metrics: perplexity: 1.107 cross_entropy: 0.101 accuracy: 0.963 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:55 INFO 140683922831168] Epoch: 6, batches: 400, num_examples: 204800, 14447.4 samples/sec, epoch time so far: 0:00:14.175543\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:55 INFO 140683922831168] #011Training metrics: perplexity: 1.106 cross_entropy: 0.101 accuracy: 0.963 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:59 INFO 140683922831168] Epoch: 6, batches: 500, num_examples: 256000, 14435.9 samples/sec, epoch time so far: 0:00:17.733526\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:53:59 INFO 140683922831168] #011Training metrics: perplexity: 1.106 cross_entropy: 0.101 accuracy: 0.963 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:02 INFO 140683922831168] Epoch: 6, batches: 600, num_examples: 307200, 14403.0 samples/sec, epoch time so far: 0:00:21.328937\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:02 INFO 140683922831168] #011Training metrics: perplexity: 1.107 cross_entropy: 0.102 accuracy: 0.963 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:06 INFO 140683922831168] Epoch: 6, batches: 700, num_examples: 358400, 14299.3 samples/sec, epoch time so far: 0:00:25.064219\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:06 INFO 140683922831168] #011Training metrics: perplexity: 1.107 cross_entropy: 0.102 accuracy: 0.963 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:10 INFO 140683922831168] Epoch: 6, batches: 800, num_examples: 409600, 14317.3 samples/sec, epoch time so far: 0:00:28.608697\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:10 INFO 140683922831168] #011Training metrics: perplexity: 1.108 cross_entropy: 0.103 accuracy: 0.962 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:13 INFO 140683922831168] Epoch: 6, batches: 900, num_examples: 460800, 14336.3 samples/sec, epoch time so far: 0:00:32.142143\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:13 INFO 140683922831168] #011Training metrics: perplexity: 1.109 cross_entropy: 0.103 accuracy: 0.962 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:17 INFO 140683922831168] Epoch: 6, batches: 1000, num_examples: 512000, 14348.7 samples/sec, epoch time so far: 0:00:35.682742\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:17 INFO 140683922831168] #011Training metrics: perplexity: 1.109 cross_entropy: 0.103 accuracy: 0.962 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:20 INFO 140683922831168] Epoch: 6, batches: 1100, num_examples: 563200, 14347.2 samples/sec, epoch time so far: 0:00:39.255170\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:20 INFO 140683922831168] #011Training metrics: perplexity: 1.109 cross_entropy: 0.104 accuracy: 0.962 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:24 INFO 140683922831168] Epoch: 6, batches: 1200, num_examples: 614400, 14347.8 samples/sec, epoch time so far: 0:00:42.822016\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:24 INFO 140683922831168] #011Training metrics: perplexity: 1.110 cross_entropy: 0.104 accuracy: 0.962 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:28 INFO 140683922831168] Epoch: 6, batches: 1300, num_examples: 665600, 14351.7 samples/sec, epoch time so far: 0:00:46.377895\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:28 INFO 140683922831168] #011Training metrics: perplexity: 1.110 cross_entropy: 0.105 accuracy: 0.962 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:31 INFO 140683922831168] Epoch: 6, batches: 1400, num_examples: 716800, 14351.6 samples/sec, epoch time so far: 0:00:49.945607\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:31 INFO 140683922831168] #011Training metrics: perplexity: 1.111 cross_entropy: 0.105 accuracy: 0.961 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:35 INFO 140683922831168] Epoch: 6, batches: 1500, num_examples: 768000, 14350.6 samples/sec, epoch time so far: 0:00:53.516960\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:35 INFO 140683922831168] #011Training metrics: perplexity: 1.111 cross_entropy: 0.106 accuracy: 0.961 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:38 INFO 140683922831168] Epoch: 6, batches: 1600, num_examples: 819200, 14347.5 samples/sec, epoch time so far: 0:00:57.096910\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:38 INFO 140683922831168] #011Training metrics: perplexity: 1.112 cross_entropy: 0.106 accuracy: 0.961 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:42 INFO 140683922831168] Epoch: 6, batches: 1700, num_examples: 870400, 14350.7 samples/sec, epoch time so far: 0:01:00.652203\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:42 INFO 140683922831168] #011Training metrics: perplexity: 1.112 cross_entropy: 0.106 accuracy: 0.961 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:45 INFO 140683922831168] Epoch: 6, batches: 1800, num_examples: 921600, 14358.1 samples/sec, epoch time so far: 0:01:04.186581\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:45 INFO 140683922831168] #011Training metrics: perplexity: 1.112 cross_entropy: 0.106 accuracy: 0.961 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:49 INFO 140683922831168] Epoch: 6, batches: 1900, num_examples: 972800, 14358.8 samples/sec, epoch time so far: 0:01:07.749385\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:49 INFO 140683922831168] #011Training metrics: perplexity: 1.112 cross_entropy: 0.107 accuracy: 0.961 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:51 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:51 INFO 140683922831168] Completed Epoch: 6, time taken: 0:01:09.752853\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:51 INFO 140683922831168] Epoch 6 Training metrics:   perplexity: 1.113 cross_entropy: 0.107 accuracy: 0.961 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:51 INFO 140683922831168] #quality_metric: host=algo-1, epoch=6, train cross_entropy <loss>=0.10668743990079818\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:51 INFO 140683922831168] #quality_metric: host=algo-1, epoch=6, train accuracy <score>=0.9609185279268916\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] Epoch 6 Validation metrics: perplexity: 1.218 cross_entropy: 0.198 accuracy: 0.931 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] #quality_metric: host=algo-1, epoch=6, validation cross_entropy <loss>=0.1976500587948298\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] #quality_metric: host=algo-1, epoch=6, validation accuracy <score>=0.9313426906779662\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] **************\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] patience losses: [0.19688176300566076, 0.1930039009805453]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] min patience losses: 0.1930039009805453\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] current loss: 0.1976500587948298\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] absolute loss difference: 0.004646157814284513\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] Early stopping criterion met! Stopping training at epoch: 6\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970421.4411254, \"EndTime\": 1623970494.4822855, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"early_stop.time\": {\"sum\": 0.7569789886474609, \"count\": 1, \"min\": 0.7569789886474609, \"max\": 0.7569789886474609}, \"update.time\": {\"sum\": 72839.76531028748, \"count\": 1, \"min\": 72839.76531028748, \"max\": 72839.76531028748}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970421.642486, \"EndTime\": 1623970494.4826891, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 7008768.0, \"count\": 1, \"min\": 7008768, \"max\": 7008768}, \"Total Batches Seen\": {\"sum\": 13689.0, \"count\": 1, \"min\": 13689, \"max\": 13689}, \"Max Records Seen Between Resets\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Max Batches Seen Between Resets\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 1001472.0, \"count\": 1, \"min\": 1001472, \"max\": 1001472}, \"Number of Batches Since Last Reset\": {\"sum\": 1956.0, \"count\": 1, \"min\": 1956, \"max\": 1956}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] #throughput_metric: host=algo-1, train throughput=13748.854471847568 records/second\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 WARNING 140683922831168] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] Best model based on epoch 3. Best loss: 0.181\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970494.4823806, \"EndTime\": 1623970494.4873104, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 4.147052764892578, \"count\": 1, \"min\": 4.147052764892578, \"max\": 4.147052764892578}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:54 INFO 140683922831168] Saved checkpoint to \"/tmp/tmpsg7thk2p/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] Finished scoring on 60416 examples from 118 batches, each of size 512.\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] Test Metrics:perplexity: 1.198 cross_entropy: 0.181 accuracy: 0.930 \u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] Test Metric names:['perplexity', 'cross_entropy', 'accuracy']\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] Test Metric values:[1.1981925797945063, 0.18081413600909507, 0.9302006091101694]\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] Test Metric names and values:[('perplexity', 1.1981925797945063), ('cross_entropy', 0.18081413600909507), ('accuracy', 0.9302006091101694)]\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970495.8355649, \"EndTime\": 1623970498.9607232, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 60416.0, \"count\": 1, \"min\": 60416, \"max\": 60416}, \"Total Batches Seen\": {\"sum\": 118.0, \"count\": 1, \"min\": 118, \"max\": 118}, \"Max Records Seen Between Resets\": {\"sum\": 60416.0, \"count\": 1, \"min\": 60416, \"max\": 60416}, \"Max Batches Seen Between Resets\": {\"sum\": 118.0, \"count\": 1, \"min\": 118, \"max\": 118}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 60416.0, \"count\": 1, \"min\": 60416, \"max\": 60416}, \"Number of Batches Since Last Reset\": {\"sum\": 118.0, \"count\": 1, \"min\": 118, \"max\": 118}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] #test_score (algo-1) : ('perplexity', 1.1981925797945063)\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] #test_score (algo-1) : ('cross_entropy', 0.18081413600909507)\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] #test_score (algo-1) : ('accuracy', 0.9302006091101694)\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] #quality_metric: host=algo-1, test cross_entropy <loss>=0.18081413600909507\u001b[0m\n",
      "\u001b[34m[06/17/2021 22:54:58 INFO 140683922831168] #quality_metric: host=algo-1, test accuracy <score>=0.9302006091101694\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1623970494.487443, \"EndTime\": 1623970498.9910002, \"Dimensions\": {\"Algorithm\": \"ObjectToVec\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 1347.6612567901611, \"count\": 1, \"min\": 1347.6612567901611, \"max\": 1347.6612567901611}, \"model.score.time\": {\"sum\": 3125.0762939453125, \"count\": 1, \"min\": 3125.0762939453125, \"max\": 3125.0762939453125}, \"setuptime\": {\"sum\": 10.050773620605469, \"count\": 1, \"min\": 10.050773620605469, \"max\": 10.050773620605469}, \"totaltime\": {\"sum\": 579416.9428348541, \"count\": 1, \"min\": 579416.9428348541, \"max\": 579416.9428348541}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-17 22:55:17 Uploading - Uploading generated training model\n",
      "2021-06-17 22:56:24 Completed - Training job completed\n",
      "ProfilerReport-1623969612: NoIssuesFound\n",
      "Training seconds: 756\n",
      "Billable seconds: 756\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters\n",
    "doc2vec.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# fit estimator with data\n",
    "doc2vec.fit({\"train\": s3_train, \"validation\": s3_valid, \"test\": s3_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "economic-elite",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy model\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "doc2vec_model = doc2vec.create_model(\n",
    "    #serializer=json_serializer, deserializer=json_deserializer, content_type=\"application/json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "capable-administrator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "predictor = doc2vec_model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=JSONSerializer(), \n",
    "    deserialzier=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-amber",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Apply learned embeddings to document retrieval task\n",
    "\n",
    "After training the model, we can use the encoders in Object2Vec to map new articles and sentences into a shared embedding space. Then we evaluate the quality of these embeddings with a downstream document retrieval task.\n",
    "\n",
    "In the retrieval task, given a sentence query, the trained algorithm needs to find its best matching document (the ground-truth document is the one that contains it) from a pool of documents, where the pool contains 10,000 other non ground-truth documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "nominated-oakland",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_tokenized_articles_from_single_file(fname, word_dict):\n",
    "    for article in get_article_iter_from_file(fname):\n",
    "        integer_article = []\n",
    "        for sent in readlines_from_article(article):\n",
    "            integer_article += sentence_to_integers(sent, word_dict)\n",
    "        yield integer_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "extraordinary-album",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_jsonline(fname):\n",
    "    \"\"\"\n",
    "    Reads jsonline files and returns iterator\n",
    "    \"\"\"\n",
    "    with jsonlines.open(fname) as reader:\n",
    "        for line in reader:\n",
    "            yield line\n",
    "\n",
    "\n",
    "def send_payload(predictor, payload):\n",
    "    return predictor.predict(payload)\n",
    "\n",
    "\n",
    "def write_to_jsonlines(data, fname):\n",
    "    with jsonlines.open(fname, \"a\") as writer:\n",
    "        data = data[\"predictions\"]\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "def eval_and_write(predictor, fname, to_fname, batch_size):\n",
    "    if os.path.exists(to_fname):\n",
    "        print(\"Removing exisiting embedding file {}\".format(to_fname))\n",
    "        os.remove(to_fname)\n",
    "    print(\"Getting embedding of data in {} and store to {}...\".format(fname, to_fname))\n",
    "    test_data_content = list(read_jsonline(fname))\n",
    "    n_test = len(test_data_content)\n",
    "    n_batches = math.ceil(n_test / float(batch_size))\n",
    "    start = 0\n",
    "    for idx in range(n_batches):\n",
    "        if idx % 10 == 0:\n",
    "            print(\"Inference on the {}-th batch\".format(idx + 1))\n",
    "        end = (start + batch_size) if (start + batch_size) <= n_test else n_test\n",
    "        payload = {\"instances\": test_data_content[start:end]}\n",
    "        data = send_payload(predictor, payload)\n",
    "        data = json.loads(data)\n",
    "        write_to_jsonlines(data, to_fname)\n",
    "        start = end\n",
    "\n",
    "\n",
    "def get_embeddings(predictor, test_data_content, batch_size):\n",
    "    n_test = len(test_data_content)\n",
    "    n_batches = math.ceil(n_test / float(batch_size))\n",
    "    start = 0\n",
    "    embeddings = []\n",
    "    for idx in range(n_batches):\n",
    "        if idx % 10 == 0:\n",
    "            print(\"Inference the {}-th batch\".format(idx + 1))\n",
    "        end = (start + batch_size) if (start + batch_size) <= n_test else n_test\n",
    "        payload = {\"instances\": test_data_content[start:end]}\n",
    "        data = send_payload(predictor, payload)\n",
    "        data = json.loads(data)\n",
    "        embeddings += data[\"predictions\"]\n",
    "        start = end\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "supposed-tsunami",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "basedocs_fpath = os.path.join(datadir, \"wikipedia_test_basedocs.txt\")\n",
    "test_fpath = \"{}_tokenized-nsr{}.jsonl\".format(\"test10k\", test_nsr)\n",
    "eval_basedocs = \"test_basedocs_tokenized_in0.jsonl\"\n",
    "basedocs_emb = \"test_basedocs_embeddings.jsonl\"\n",
    "sent_doc_emb = \"test10k_embeddings_pairs.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "welsh-exploration",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import numpy as np\n",
    "\n",
    "basedocs_emb = \"test_basedocs_embeddings.jsonl\"\n",
    "sent_doc_emb = \"test10k_embeddings_pairs.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-speed",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# tokenize basedocs\n",
    "with jsonlines.open(eval_basedocs, \"w\") as writer:\n",
    "    for data in generate_tokenized_articles_from_single_file(basedocs_fpath, w_dict):\n",
    "        writer.write({\"in0\": data})\n",
    "\n",
    "# get basedocs embedding\n",
    "eval_and_write(predictor, eval_basedocs, basedocs_emb, batch_size)\n",
    "\n",
    "\n",
    "# get embeddings for sentence and ground-truth article pairs\n",
    "sentences = []\n",
    "gt_articles = []\n",
    "for data in read_jsonline(test_fpath):\n",
    "    if data[\"label\"] == 1:\n",
    "        sentences.append({\"in0\": data[\"in0\"]})\n",
    "        gt_articles.append({\"in0\": data[\"in1\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "understanding-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference the 1-th batch\n",
      "Inference the 11-th batch\n",
      "Inference the 21-th batch\n",
      "Inference the 31-th batch\n",
      "Inference the 41-th batch\n",
      "Inference the 51-th batch\n",
      "Inference the 61-th batch\n",
      "Inference the 71-th batch\n",
      "Inference the 81-th batch\n",
      "Inference the 91-th batch\n",
      "Inference the 1-th batch\n",
      "Inference the 11-th batch\n",
      "Inference the 21-th batch\n",
      "Inference the 31-th batch\n",
      "Inference the 41-th batch\n",
      "Inference the 51-th batch\n",
      "Inference the 61-th batch\n",
      "Inference the 71-th batch\n",
      "Inference the 81-th batch\n",
      "Inference the 91-th batch\n"
     ]
    }
   ],
   "source": [
    "sent_emb = get_embeddings(predictor, sentences, batch_size)\n",
    "doc_emb = get_embeddings(predictor, gt_articles, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "urban-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(sent_doc_emb, \"w\") as writer:\n",
    "    for (sent, doc) in zip(sent_emb, doc_emb):\n",
    "        writer.write({\"sent\": sent[\"embeddings\"], \"doc\": doc[\"embeddings\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "practical-holder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "thousand-douglas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-affect",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del w_dict\n",
    "del sent_emb, doc_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-space",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "The blocks below evaluate the performance of Object2Vec model on the document retrieval task.\n",
    "\n",
    "We use two metrics hits@k and mean rank to evaluate the retrieval performance. Note that the ground-truth documents in the pool have the query sentence removed from them -- else the task would have been trivial.\n",
    "\n",
    "* hits@k:  It calculates the fraction of queries where its best-matching (ground-truth) document is contained in top k retrieved documents by the algorithm.\n",
    "* mean rank: It is the average rank of the best-matching documents, as determined by the algorithm, over all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fewer-adrian",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct normalized basedocs, sentences, and ground-truth docs embedding matrix\n",
    "\n",
    "basedocs = []\n",
    "with jsonlines.open(basedocs_emb) as reader:\n",
    "    for line in reader:\n",
    "        basedocs.append(np.array(line[\"embeddings\"]))\n",
    "\n",
    "\n",
    "sent_embs = []\n",
    "gt_doc_embs = []\n",
    "\n",
    "with jsonlines.open(sent_doc_emb) as reader2:\n",
    "    for line2 in reader2:\n",
    "        sent_embs.append(line2[\"sent\"])\n",
    "        gt_doc_embs.append(line2[\"doc\"])\n",
    "\n",
    "basedocs_emb_mat = normalize(np.array(basedocs).T, axis=0)\n",
    "sent_emb_mat = normalize(np.array(sent_embs), axis=1)\n",
    "gt_emb_mat = normalize(np.array(gt_doc_embs).T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "synthetic-storm",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_chunk_query_rank(sent_emb_mat, basedocs_emb_mat, gt_emb_mat, largest_k):\n",
    "    # this is a memory-consuming step if chunk is large\n",
    "    dot_with_basedocs = np.matmul(sent_emb_mat, basedocs_emb_mat)\n",
    "    dot_with_gt = np.diag(np.matmul(sent_emb_mat, gt_emb_mat))\n",
    "    final_ranking_scores = np.insert(dot_with_basedocs, 0, dot_with_gt, axis=1)\n",
    "    query_rankings = list()\n",
    "    largest_k_list = list()\n",
    "    for row in final_ranking_scores:\n",
    "        ranking_ind = np.argsort(row)  # sorts row in increasing order of similarity score\n",
    "        num_scores = len(ranking_ind)\n",
    "        query_rankings.append(num_scores - list(ranking_ind).index(0))\n",
    "        largest_k_list.append(np.array(ranking_ind[-largest_k:]).astype(int))\n",
    "    return query_rankings, largest_k_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-roller",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "`Note: We evaluate the learned embeddings on chunks of test sentences-document pairs to save run-time memory; this is to make sure that our code works on the smallest notebook instance *ml.t2.medium*. If you have a larger notebook instance, you can increase the chunk_size to speed up evaluation. For instances larger than ml.t2.xlarge, you can set chunk_size = num_test_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "specified-calibration",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on the 0-th chunk\n",
      "Evaluating on the 1000-th chunk\n",
      "Evaluating on the 2000-th chunk\n",
      "Evaluating on the 3000-th chunk\n",
      "Evaluating on the 4000-th chunk\n",
      "Evaluating on the 5000-th chunk\n",
      "Evaluating on the 6000-th chunk\n",
      "Evaluating on the 7000-th chunk\n",
      "Evaluating on the 8000-th chunk\n",
      "Evaluating on the 9000-th chunk\n",
      "Summary:\n",
      "Mean query ranks is 242.2205\n",
      "Percentiles of query ranks is 50%:13.0, 80%:185.0, 90%:611.1000000000004, 99%:3848.1600000000035\n",
      "The hits at 1 score is 2684/10000\n",
      "The hits at 5 score is 4111/10000\n",
      "The hits at 10 score is 4795/10000\n",
      "The hits at 20 score is 5530/10000\n",
      "The hits at 50 score is 6556/10000\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1000\n",
    "num_test_samples = len(sent_embs)\n",
    "assert num_test_samples % chunk_size == 0, \"Chunk_size must be divisible by {}\".format(\n",
    "    num_test_samples\n",
    ")\n",
    "num_chunks = int(num_test_samples / chunk_size)\n",
    "k_list = [1, 5, 10, 20, 50]\n",
    "largest_k = max(k_list)\n",
    "query_all_rankings = list()\n",
    "all_largest_k_list = list()\n",
    "\n",
    "for i in range(0, num_chunks * chunk_size, chunk_size):\n",
    "    print(\"Evaluating on the {}-th chunk\".format(i))\n",
    "    j = i + chunk_size\n",
    "    sent_emb_submat = sent_emb_mat[i:j, :]\n",
    "    gt_emb_submat = gt_emb_mat[:, i:j]\n",
    "    query_rankings, largest_k_list = get_chunk_query_rank(\n",
    "        sent_emb_submat, basedocs_emb_mat, gt_emb_submat, largest_k\n",
    "    )\n",
    "    query_all_rankings += query_rankings\n",
    "    all_largest_k_list.append(np.array(largest_k_list).astype(int))\n",
    "\n",
    "all_largest_k_mat = np.concatenate(all_largest_k_list, axis=0).astype(int)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(\"Mean query ranks is {}\".format(np.mean(query_all_rankings)))\n",
    "print(\n",
    "    \"Percentiles of query ranks is 50%:{}, 80%:{}, 90%:{}, 99%:{}\".format(\n",
    "        *np.percentile(query_all_rankings, [50, 80, 90, 99])\n",
    "    )\n",
    ")\n",
    "\n",
    "for k in k_list:\n",
    "    top_k_mat = all_largest_k_mat[:, -k:]\n",
    "    unique, counts = np.unique(top_k_mat, return_counts=True)\n",
    "    print(\"The hits at {} score is {}/{}\".format(k, counts[0], len(top_k_mat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-deposit",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Comparison with the StarSpace algorithm \n",
    "\n",
    "We compare the performance of Object2Vec with the StarSpace (https://github.com/facebookresearch/StarSpace) algorithm on the document retrieval evaluation task, using a set of 250 thousand Wikipedia documents. The experimental results displayed in the table below, show that Object2Vec significantly outperforms StarSpace on all metrics although both models use the same kind of encoders for sentences and documents.\n",
    "\n",
    "\n",
    "| Algorithm      | hits@1       | hits@10      | hits@20      |  mean rank  |\n",
    "| :------------- | :----------: | :-----------:| :----------: | ----------: |\n",
    "|  StarSpace     | 21.98%       | 42.77%       | 50.55%       |  303.34     |\n",
    "|  Object2Vec    | 26.40%       | 47.42%       | 53.83%       |  248.67     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "nominated-journey",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 375.837456,
   "end_time": "2021-06-17T00:12:50.765156",
   "environment_variables": {},
   "exception": true,
   "input_path": "object2vec_document_embedding.ipynb",
   "output_path": "/opt/ml/processing/output/object2vec_document_embedding-2021-06-17-00-02-54.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-06-17T00:06:34.927700",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
