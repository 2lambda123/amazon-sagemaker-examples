{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infrared-teens",
   "metadata": {
    "papermill": {
     "duration": 0.009132,
     "end_time": "2021-05-25T00:20:13.803389",
     "exception": false,
     "start_time": "2021-05-25T00:20:13.794257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Converting the Parquet data format to recordIO-wrapped protobuf\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Optional data ingestion](#Optional-data-ingestion)\n",
    "    1. [Download the data](#Download-the-data)\n",
    "    1. [Convert into Parquet format](#Convert-into-Parquet-format)\n",
    "1. [Data conversion](#Data-conversion)\n",
    "    1. [Convert to recordIO protobuf format](#Convert-to-recordIO-protobuf-format)\n",
    "    1. [Upload to S3](#Upload-to-S3)\n",
    "1. [Training the linear model](#Training-the-linear-model)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "In this notebook we illustrate how to convert a Parquet data format into the recordIO-protobuf format that many SageMaker algorithms consume. For the demonstration, first we'll convert the publicly available MNIST dataset into the Parquet format. Subsequently, it is converted into the recordIO-protobuf format and uploaded to S3 for consumption by the linear learner algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "endless-consultation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:20:13.825549Z",
     "iopub.status.busy": "2021-05-25T00:20:13.825107Z",
     "iopub.status.idle": "2021-05-25T00:20:14.969724Z",
     "shell.execute_reply": "2021-05-25T00:20:14.969332Z"
    },
    "isConfigCell": true,
    "papermill": {
     "duration": 1.157267,
     "end_time": "2021-05-25T00:20:14.969840",
     "exception": false,
     "start_time": "2021-05-25T00:20:13.812573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-helping",
   "metadata": {
    "papermill": {
     "duration": 0.063811,
     "end_time": "2021-05-25T00:20:54.554009",
     "exception": false,
     "start_time": "2021-05-25T00:20:54.490198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Optional data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-layer",
   "metadata": {
    "papermill": {
     "duration": 0.062853,
     "end_time": "2021-05-25T00:20:54.679597",
     "exception": false,
     "start_time": "2021-05-25T00:20:54.616744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "north-exhibit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE model/\n",
      "2020-11-18 02:38:22          0 \n",
      "2020-12-28 19:34:02  220080426 mnist.pkl\n",
      "2021-05-20 17:34:52   16168813 mnist.pkl.gz\n",
      "2020-11-18 02:41:35    1648877 t10k-images-idx3-ubyte.gz\n",
      "2020-11-18 02:41:27       4542 t10k-labels-idx1-ubyte.gz\n",
      "2020-11-18 02:41:45    9912422 train-images-idx3-ubyte.gz\n",
      "2020-11-18 02:41:28      28881 train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# view the s3 uri of the MNIST dataset\n",
    "!aws s3 ls s3://sagemaker-sample-files/datasets/image/MNIST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "infrared-saskatchewan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:20:54.810589Z",
     "iopub.status.busy": "2021-05-25T00:20:54.810120Z",
     "iopub.status.idle": "2021-05-25T00:23:05.312584Z",
     "shell.execute_reply": "2021-05-25T00:23:05.312945Z"
    },
    "papermill": {
     "duration": 130.570847,
     "end_time": "2021-05-25T00:23:05.313083",
     "exception": false,
     "start_time": "2021-05-25T00:20:54.742236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.1 ms, sys: 34.2 ms, total: 89.3 ms\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, json\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "obj = s3.get_object(Bucket='sagemaker-sample-files', \n",
    "                    Key=\"datasets/image/MNIST/mnist.pkl.gz\")['Body']\n",
    "\n",
    "with open('/tmp/mnist.pkl.gz', 'wb') as f:\n",
    "    f.write(obj.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "confidential-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with gzip.open(\"/tmp/mnist.pkl.gz\", \"rb\") as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "honey-durham",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T00:23:05.445087Z",
     "iopub.status.busy": "2021-05-25T00:23:05.444614Z",
     "iopub.status.idle": "2021-05-25T00:23:05.464245Z",
     "shell.execute_reply": "2021-05-25T00:23:05.463390Z"
    },
    "papermill": {
     "duration": 0.088254,
     "end_time": "2021-05-25T00:23:05.464435",
     "exception": true,
     "start_time": "2021-05-25T00:23:05.376181",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastparquet import write\n",
    "from fastparquet import ParquetFile\n",
    "\n",
    "\n",
    "def save_as_parquet_file(dataset, filename, label_col):\n",
    "    X = dataset[0]\n",
    "    y = dataset[1]\n",
    "    data = pd.DataFrame(X)\n",
    "    data[label_col] = y\n",
    "    data.columns = data.columns.astype(str)  # Parquet expexts the column names to be strings\n",
    "    write(filename, data)\n",
    "\n",
    "\n",
    "def read_parquet_file(filename):\n",
    "    pf = ParquetFile(filename)\n",
    "    return pf.to_pandas()\n",
    "\n",
    "\n",
    "def features_and_target(df, label_col):\n",
    "    X = df.loc[:, df.columns != label_col].values\n",
    "    y = df[label_col].values\n",
    "    return [X, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-lawsuit",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Convert into Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "behavioral-component",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainFile = \"train.parquet\"\n",
    "validFile = \"valid.parquet\"\n",
    "testFile = \"test.parquet\"\n",
    "label_col = \"target\"\n",
    "\n",
    "save_as_parquet_file(train_set, trainFile, label_col)\n",
    "save_as_parquet_file(valid_set, validFile, label_col)\n",
    "save_as_parquet_file(test_set, testFile, label_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-prescription",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. E.g., the Amazon SageMaker implementation of Linear Learner takes recordIO-wrapped protobuf. Most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as `sagemaker` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fossil-disclosure",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfTrain = read_parquet_file(trainFile)\n",
    "dfValid = read_parquet_file(validFile)\n",
    "dfTest = read_parquet_file(testFile)\n",
    "\n",
    "train_X, train_y = features_and_target(dfTrain, label_col)\n",
    "valid_X, valid_y = features_and_target(dfValid, label_col)\n",
    "test_X, test_y = features_and_target(dfTest, label_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-wisdom",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Convert to recordIO protobuf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cardiovascular-snowboard",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "trainVectors = np.array([t.tolist() for t in train_X]).astype(\"float32\")\n",
    "trainLabels = np.where(np.array([t.tolist() for t in train_y]) == 0, 1, 0).astype(\"float32\")\n",
    "\n",
    "bufTrain = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(bufTrain, trainVectors, trainLabels)\n",
    "bufTrain.seek(0)\n",
    "\n",
    "\n",
    "validVectors = np.array([t.tolist() for t in valid_X]).astype(\"float32\")\n",
    "validLabels = np.where(np.array([t.tolist() for t in valid_y]) == 0, 1, 0).astype(\"float32\")\n",
    "\n",
    "bufValid = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(bufValid, validVectors, validLabels)\n",
    "bufValid.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "israeli-rebel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def write_numpy_to_dense_tensor(file, array, labels=None):\n",
      "    \"\"\"Writes a numpy array to a dense tensor\n",
      "\n",
      "    Args:\n",
      "        file:\n",
      "        array:\n",
      "        labels:\n",
      "    \"\"\"\n",
      "\n",
      "    # Validate shape of array and labels, resolve array and label types\n",
      "    if not len(array.shape) == 2:\n",
      "        raise ValueError(\"Array must be a Matrix\")\n",
      "    if labels is not None:\n",
      "        if not len(labels.shape) == 1:\n",
      "            raise ValueError(\"Labels must be a Vector\")\n",
      "        if labels.shape[0] not in array.shape:\n",
      "            raise ValueError(\n",
      "                \"Label shape {} not compatible with array shape {}\".format(\n",
      "                    labels.shape, array.shape\n",
      "                )\n",
      "            )\n",
      "        resolved_label_type = _resolve_type(labels.dtype)\n",
      "    resolved_type = _resolve_type(array.dtype)\n",
      "\n",
      "    # Write each vector in array into a Record in the file object\n",
      "    record = Record()\n",
      "    for index, vector in enumerate(array):\n",
      "        record.Clear()\n",
      "        _write_feature_tensor(resolved_type, record, vector)\n",
      "        if labels is not None:\n",
      "            _write_label_tensor(resolved_label_type, record, labels[index])\n",
      "        _write_recordio(file, record.SerializeToString())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(smac.write_numpy_to_dense_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "decreased-stanley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bufTrain.seek(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-voice",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adaptive-facial",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded training data location: s3://sagemaker-us-west-2-688520471316/sagemaker/DEMO-parquet/train/recordio-pb-data\n",
      "uploaded validation data location: s3://sagemaker-us-west-2-688520471316/sagemaker/DEMO-parquet/validation/recordio-pb-data\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "key = \"recordio-pb-data\"\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(os.path.join(prefix, \"train\", key)).upload_fileobj(\n",
    "    bufTrain\n",
    ")\n",
    "s3_train_data = \"s3://{}/{}/train/{}\".format(bucket, prefix, key)\n",
    "print(\"uploaded training data location: {}\".format(s3_train_data))\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(os.path.join(prefix, \"validation\", key)).upload_fileobj(\n",
    "    bufValid\n",
    ")\n",
    "s3_validation_data = \"s3://{}/{}/validation/{}\".format(bucket, prefix, key)\n",
    "print(\"uploaded validation data location: {}\".format(s3_validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-newcastle",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Training the linear model\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. Since this data is relatively small, it isn't meant to show off the performance of the Linear Learner training algorithm, although we have tested it on multi-terabyte datasets.\n",
    "\n",
    "This example takes four to six minutes to complete. Majority of the time is spent provisioning hardware and loading the algorithm container since the dataset is small.\n",
    "\n",
    "First, let's specify our containers.  Since we want this notebook to run in all 4 of Amazon SageMaker's regions, we'll create a small lookup.  More details on algorithm containers can be found in [AWS documentation](https://docs-aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "extensive-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "container = retrieve('linear-learner', boto3.Session().region_name, 'latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cognitive-haven",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name is: DEMO-linear-2021-05-26-19-09-45\n"
     ]
    }
   ],
   "source": [
    "linear_job = \"DEMO-linear-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "print(\"Job name is:\", linear_job)\n",
    "\n",
    "linear_training_params = {\n",
    "    \"RoleArn\": role,\n",
    "    \"TrainingJobName\": linear_job,\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"File\"},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.c4.2xlarge\", \"VolumeSizeInGB\": 10},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/train/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://{}/{}/validation/\".format(bucket, prefix),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                }\n",
    "            },\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"None\",\n",
    "        },\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": \"s3://{}/{}/\".format(bucket, prefix)},\n",
    "    \"HyperParameters\": {\n",
    "        \"feature_dim\": \"784\",\n",
    "        \"mini_batch_size\": \"200\",\n",
    "        \"predictor_type\": \"binary_classifier\",\n",
    "        \"epochs\": \"10\",\n",
    "        \"num_models\": \"32\",\n",
    "        \"loss\": \"absolute_loss\",\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 60 * 60},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-petroleum",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now let's kick off our training job in SageMaker's distributed, managed training, using the parameters we just created. Because training is managed (AWS handles spinning up and spinning down hardware), we don't have to wait for our job to finish to continue, but for this case, let's setup a while loop so we can monitor the status of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-russia",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sm = boto3.Session().client(\"sagemaker\")\n",
    "sm.create_training_job(**linear_training_params)\n",
    "\n",
    "status = sm.describe_training_job(TrainingJobName=linear_job)[\"TrainingJobStatus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "downtown-bottom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiter TrainingJobCompletedOrStopped failed: Waiter encountered a terminal failure state\n",
      "ClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sm.get_waiter(\"training_job_completed_or_stopped\").wait(TrainingJobName=linear_job)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "if status == \"Failed\":\n",
    "    message = sm.describe_training_job(TrainingJobName=linear_job)[\"FailureReason\"]\n",
    "    print(\"Training failed with the following error: {}\".format(message))\n",
    "    raise Exception(\"Training job failed\")\n",
    "message = sm.describe_training_job(TrainingJobName=linear_job)[\"FailureReason\"]\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dirty-sauce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Failed'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.describe_training_job(TrainingJobName=linear_job)[\"TrainingJobStatus\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "papermill": {
   "default_parameters": {},
   "duration": 173.016452,
   "end_time": "2021-05-25T00:23:05.969459",
   "environment_variables": {},
   "exception": true,
   "input_path": "parquet_to_recordio_protobuf.ipynb",
   "output_path": "/opt/ml/processing/output/parquet_to_recordio_protobuf-2021-05-25-00-16-08.ipynb",
   "parameters": {
    "kms_key": "arn:aws:kms:us-west-2:521695447989:key/6e9984db-50cf-4c7e-926c-877ec47a8b25"
   },
   "start_time": "2021-05-25T00:20:12.953007",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
