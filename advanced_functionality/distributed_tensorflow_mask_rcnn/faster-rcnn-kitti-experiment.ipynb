{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment for Transfer Learning of Faster R-CNN on KITTI dataset\n",
    "\n",
    "This notebook is a step-by-step tutorial on Amazon SageMaker experiment for transfer learning of [Faster R-CNN](https://arxiv.org/abs/1506.01497) model on [Kitti](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d) 2-D object dataset starting with a pre-trained Mask R-CNN model trained on [COCO 2017 dataset](http://cocodataset.org/#download). \n",
    "\n",
    "The outline of steps is as follows:\n",
    "\n",
    "1. Stage Kitti dataset in COCO format on S3\n",
    "2. Copy Kitti dataset from Amazon S3 to Amazon EFS file-system mounted on this notebook instance\n",
    "3. Build Docker training image and push it to [Amazon ECR](https://aws.amazon.com/ecr/)\n",
    "4. Configure data input channels\n",
    "5. Configure hyper-prarameters\n",
    "6. Define training metrics\n",
    "7. Define training job and start training\n",
    "\n",
    "Before we get started, let us initialize two python variables ```aws_region``` and ```s3_bucket``` that we will use throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region =  # <aws-region>\n",
    "s3_bucket  =  # <your-s3_bucket>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Kitti dataset in COCO format in Amazon S3\n",
    "\n",
    "Below are steps to convert Kitti dataset to COCO format and stage it in S3:\n",
    "\n",
    "1. Download [Kitti object dataset left camera color images](http://www.cvlibs.net/download.php?file=data_object_image_2.zip). \n",
    "2. Download [Kitti object dataset training labels](http://www.cvlibs.net/download.php?file=data_object_label_2.zip)\n",
    "3. `unzip` the downloaded zip files, which should create a folder structure as shown below:\n",
    "\n",
    "```\n",
    "   |-- training\n",
    "   |   |-- images_2\n",
    "   |   |__ label2\n",
    "   |\n",
    "   |-- testing\n",
    "   |__|__images_2\n",
    "      \n",
    "   ```\n",
    "3. Split the Kitti `training` dataset `image_2` and `label_2` files randomly into `train` and `val` datasets using a 90-10 split. We do not provide any tool for doing this split.\n",
    "4. Use [convert-dataset](https://github.com/eweill/convert-datasets/blob/master/convert-dataset.py) or any other tool to convert `train` and `val` datasets from `Kitti` to `Voc` format. \n",
    "5. Use [voc2coco](https://github.com/yukkyo/voc2coco) or any other tool to convert data from `Voc` to `Coco` format.\n",
    "    * Name the converted COCO annotations files for `train` and `val`  as `instances_trainKitti.json` and `instances_valKitti.json` respectively\n",
    "6. Copy the Kitti dataset (in COCO format) and [pretrained models](http://models.tensorpack.com/#FasterRCNN) to your S3 bucket. The required prefix structure in your S3 bucket is shown below:\n",
    "   \n",
    "```\n",
    "|-- faster-rcnn-kitti\n",
    "|   |-- testing\n",
    "|   |__ ...\n",
    "|   |\n",
    "|   |-- sagemaker\n",
    "|   |   |-- input\n",
    "|   |   |   |-- train\n",
    "|   |   |   |   |-- trainKitti\n",
    "|   |   |   |   |   |__ ...\n",
    "|   |   |   |   |\n",
    "|   |   |   |   |-- valKitti\n",
    "|   |   |   |   |   |__ ...\n",
    "|   |   |   |   |\n",
    "|   |   |   |   |-- annotations\n",
    "|   |   |   |   |   |-- instances_trainKitti.json\n",
    "|   |   |   |   |   |__ instances_valKitti.json\n",
    "|   |   |   |   |\n",
    "|   |   |   |   |-- pretrained-models\n",
    "|   |   |   |   |   |-- COCO-MaskRCNN-R101FPN1x.npz\n",
    "|   |   |   |   |   |-- COCO-MaskRCNN-R50FPN2x.npz\n",
    "|   |   |   |   |   |-- ImageNet-R101-AlignPadding.npz\n",
    "|___|___|___|___|___|__ ImageNet-R50-AlignPadding.npz\n",
    "       \n",
    "   ```\n",
    "The split `train` and `val` dataset images are copied under `faster-rcnn-kitti/sagemaker/input/train/trainKitti` and `faster-rcnn-kitti/sagemaker/input/train/valKitti` prefixes in S3, respectively. The `testing/image_2` images are copied under `faster-rcnn-kitti/testing` prefix in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Kitti dataset from S3 to Amazon EFS\n",
    "\n",
    "Next, we copy Kitti dataset from S3 to EFS file-system.  The ```prepare-kitti-efs.sh``` script executes this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./prepare-kitti-efs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already copied COCO 2017 dataset from S3 to your EFS file-system, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!./prepare-kitti-efs.sh {s3_bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and push SageMaker training images\n",
    "\n",
    "For this step, the [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) attached to this notebook instance needs full access to Amazon ECR service. If you created this notebook instance using the ```./stack-sm.sh``` script in this repository, the IAM Role attached to this notebook instance is already setup with full access to ECR service. \n",
    "\n",
    "Below, we will build an image for [TensorPack Faster-RCNN/Mask-RCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN) implementation and push it to Amazon ECR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorPack Faster-RCNN/Mask-RCNN\n",
    "\n",
    "Use ```./container-kitti/build_tools/build_and_push.sh``` script to build and push the TensorPack Faster-RCNN/Mask-RCNN  training image to Amazon ECR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./container-kitti/build_tools/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your *AWS region* as argument, run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! ./container-kitti/build_tools/build_and_push.sh {aws_region}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set ```tensorpack_image``` below to Amazon ECR URI of the image you pushed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorpack_image = #<amazon-ecr-uri>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Initialization \n",
    "We have staged the data and we have built and pushed the training docker image to Amazon ECR. Now we are ready to start using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "role = get_execution_role() # provide a pre-existing role ARN as an alternative to creating a new role\n",
    "print(f'SageMaker Execution Role:{role}')\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "print(f'AWS account:{account}')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f'AWS region:{region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set ```training_image``` to the Amazon ECR image URI you saved in a previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image = tensorpack_image \n",
    "print(f'Training image: {training_image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Data Channels\n",
    "\n",
    "Next, we define the *train* data channel using EFS file-system. To do so, we need to specify the EFS file-system id, which is shown in the output of the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -kh | grep 'fs-' | sed 's/\\(fs-[0-9a-z]*\\).*/\\1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the EFS ```file_system_id``` below to the ouput of the command shown above. In the cell below, we define the `train` data input channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# Specify EFS ile system id.\n",
    "file_system_id =  # 'fs-xxxxxxxx'\n",
    "print(f\"EFS file-system-id: {file_system_id}\")\n",
    "\n",
    "# Specify directory path for input data on the file system. \n",
    "# You need to provide normalized and absolute path below.\n",
    "file_system_directory_path = '/faster-rcnn-kitti/sagemaker/input/train'\n",
    "print(f'EFS file-system data input path: {file_system_directory_path}')\n",
    "\n",
    "# Specify the access mode of the mount of the directory associated with the file system. \n",
    "# Directory must be mounted  'ro'(read-only).\n",
    "file_system_access_mode = 'ro'\n",
    "\n",
    "# Specify your file system type\n",
    "file_system_type = 'EFS'\n",
    "\n",
    "train = FileSystemInput(file_system_id=file_system_id,\n",
    "                                    file_system_type=file_system_type,\n",
    "                                    directory_path=file_system_directory_path,\n",
    "                                    file_system_access_mode=file_system_access_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the model output location in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"faster-rcnn-kitti/sagemaker\" #prefix in your bucket\n",
    "s3_output_location = f's3://{s3_bucket}/{prefix}/output'\n",
    "print(f'S3 model output location: {s3_output_location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Hyper-parameters\n",
    "Next we define the hyper-parameters. \n",
    "\n",
    "Note, some hyper-parameters are different between the two implementations. The batch size per GPU in TensorPack Faster-RCNN/Mask-RCNN is fixed at 1, but is configurable in AWS Samples Mask-RCNN. The learning rate schedule is specified in units of steps in TensorPack Faster-RCNN/Mask-RCNN, but in epochs in AWS Samples Mask-RCNN.\n",
    "\n",
    "The detault learning rate schedule values shown below correspond to training for a total of 24 epochs, at 120,000 images per epoch.\n",
    "\n",
    "<table align='left'>\n",
    "    <caption>TensorPack Faster-RCNN/Mask-RCNN  Hyper-parameters</caption>\n",
    "    <tr>\n",
    "    <th style=\"text-align:center\">Hyper-parameter</th>\n",
    "    <th style=\"text-align:center\">Description</th>\n",
    "    <th style=\"text-align:center\">Default</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">mode_fpn</td>\n",
    "        <td style=\"text-align:left\">Flag to indicate use of Feature Pyramid Network (FPN) in the Mask R-CNN model backbone</td>\n",
    "        <td style=\"text-align:center\">\"True\"</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">mode_mask</td>\n",
    "        <td style=\"text-align:left\">A value of \"False\" means Faster-RCNN model, \"True\" means Mask R-CNN moodel</td>\n",
    "        <td style=\"text-align:center\">\"True\"</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">eval_period</td>\n",
    "        <td style=\"text-align:left\">Number of epochs period for evaluation during training</td>\n",
    "        <td style=\"text-align:center\">1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">lr_schedule</td>\n",
    "        <td style=\"text-align:left\">Learning rate schedule in training steps</td>\n",
    "        <td style=\"text-align:center\">'[240000, 320000, 360000]'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">batch_norm</td>\n",
    "        <td style=\"text-align:left\">Batch normalization option ('FreezeBN', 'SyncBN', 'GN', 'None') </td>\n",
    "        <td style=\"text-align:center\">'FreezeBN'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">images_per_epoch</td>\n",
    "        <td style=\"text-align:left\">Images per epoch </td>\n",
    "        <td style=\"text-align:center\">120000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">data_train</td>\n",
    "        <td style=\"text-align:left\">Training data under data directory</td>\n",
    "        <td style=\"text-align:center\">'coco_train2017'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">data_val</td>\n",
    "        <td style=\"text-align:left\">Validation data under data directory</td>\n",
    "        <td style=\"text-align:center\">'coco_val2017'</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">resnet_arch</td>\n",
    "        <td style=\"text-align:left\">Must be 'resnet50' or 'resnet101'</td>\n",
    "        <td style=\"text-align:center\">'resnet50'</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">backbone_weights</td>\n",
    "        <td style=\"text-align:left\">Pretrained RestNet Backbone weights</td>\n",
    "        <td style=\"text-align:center\">'ImageNet-R50-AlignPadding.npz'</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td style=\"text-align:center\">load_model</td>\n",
    "        <td style=\"text-align:left\">Load pretrained model weights</td>\n",
    "        <td style=\"text-align:center\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "                    \"mode_fpn\": \"True\",\n",
    "                    \"mode_mask\": \"False\",\n",
    "                    \"eval_period\": 1,\n",
    "                    \"batch_norm\": \"FreezeBN\",\n",
    "                    \"data_train\": \"coco_trainKitti\",\n",
    "                    \"data_val\": \"coco_valKitti\",\n",
    "                    \"lr_schedule\": '[14400, 18000, 21600]',\n",
    "                    \"images_per_epoch\": 7200\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Metrics\n",
    "Next, we define the regular expressions that SageMaker uses to extract algorithm metrics from training logs and send them to [AWS CloudWatch metrics](https://docs.aws.amazon.com/en_pv/AmazonCloudWatch/latest/monitoring/working_with_metrics.html). These algorithm metrics are visualized in SageMaker console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "             {\n",
    "                \"Name\": \"fastrcnn_losses/box_loss\",\n",
    "                \"Regex\": \".*fastrcnn_losses/box_loss:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/label_loss\",\n",
    "                \"Regex\": \".*fastrcnn_losses/label_loss:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/label_metrics/accuracy\",\n",
    "                \"Regex\": \".*fastrcnn_losses/label_metrics/accuracy:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/label_metrics/false_negative\",\n",
    "                \"Regex\": \".*fastrcnn_losses/label_metrics/false_negative:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/label_metrics/fg_accuracy\",\n",
    "                \"Regex\": \".*fastrcnn_losses/label_metrics/fg_accuracy:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"fastrcnn_losses/num_fg_label\",\n",
    "                \"Regex\": \".*fastrcnn_losses/num_fg_label:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/IoU=0.5\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.5:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/IoU=0.5:0.95\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.5:0\\\\.95:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/IoU=0.75\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/IoU=0\\\\.75:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/large\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/large:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/medium\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/medium:\\\\s*(\\\\S+).*\"\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"mAP(bbox)/small\",\n",
    "                \"Regex\": \".*mAP\\\\(bbox\\\\)/small:\\\\s*(\\\\S+).*\"\n",
    "            }\n",
    "            \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SageMaker Experiment\n",
    "\n",
    "To define SageMaker Experiment, we first install `sagemaker-experiments` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the SageMaker Experiment modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a `Tracker` for tracking input data used in the SageMaker Trials in this Experiment. Specify the S3 URL of your dataset in the `value` below and change the name of the dataset if you are using a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = session.client('sagemaker')\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    # we can log the s3 uri to the dataset used for training\n",
    "    tracker.log_input(name=\"kitti-object-dataset\", \n",
    "                      media_type=\"s3/uri\", \n",
    "                      value= f's3://{s3_bucket}/{prefix}/input/train' # specify S3 URL to your dataset\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a SageMaker Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_rcnn_experiment = Experiment.create(\n",
    "    experiment_name=f\"faster-rcnn-experiment-{int(time.time())}\", \n",
    "    description=\"Faster R-CNN Kitti Experiment\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(faster_rcnn_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the training job in your private VPC, so we need to set the ```subnets``` and ```security_group_ids``` prior to running the cell below. You may specify multiple subnet ids in the ```subnets``` list. The subnets included in the ```sunbets``` list must be part of the output of  ```./stack-sm.sh``` CloudFormation stack script used to create this notebook instance. Specify only one security group id in ```security_group_ids``` list. The security group id must be part of the output of  ```./stack-sm.sh``` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_group_ids =  # ['sg-xxxxxxxx'] \n",
    "subnets =   # [ 'subnet-xxxxxxx', ]\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use SageMaker [Estimator](https://sagemaker.readthedocs.io/en/stable/estimators.html) API to define a SageMaker Training Job for each SageMaker Trial we need to run within the SageMaker Experiment.\n",
    "\n",
    "We recommned using 8 GPUs, so we set ```train_instance_count=1``` and ```train_instance_type='ml.p3.16xlarge'```, because there are 8 Tesla V100 GPUs per ```ml.p3.16xlarge``` instance. We recommend using 100 GB [Amazon EBS](https://aws.amazon.com/ebs/) storage volume with each training instance, so we set ```train_volume_size = 100```. We want to replicate training data to each training instance, so we set ```input_mode= 'File'```.\n",
    "\n",
    "Next, we will iterate through the Trial parameters and start 8 trials, 4 each for ResNet architecture `resnet50` and `resnet101`, respectively. Note, if we need to set a configration variable directly, we prefix the variable name with `config:` and set it in the `hyperparameters`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_params = [ ('resnet50', 'ImageNet-R50-AlignPadding.npz', 'COCO-MaskRCNN-R50FPN2x.npz'), \n",
    "                 ('resnet101', 'ImageNet-R101-AlignPadding.npz', 'COCO-MaskRCNN-R101FPN1x.npz'),\n",
    "               ]\n",
    "\n",
    "anchor_params =  [ (\"'(32, 64, 128, 256, 512)'\", '16'),\n",
    "                  (\"'(16, 24, 32, 40, 48)'\",  '8'),\n",
    "                  (\"'(16, 32, 48, 64, 80)'\",  '8'),\n",
    "                  (\"'(16, 32, 64, 128, 256)'\", '8')]\n",
    "\n",
    "for resnet_arch, backbone_weights, load_model in trial_params:\n",
    "    \n",
    "    hyperparameters['resnet_arch'] = resnet_arch\n",
    "    hyperparameters['backbone_weights'] = backbone_weights\n",
    "    hyperparameters[\"load_model\"] = load_model\n",
    "        \n",
    "    for a_param in anchor_params:\n",
    "        hyperparameters['config:RPN.ANCHOR_SIZES'] = a_param[0]\n",
    "        hyperparameters['config:RPN.ANCHOR_STRIDE'] = a_param[1]\n",
    "       \n",
    "        trial_name = f\"faster-rcnn-{int(time.time())}\"\n",
    "        faster_rcnn_trial = Trial.create(\n",
    "                        trial_name=trial_name, \n",
    "                        experiment_name=faster_rcnn_experiment.experiment_name,\n",
    "                        sagemaker_boto_client=sm,\n",
    "        )\n",
    "    \n",
    "        # associate the proprocessing trial component with the current trial\n",
    "        faster_rcnn_trial.add_trial_component(tracker.trial_component)\n",
    "        print(faster_rcnn_trial)\n",
    "\n",
    "        faster_rcnn_estimator = Estimator(training_image,\n",
    "                                         role, \n",
    "                                         instance_count=1, \n",
    "                                         instance_type='ml.p3.16xlarge',\n",
    "                                         volume_size = 100,\n",
    "                                         max_run = 400000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sagemaker_session, \n",
    "                                         hyperparameters = hyperparameters,\n",
    "                                         metric_definitions = metric_definitions,\n",
    "                                         subnets=subnets,\n",
    "                                         security_group_ids=security_group_ids)\n",
    "    \n",
    "        # Specify directory path for log output on the EFS file system.\n",
    "        # You need to provide normalized and absolute path below.\n",
    "        # For example, '/faster-rcnn-kitti/sagemaker/output/log'\n",
    "        # Log output directory must not exist\n",
    "        file_system_directory_path = f'/faster-rcnn-kitti/sagemaker/output/{faster_rcnn_trial.trial_name}'\n",
    "        print(f\"EFS log directory:{file_system_directory_path}\")\n",
    "\n",
    "        # Create the log output directory. \n",
    "        # EFS file-system is mounted on '$HOME/efs' mount point for this notebook.\n",
    "        home_dir=os.environ['HOME']\n",
    "        local_efs_path = os.path.join(home_dir,'efs', file_system_directory_path[1:])\n",
    "        print(f\"Creating log directory on EFS: {local_efs_path}\")\n",
    "\n",
    "        assert not os.path.isdir(local_efs_path)\n",
    "        ! sudo mkdir -p -m a=rw {local_efs_path}\n",
    "        assert os.path.isdir(local_efs_path)\n",
    "\n",
    "        # Specify the access mode of the mount of the directory associated with the file system. \n",
    "        # Directory must be mounted 'rw'(read-write).\n",
    "        file_system_access_mode = 'rw'\n",
    "\n",
    "\n",
    "        log = FileSystemInput(file_system_id=file_system_id,\n",
    "                                        file_system_type=file_system_type,\n",
    "                                        directory_path=file_system_directory_path,\n",
    "                                        file_system_access_mode=file_system_access_mode)\n",
    "\n",
    "        data_channels = {'train': train, 'log': log}\n",
    "\n",
    "        print(f\"Starting Trial {faster_rcnn_trial.trial_name}\")\n",
    "        faster_rcnn_estimator.fit(inputs=data_channels, \n",
    "                                job_name=faster_rcnn_trial.trial_name,\n",
    "                                logs=True,  \n",
    "                                experiment_config={\"TrialName\": faster_rcnn_trial.trial_name, \n",
    "                                               \"TrialComponentDisplayName\": \"Training\"},\n",
    "                                wait=False)\n",
    "\n",
    "        # sleep in between starting trials\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a filter for searching through the experiment analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression = {\n",
    "    \"Filters\":[\n",
    "        {\n",
    "            \"Name\": \"DisplayName\",\n",
    "            \"Operator\": \"Equals\",\n",
    "            \"Value\": \"Training\"\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a query for experiment analytics,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    experiment_name=faster_rcnn_experiment.experiment_name,\n",
    "    search_expression=search_expression,\n",
    "    sort_by=\"metrics.fastrcnn_losses/label_metrics/accuracy.max\",\n",
    "    sort_order=\"Descending\",\n",
    "    parameter_names=['resnet_arch', 'config:RPN.ANCHOR_SIZES']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the analytics table based on our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_table = trial_component_analytics.dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we view selected columns of the analytics table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_map=analytic_table[['resnet_arch',\n",
    "                         'config:RPN.ANCHOR_SIZES',\n",
    "                         'mAP(bbox)/IoU=0.5:0.95 - Max', \n",
    "                         'mAP(bbox)/IoU=0.5 - Max', \n",
    "                         'mAP(bbox)/IoU=0.75 - Max']]\n",
    "bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
