{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation in Spark cluster for ML in Amazon SageMaker\n",
    "\n",
    "This example prepares [New York City Taxi and Limousine Commission Trip Record Data](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) dataset for machine learning in [Amazon SageMaker](https://aws.amazon.com/sagemaker/). This example requires that this Jupyter notebook is running in an [Amazon SageMaker Notebook instance attached to an AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-how-it-works.html). To create an Amazon SageMaker Notebook instance attached to an [AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint.html), execute the steps below:\n",
    "\n",
    "  1.  [Create a new Amazon S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html), use an existing S3 bucket, or use the default SageMaker bucket. **Note:** The S3 bucket must be in the same AWS region as this notebook.\n",
    "  2.  [Create an IAM Role to use with AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html)\n",
    "  3.  [Add an AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/add-dev-endpoint.html)\n",
    "      * Specify the IAM Role you created in Step 1 above\n",
    "      * In Step 4, for **worker type**, select `G.2X`, and use at least 10 workers\n",
    "  4.  [Use an Amazon SageMaker Notebook with your AWS Glue Development Endpoint](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-sage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into a DataFrame \n",
    "Next, we create a Resilient Distributed Dataset (RDD) from a CSV file stored in S3 bucket as part of [New York City Taxi and Limousine Commission (TLC) Trip Record Data](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) dataset in [Registry of Open Data on AWS](https://registry.opendata.aws/). We convert the RDD to a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "s3_bucket  = # your s3 bucket name\n",
    "\n",
    "try:\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.get_bucket_location(Bucket=s3_bucket)\n",
    "    print(f\"S3 Bucket: {s3_bucket}, AWS region: {response['LocationConstraint']}\")\n",
    "except:\n",
    "    print(f\"Access Error: Check if '{s3_bucket}' S3 bucket is in '{aws_region}' region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads RDD\n",
    "lines = sc.textFile(\"s3://nyc-tlc/misc/uber_nyc_data.csv\")\n",
    "# Split lines into columns; change split() argument depending on deliminiter e.g. '\\t'\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "# Convert RDD into DataFrame\n",
    "uber_df = spark.createDataFrame(parts, ['id','origin_taz','destination_taz','pickup_datetime','trip_distance','trip_duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the schema of the data we just loaded and also show 10 rows to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uber_df.printSchema())\n",
    "uber_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data ###\n",
    "Next, we clean the data. We drop any rows with any NULL or NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up data \n",
    "# remove id column as we don't need it\n",
    "uber_df1=uber_df.drop(uber_df.id)\n",
    "\n",
    "# drop all rows with any null value\n",
    "uber_df1=uber_df1.dropna(how='any')\n",
    "\n",
    "# filter rows where destnation, orign and trip duration are not set to NULL\n",
    "uber_df1=uber_df1.filter((uber_df1.destination_taz != 'NULL')  & \n",
    "    (uber_df1.origin_taz != 'NULL')  & \n",
    "    (uber_df1.trip_duration != 'NULL')  & \n",
    "    (uber_df1.destination_taz != 'destination_taz'))\n",
    "\n",
    "# show 10 rows\n",
    "uber_df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PySpark user-defined functions ###\n",
    "Below, we import relevant Python clasess for defining PySpark user-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, to_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting ordinal day of the week from pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define UDF for extracting pickup day of the week from datetime\n",
    "\n",
    "def weekday(x):\n",
    "    pickup=datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    return int(pickup.date().weekday())\n",
    "    \n",
    "pickup_day_udf = udf(weekday, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting month from the pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define month udf for extracting pickup month from datetime\n",
    "def month(x):\n",
    "    pickup=datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "    return int(pickup.date().month)\n",
    "    \n",
    "pickup_month_udf = udf(month, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function for extracting hour of the day from the pickup date timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pickup_time udf for extracting pickup hour from datetime\n",
    "\n",
    "def pickup_time(x):\n",
    "    ptime = datetime.strptime(x, '%Y-%m-%d %H:%M:%S').time()\n",
    "    return int(ptime.hour)\n",
    "    \n",
    "pickup_time_udf = udf(pickup_time, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a PySpark user-defined function that parses source and target zones as hexadecimal integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_taz(x):\n",
    "   return int(x, 16)\n",
    "\n",
    "taz_udf=udf(encode_taz, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a PySpark user-defined function that computes duration of the trip in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define duration udf for extracting duration in minutes\n",
    "def duration(x):\n",
    "    time=x.split(':')\n",
    "    duration = int(time[0]*60) + int(time[1])\n",
    "    return duration\n",
    "\n",
    "duration_udf = udf(duration, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for SageMaker XGBoost algorithm ###\n",
    "SageMaker XGBoost algorithm expects the label to be the first column. So, we transform the PySpark DataFrame to make `duration` as the first column, because we want to train the model to predict duration of an Uber ride, given ride pickup source and target zones, the month of the year, the day of the week, and hour of the day. Columns of the DataFrame are transformed using PySpark user-defined functions defined above. \n",
    "\n",
    "We also drop any rows with Null or NaN values as a result of transformations. We filter rows to keep rows with duration greater than 0 but less than 120 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new data frame\n",
    "# we want trip duration (minutes) in the first column as label for the row\n",
    "# our feature vector includes origin, desination, and pickup month, day, and hour\n",
    "# we will discard other columms\n",
    "uber_df2 = uber_df1.select(duration_udf(uber_df1.trip_duration).alias('duration'),\n",
    "    taz_udf(uber_df1.origin_taz).alias('origin'), \n",
    "    taz_udf(uber_df1.destination_taz).alias('destination'), \n",
    "    pickup_month_udf(uber_df1.pickup_datetime).alias('month'), \n",
    "    pickup_day_udf(uber_df1.pickup_datetime).alias('day'), \n",
    "    pickup_time_udf(uber_df1.pickup_datetime).alias('pickup_time'))\n",
    "\n",
    "uber_df3 = uber_df2.dropna(how='any')\n",
    "uber_df4 = uber_df3.filter((uber_df3.duration > 0) & (uber_df3.duration < 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show \n",
    "uber_df4.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a PySpark DataFrame is a RDD, data will be saved in S3 in multiple files in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save prepared data frame in your S3 bucket\n",
    "uber_df4.write.save(f\"s3://{s3_bucket}/glue/output/uber_nyc/v1\", format='csv', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
